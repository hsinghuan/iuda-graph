{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8cb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e860f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhchung/dyngraph-uda/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72e5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88b8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_nc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61fe5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585e964",
   "metadata": {},
   "source": [
    "## Model Structure ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5669cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        return x\n",
    "    \n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.linear2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.linear2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a4884",
   "metadata": {},
   "source": [
    "## Data Preparation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3195bdf",
   "metadata": {},
   "source": [
    "* Train: 0-6\n",
    "* Val: 7, 8\n",
    "* Test and adapt: 9-13, 14-18, 19-23, 24-28, 29-33, 34-38, 39-43, 44-48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3f3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir, dataset, sub_dataset=None):\n",
    "    if dataset == 'elliptic':\n",
    "        data = load_nc_dataset(data_dir, 'elliptic', sub_dataset)\n",
    "    else:\n",
    "        raise ValueError('Invalid dataname')\n",
    "    # if len(data.y.shape) == 1:\n",
    "    #     data.y = data.y.unsqueeze(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2e981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/hhchung/data/graph-data/elliptic_bitcoin_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca0a5c",
   "metadata": {},
   "source": [
    "## Train / Test / Adapt Loop ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c561d275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhchung/dyngraph-uda/elliptic/dataset.py:75: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "data = get_data(data_dir, 'elliptic', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1a89c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, mlp, optimizer, loader, loss_fn, device='cpu'):\n",
    "    encoder.train()\n",
    "    mlp.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = mlp(encoder(data.x, data.edge_index))\n",
    "        loss = loss_fn(out[data.mask], data.y[data.mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    total_train_loss /= len(loader)\n",
    "    return total_train_loss\n",
    "\n",
    "def adapt(encoder, mlp, domain_classifier, reverse_layer, optimizer, src_loader, tgt_loader, e, epochs, loss_fn_class, loss_fn_domain, device='cpu'):\n",
    "    encoder.train()\n",
    "    # mlp.train()\n",
    "    mlp.eval()\n",
    "    for param in mlp.parameters():\n",
    "        param.requires_grad = False\n",
    "    domain_classifier.train()\n",
    "    \n",
    "    \n",
    "    len_dataloader = min(len(src_loader), len(tgt_loader))\n",
    "    src_iter = iter(src_loader)\n",
    "    tgt_iter = iter(tgt_loader)\n",
    "    \n",
    "    # total_train_loss = 0\n",
    "    total_src_label_loss = 0\n",
    "    total_src_domain_loss = 0\n",
    "    total_tgt_domain_loss = 0\n",
    "    for i in range(len_dataloader):\n",
    "        p = float(i + e * len_dataloader) / epochs / len_dataloader\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        src_data = src_iter.next().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        src_batch_size = src_data.x.shape[0]\n",
    "        # print(\"source batch size\", batch_size)\n",
    "        domain_label = torch.zeros(src_batch_size).long().to(device)\n",
    "        \n",
    "        feature = encoder(src_data.x, src_data.edge_index)\n",
    "        class_output = mlp(feature)\n",
    "        domain_output = domain_classifier(ReverseLayerF.apply(feature, alpha))\n",
    "        \n",
    "        \n",
    "        loss_src_label = loss_fn_class(class_output[src_data.mask], src_data.y[src_data.mask])\n",
    "        loss_src_domain = loss_fn_domain(domain_output, domain_label)\n",
    "        \n",
    "\n",
    "        tgt_data = tgt_iter.next().to(device)\n",
    "        tgt_batch_size = tgt_data.x.shape[0]\n",
    "        # print(\"target batch size\", batch_size)\n",
    "        domain_label = torch.ones(tgt_batch_size).long().to(device)\n",
    "        \n",
    "        domain_output = domain_classifier(ReverseLayerF.apply(encoder(tgt_data.x, tgt_data.edge_index), alpha))\n",
    "        loss_tgt_domain = loss_fn_domain(domain_output, domain_label)\n",
    "        \n",
    "        total_batch_size = src_batch_size + tgt_batch_size\n",
    "        loss = loss_src_label + loss_src_domain + loss_tgt_domain\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # total_train_loss += loss.item()\n",
    "        total_src_label_loss += loss_src_label.item()\n",
    "        total_src_domain_loss += loss_src_domain.item()\n",
    "        total_tgt_domain_loss += loss_tgt_domain.item()\n",
    "    \n",
    "    # total_train_loss /= len_dataloader\n",
    "    total_src_label_loss /= len_dataloader\n",
    "    total_src_domain_loss /= len_dataloader\n",
    "    total_tgt_domain_loss /= len_dataloader\n",
    "    \n",
    "    return total_src_label_loss, total_src_domain_loss, total_tgt_domain_loss\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def test(encoder, mlp, loader, loss_fn, device='cpu'):\n",
    "    encoder.eval()\n",
    "    mlp.eval()\n",
    "    total_val_loss = 0\n",
    "    total_f1 = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = mlp(encoder(data.x, data.edge_index))\n",
    "        loss = loss_fn(out[data.mask], data.y[data.mask])\n",
    "        y_pred = torch.argmax(out, dim=1)\n",
    "        f1 = f1_score(y_pred[data.mask].detach().cpu().numpy(), data.y[data.mask].detach().cpu().numpy())\n",
    "        total_val_loss += loss.item()\n",
    "        total_f1 += f1\n",
    "    total_val_loss /= len(loader)\n",
    "    total_f1 /= len(loader)\n",
    "    return total_val_loss, total_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598cced",
   "metadata": {},
   "source": [
    "## Initial Source Stage ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d96f484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhchung/dyngraph-uda/elliptic/dataset.py:75: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "elliptic_0 = get_data(data_dir, 'elliptic', 0)\n",
    "feat_dim = elliptic_0.x.shape[1]\n",
    "hidden_dim = 128\n",
    "emb_dim = 128\n",
    "encoder = TwoLayerGraphSAGE(feat_dim, hidden_dim, emb_dim)\n",
    "mlp = MLPHead(emb_dim, emb_dim // 4, 2)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(mlp.parameters()), lr=1e-3)\n",
    "epochs = 500\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = encoder.to(device)\n",
    "mlp = mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c2f7706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split = [0,7,9]\n",
    "train_data = [get_data(data_dir, 'elliptic', i) for i in range(split[0],split[1])]\n",
    "val_data = [get_data(data_dir, 'elliptic', i) for i in range(split[1],split[2])]\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7671fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/500 Train Loss:0.5438 Val Loss:0.566 Val F1:0.0079\n",
      "Epoch:2/500 Train Loss:0.378 Val Loss:0.5199 Val F1:0.004\n",
      "Epoch:3/500 Train Loss:0.3441 Val Loss:0.5082 Val F1:0.004\n",
      "Epoch:4/500 Train Loss:0.3377 Val Loss:0.5067 Val F1:0.0\n",
      "Epoch:5/500 Train Loss:0.3361 Val Loss:0.5068 Val F1:0.0\n",
      "Epoch:6/500 Train Loss:0.3356 Val Loss:0.505 Val F1:0.0\n",
      "Epoch:7/500 Train Loss:0.3354 Val Loss:0.5028 Val F1:0.0\n",
      "Epoch:8/500 Train Loss:0.3352 Val Loss:0.5024 Val F1:0.0\n",
      "Epoch:9/500 Train Loss:0.3351 Val Loss:0.5017 Val F1:0.0\n",
      "Epoch:10/500 Train Loss:0.3349 Val Loss:0.5017 Val F1:0.0147\n",
      "Epoch:11/500 Train Loss:0.3348 Val Loss:0.5007 Val F1:0.0147\n",
      "Epoch:12/500 Train Loss:0.3346 Val Loss:0.501 Val F1:0.0185\n",
      "Epoch:13/500 Train Loss:0.3345 Val Loss:0.5007 Val F1:0.0227\n",
      "Epoch:14/500 Train Loss:0.3342 Val Loss:0.5002 Val F1:0.0185\n",
      "Epoch:15/500 Train Loss:0.3339 Val Loss:0.5007 Val F1:0.0393\n",
      "Epoch:16/500 Train Loss:0.3337 Val Loss:0.502 Val F1:0.0575\n",
      "Epoch:17/500 Train Loss:0.3338 Val Loss:0.5015 Val F1:0.0546\n",
      "Epoch:18/500 Train Loss:0.3336 Val Loss:0.4992 Val F1:0.058\n",
      "Epoch:19/500 Train Loss:0.3335 Val Loss:0.4991 Val F1:0.0541\n",
      "Epoch:20/500 Train Loss:0.3335 Val Loss:0.499 Val F1:0.0585\n",
      "Epoch:21/500 Train Loss:0.3335 Val Loss:0.4988 Val F1:0.0612\n",
      "Epoch:22/500 Train Loss:0.3334 Val Loss:0.4996 Val F1:0.0574\n",
      "Epoch:23/500 Train Loss:0.3333 Val Loss:0.5059 Val F1:0.0578\n",
      "Epoch:24/500 Train Loss:0.3332 Val Loss:0.5081 Val F1:0.0584\n",
      "Epoch:25/500 Train Loss:0.3332 Val Loss:0.5033 Val F1:0.0726\n",
      "Epoch:26/500 Train Loss:0.3331 Val Loss:0.5001 Val F1:0.0684\n",
      "Epoch:27/500 Train Loss:0.3328 Val Loss:0.4987 Val F1:0.0977\n",
      "Epoch:28/500 Train Loss:0.3327 Val Loss:0.4959 Val F1:0.1197\n",
      "Epoch:29/500 Train Loss:0.3326 Val Loss:0.4918 Val F1:0.1565\n",
      "Epoch:30/500 Train Loss:0.3324 Val Loss:0.4904 Val F1:0.1599\n",
      "Epoch:31/500 Train Loss:0.3323 Val Loss:0.4898 Val F1:0.1639\n",
      "Epoch:32/500 Train Loss:0.3324 Val Loss:0.4905 Val F1:0.163\n",
      "Epoch:33/500 Train Loss:0.3324 Val Loss:0.4906 Val F1:0.1358\n",
      "Epoch:34/500 Train Loss:0.3323 Val Loss:0.4896 Val F1:0.1499\n",
      "Epoch:35/500 Train Loss:0.3322 Val Loss:0.4879 Val F1:0.1699\n",
      "Epoch:36/500 Train Loss:0.3321 Val Loss:0.4866 Val F1:0.179\n",
      "Epoch:37/500 Train Loss:0.3321 Val Loss:0.4861 Val F1:0.1587\n",
      "Epoch:38/500 Train Loss:0.332 Val Loss:0.4842 Val F1:0.184\n",
      "Epoch:39/500 Train Loss:0.3321 Val Loss:0.4823 Val F1:0.1881\n",
      "Epoch:40/500 Train Loss:0.3319 Val Loss:0.483 Val F1:0.2076\n",
      "Epoch:41/500 Train Loss:0.3318 Val Loss:0.4824 Val F1:0.211\n",
      "Epoch:42/500 Train Loss:0.3318 Val Loss:0.4834 Val F1:0.212\n",
      "Epoch:43/500 Train Loss:0.3317 Val Loss:0.4821 Val F1:0.2127\n",
      "Epoch:44/500 Train Loss:0.3315 Val Loss:0.48 Val F1:0.2488\n",
      "Epoch:45/500 Train Loss:0.3316 Val Loss:0.4778 Val F1:0.2619\n",
      "Epoch:46/500 Train Loss:0.3315 Val Loss:0.4773 Val F1:0.2663\n",
      "Epoch:47/500 Train Loss:0.3313 Val Loss:0.4753 Val F1:0.2714\n",
      "Epoch:48/500 Train Loss:0.3314 Val Loss:0.473 Val F1:0.2752\n",
      "Epoch:49/500 Train Loss:0.3311 Val Loss:0.4707 Val F1:0.2836\n",
      "Epoch:50/500 Train Loss:0.3306 Val Loss:0.4658 Val F1:0.3341\n",
      "Epoch:51/500 Train Loss:0.3306 Val Loss:0.4654 Val F1:0.3408\n",
      "Epoch:52/500 Train Loss:0.3301 Val Loss:0.465 Val F1:0.3337\n",
      "Epoch:53/500 Train Loss:0.3302 Val Loss:0.4644 Val F1:0.3507\n",
      "Epoch:54/500 Train Loss:0.3304 Val Loss:0.4613 Val F1:0.383\n",
      "Epoch:55/500 Train Loss:0.3303 Val Loss:0.4612 Val F1:0.3779\n",
      "Epoch:56/500 Train Loss:0.3298 Val Loss:0.4479 Val F1:0.4296\n",
      "Epoch:57/500 Train Loss:0.3292 Val Loss:0.4375 Val F1:0.5043\n",
      "Epoch:58/500 Train Loss:0.3283 Val Loss:0.4213 Val F1:0.5674\n",
      "Epoch:59/500 Train Loss:0.3271 Val Loss:0.4199 Val F1:0.5595\n",
      "Epoch:60/500 Train Loss:0.3286 Val Loss:0.4141 Val F1:0.5972\n",
      "Epoch:61/500 Train Loss:0.3253 Val Loss:0.3955 Val F1:0.6569\n",
      "Epoch:62/500 Train Loss:0.325 Val Loss:0.4049 Val F1:0.6262\n",
      "Epoch:63/500 Train Loss:0.3255 Val Loss:0.4107 Val F1:0.5926\n",
      "Epoch:64/500 Train Loss:0.3241 Val Loss:0.3887 Val F1:0.7019\n",
      "Epoch:65/500 Train Loss:0.3243 Val Loss:0.3859 Val F1:0.7083\n",
      "Epoch:66/500 Train Loss:0.3253 Val Loss:0.3921 Val F1:0.6529\n",
      "Epoch:67/500 Train Loss:0.3238 Val Loss:0.3768 Val F1:0.7529\n",
      "Epoch:68/500 Train Loss:0.3257 Val Loss:0.3883 Val F1:0.6784\n",
      "Epoch:69/500 Train Loss:0.3242 Val Loss:0.4131 Val F1:0.5733\n",
      "Epoch:70/500 Train Loss:0.3238 Val Loss:0.3939 Val F1:0.6528\n",
      "Epoch:71/500 Train Loss:0.3239 Val Loss:0.3879 Val F1:0.6964\n",
      "Epoch:72/500 Train Loss:0.3236 Val Loss:0.3926 Val F1:0.6368\n",
      "Epoch:73/500 Train Loss:0.3235 Val Loss:0.3837 Val F1:0.7176\n",
      "Epoch:74/500 Train Loss:0.3229 Val Loss:0.3774 Val F1:0.7634\n",
      "Epoch:75/500 Train Loss:0.3222 Val Loss:0.3795 Val F1:0.7302\n",
      "Epoch:76/500 Train Loss:0.3219 Val Loss:0.381 Val F1:0.7042\n",
      "Epoch:77/500 Train Loss:0.3218 Val Loss:0.3768 Val F1:0.7443\n",
      "Epoch:78/500 Train Loss:0.3213 Val Loss:0.3722 Val F1:0.7824\n",
      "Epoch:79/500 Train Loss:0.3219 Val Loss:0.3745 Val F1:0.7651\n",
      "Epoch:80/500 Train Loss:0.3212 Val Loss:0.3754 Val F1:0.7398\n",
      "Epoch:81/500 Train Loss:0.3215 Val Loss:0.3764 Val F1:0.7475\n",
      "Epoch:82/500 Train Loss:0.3212 Val Loss:0.3745 Val F1:0.7605\n",
      "Epoch:83/500 Train Loss:0.3211 Val Loss:0.3735 Val F1:0.7561\n",
      "Epoch:84/500 Train Loss:0.3211 Val Loss:0.3757 Val F1:0.7113\n",
      "Epoch:85/500 Train Loss:0.321 Val Loss:0.3756 Val F1:0.7322\n",
      "Epoch:86/500 Train Loss:0.3211 Val Loss:0.3732 Val F1:0.754\n",
      "Epoch:87/500 Train Loss:0.3208 Val Loss:0.3778 Val F1:0.7312\n",
      "Epoch:88/500 Train Loss:0.3209 Val Loss:0.3748 Val F1:0.7403\n",
      "Epoch:89/500 Train Loss:0.3207 Val Loss:0.3705 Val F1:0.7293\n",
      "Epoch:90/500 Train Loss:0.3207 Val Loss:0.3783 Val F1:0.7003\n",
      "Epoch:91/500 Train Loss:0.321 Val Loss:0.3747 Val F1:0.7341\n",
      "Epoch:92/500 Train Loss:0.3207 Val Loss:0.3804 Val F1:0.7182\n",
      "Epoch:93/500 Train Loss:0.3216 Val Loss:0.3773 Val F1:0.7299\n",
      "Epoch:94/500 Train Loss:0.3223 Val Loss:0.3761 Val F1:0.7676\n",
      "Epoch:95/500 Train Loss:0.3215 Val Loss:0.3762 Val F1:0.7087\n",
      "Epoch:96/500 Train Loss:0.3219 Val Loss:0.371 Val F1:0.7671\n",
      "Epoch:97/500 Train Loss:0.3215 Val Loss:0.3756 Val F1:0.775\n",
      "Epoch:98/500 Train Loss:0.321 Val Loss:0.3793 Val F1:0.7322\n",
      "Epoch:99/500 Train Loss:0.3219 Val Loss:0.3836 Val F1:0.6885\n",
      "Epoch:100/500 Train Loss:0.3214 Val Loss:0.3793 Val F1:0.7297\n",
      "Epoch:101/500 Train Loss:0.3215 Val Loss:0.3818 Val F1:0.7543\n",
      "Epoch:102/500 Train Loss:0.3207 Val Loss:0.3738 Val F1:0.7489\n",
      "Epoch:103/500 Train Loss:0.3201 Val Loss:0.3716 Val F1:0.7611\n",
      "Epoch:104/500 Train Loss:0.3214 Val Loss:0.3654 Val F1:0.7901\n",
      "Epoch:105/500 Train Loss:0.3201 Val Loss:0.3686 Val F1:0.7839\n",
      "Epoch:106/500 Train Loss:0.3211 Val Loss:0.3706 Val F1:0.7624\n",
      "Epoch:107/500 Train Loss:0.3207 Val Loss:0.3682 Val F1:0.7789\n",
      "Epoch:108/500 Train Loss:0.3211 Val Loss:0.3734 Val F1:0.7634\n",
      "Epoch:109/500 Train Loss:0.321 Val Loss:0.3731 Val F1:0.7506\n",
      "Epoch:110/500 Train Loss:0.3205 Val Loss:0.3709 Val F1:0.7515\n",
      "Epoch:111/500 Train Loss:0.3205 Val Loss:0.368 Val F1:0.8046\n",
      "Epoch:112/500 Train Loss:0.3201 Val Loss:0.3638 Val F1:0.7911\n",
      "Epoch:113/500 Train Loss:0.3198 Val Loss:0.3628 Val F1:0.8069\n",
      "Epoch:114/500 Train Loss:0.3204 Val Loss:0.3639 Val F1:0.7724\n",
      "Epoch:115/500 Train Loss:0.3199 Val Loss:0.3672 Val F1:0.7674\n",
      "Epoch:116/500 Train Loss:0.32 Val Loss:0.3732 Val F1:0.7189\n",
      "Epoch:117/500 Train Loss:0.3197 Val Loss:0.3684 Val F1:0.7662\n",
      "Epoch:118/500 Train Loss:0.3196 Val Loss:0.3673 Val F1:0.8036\n",
      "Epoch:119/500 Train Loss:0.3204 Val Loss:0.3638 Val F1:0.8111\n",
      "Epoch:120/500 Train Loss:0.3204 Val Loss:0.3719 Val F1:0.734\n",
      "Epoch:121/500 Train Loss:0.3207 Val Loss:0.3636 Val F1:0.7998\n",
      "Epoch:122/500 Train Loss:0.3217 Val Loss:0.3668 Val F1:0.8204\n",
      "Epoch:123/500 Train Loss:0.3201 Val Loss:0.368 Val F1:0.7709\n",
      "Epoch:124/500 Train Loss:0.3205 Val Loss:0.3646 Val F1:0.791\n",
      "Epoch:125/500 Train Loss:0.3203 Val Loss:0.37 Val F1:0.7825\n",
      "Epoch:126/500 Train Loss:0.3212 Val Loss:0.3704 Val F1:0.7637\n",
      "Epoch:127/500 Train Loss:0.3205 Val Loss:0.3793 Val F1:0.6723\n",
      "Epoch:128/500 Train Loss:0.3213 Val Loss:0.3718 Val F1:0.734\n",
      "Epoch:129/500 Train Loss:0.3201 Val Loss:0.3705 Val F1:0.7901\n",
      "Epoch:130/500 Train Loss:0.3211 Val Loss:0.371 Val F1:0.7911\n",
      "Epoch:131/500 Train Loss:0.3202 Val Loss:0.3747 Val F1:0.7093\n",
      "Epoch:132/500 Train Loss:0.3205 Val Loss:0.367 Val F1:0.7716\n",
      "Epoch:133/500 Train Loss:0.3199 Val Loss:0.3654 Val F1:0.7852\n",
      "Epoch:134/500 Train Loss:0.3198 Val Loss:0.3679 Val F1:0.7831\n",
      "Epoch:135/500 Train Loss:0.3198 Val Loss:0.3657 Val F1:0.7569\n",
      "Epoch:136/500 Train Loss:0.3199 Val Loss:0.3699 Val F1:0.7409\n",
      "Epoch:137/500 Train Loss:0.3196 Val Loss:0.3723 Val F1:0.7501\n",
      "Epoch:138/500 Train Loss:0.3196 Val Loss:0.376 Val F1:0.7549\n",
      "Epoch:139/500 Train Loss:0.3196 Val Loss:0.3696 Val F1:0.7702\n",
      "Epoch:140/500 Train Loss:0.3195 Val Loss:0.3739 Val F1:0.7171\n",
      "Epoch:141/500 Train Loss:0.3199 Val Loss:0.3721 Val F1:0.7383\n",
      "Epoch:142/500 Train Loss:0.3194 Val Loss:0.3692 Val F1:0.7732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:143/500 Train Loss:0.3197 Val Loss:0.3664 Val F1:0.7921\n",
      "Epoch:144/500 Train Loss:0.3194 Val Loss:0.3644 Val F1:0.7832\n",
      "Epoch:145/500 Train Loss:0.3195 Val Loss:0.3651 Val F1:0.8032\n",
      "Epoch:146/500 Train Loss:0.3195 Val Loss:0.3678 Val F1:0.7924\n",
      "Epoch:147/500 Train Loss:0.3197 Val Loss:0.369 Val F1:0.7641\n",
      "Epoch:148/500 Train Loss:0.3193 Val Loss:0.3686 Val F1:0.7562\n",
      "Epoch:149/500 Train Loss:0.3196 Val Loss:0.3752 Val F1:0.7114\n",
      "Epoch:150/500 Train Loss:0.3201 Val Loss:0.376 Val F1:0.7114\n",
      "Epoch:151/500 Train Loss:0.3199 Val Loss:0.3654 Val F1:0.7892\n",
      "Epoch:152/500 Train Loss:0.3199 Val Loss:0.3676 Val F1:0.7962\n",
      "Epoch:153/500 Train Loss:0.3198 Val Loss:0.3682 Val F1:0.7718\n",
      "Epoch:154/500 Train Loss:0.3199 Val Loss:0.3648 Val F1:0.7777\n",
      "Epoch:155/500 Train Loss:0.3199 Val Loss:0.3649 Val F1:0.7789\n",
      "Epoch:156/500 Train Loss:0.3196 Val Loss:0.367 Val F1:0.7885\n",
      "Epoch:157/500 Train Loss:0.3197 Val Loss:0.367 Val F1:0.7968\n",
      "Epoch:158/500 Train Loss:0.3193 Val Loss:0.3678 Val F1:0.7871\n",
      "Epoch:159/500 Train Loss:0.3195 Val Loss:0.368 Val F1:0.7722\n",
      "Epoch:160/500 Train Loss:0.3193 Val Loss:0.3647 Val F1:0.7854\n",
      "Epoch:161/500 Train Loss:0.3193 Val Loss:0.3672 Val F1:0.7629\n",
      "Epoch:162/500 Train Loss:0.3195 Val Loss:0.366 Val F1:0.7688\n",
      "Epoch:163/500 Train Loss:0.3193 Val Loss:0.3666 Val F1:0.7774\n",
      "Epoch:164/500 Train Loss:0.3195 Val Loss:0.3675 Val F1:0.7706\n",
      "Epoch:165/500 Train Loss:0.3193 Val Loss:0.3664 Val F1:0.7839\n",
      "Epoch:166/500 Train Loss:0.3193 Val Loss:0.3648 Val F1:0.7789\n",
      "Epoch:167/500 Train Loss:0.3194 Val Loss:0.3666 Val F1:0.7662\n",
      "Epoch:168/500 Train Loss:0.3192 Val Loss:0.3682 Val F1:0.7698\n",
      "Epoch:169/500 Train Loss:0.3193 Val Loss:0.3688 Val F1:0.7777\n",
      "Epoch:170/500 Train Loss:0.3194 Val Loss:0.3673 Val F1:0.7546\n",
      "Epoch:171/500 Train Loss:0.3193 Val Loss:0.3657 Val F1:0.7667\n",
      "Epoch:172/500 Train Loss:0.3192 Val Loss:0.3656 Val F1:0.7764\n",
      "Epoch:173/500 Train Loss:0.3192 Val Loss:0.3668 Val F1:0.7728\n",
      "Epoch:174/500 Train Loss:0.3192 Val Loss:0.3673 Val F1:0.7503\n",
      "Epoch:175/500 Train Loss:0.3192 Val Loss:0.3651 Val F1:0.7659\n",
      "Epoch:176/500 Train Loss:0.3192 Val Loss:0.3627 Val F1:0.7943\n",
      "Epoch:177/500 Train Loss:0.3192 Val Loss:0.3673 Val F1:0.7728\n",
      "Epoch:178/500 Train Loss:0.3192 Val Loss:0.3686 Val F1:0.7707\n",
      "Epoch:179/500 Train Loss:0.3192 Val Loss:0.3666 Val F1:0.7681\n",
      "Epoch:180/500 Train Loss:0.3192 Val Loss:0.3665 Val F1:0.774\n",
      "Epoch:181/500 Train Loss:0.3192 Val Loss:0.3665 Val F1:0.7689\n",
      "Epoch:182/500 Train Loss:0.3192 Val Loss:0.3639 Val F1:0.7735\n",
      "Epoch:183/500 Train Loss:0.3192 Val Loss:0.3636 Val F1:0.808\n",
      "Epoch:184/500 Train Loss:0.3192 Val Loss:0.3629 Val F1:0.7955\n",
      "Epoch:185/500 Train Loss:0.3192 Val Loss:0.3649 Val F1:0.7856\n",
      "Epoch:186/500 Train Loss:0.3192 Val Loss:0.3641 Val F1:0.7848\n",
      "Epoch:187/500 Train Loss:0.3192 Val Loss:0.3664 Val F1:0.7608\n",
      "Epoch:188/500 Train Loss:0.3193 Val Loss:0.3675 Val F1:0.7516\n",
      "Epoch:189/500 Train Loss:0.3192 Val Loss:0.3671 Val F1:0.7416\n",
      "Epoch:190/500 Train Loss:0.319 Val Loss:0.3661 Val F1:0.7774\n",
      "Epoch:191/500 Train Loss:0.3191 Val Loss:0.3676 Val F1:0.7803\n",
      "Epoch:192/500 Train Loss:0.3191 Val Loss:0.3679 Val F1:0.7819\n",
      "Epoch:193/500 Train Loss:0.3192 Val Loss:0.3679 Val F1:0.7707\n",
      "Epoch:194/500 Train Loss:0.3191 Val Loss:0.3656 Val F1:0.7828\n",
      "Epoch:195/500 Train Loss:0.3189 Val Loss:0.3657 Val F1:0.7778\n",
      "Epoch:196/500 Train Loss:0.319 Val Loss:0.3674 Val F1:0.7745\n",
      "Epoch:197/500 Train Loss:0.3191 Val Loss:0.3655 Val F1:0.7532\n",
      "Epoch:198/500 Train Loss:0.3191 Val Loss:0.37 Val F1:0.7654\n",
      "Epoch:199/500 Train Loss:0.3191 Val Loss:0.3681 Val F1:0.7905\n",
      "Epoch:200/500 Train Loss:0.319 Val Loss:0.3684 Val F1:0.7703\n",
      "Epoch:201/500 Train Loss:0.319 Val Loss:0.3676 Val F1:0.7647\n",
      "Epoch:202/500 Train Loss:0.319 Val Loss:0.3673 Val F1:0.7635\n",
      "Epoch:203/500 Train Loss:0.3192 Val Loss:0.3696 Val F1:0.767\n",
      "Epoch:204/500 Train Loss:0.319 Val Loss:0.3746 Val F1:0.7454\n",
      "Epoch:205/500 Train Loss:0.319 Val Loss:0.3716 Val F1:0.762\n",
      "Epoch:206/500 Train Loss:0.319 Val Loss:0.373 Val F1:0.7497\n",
      "Epoch:207/500 Train Loss:0.3189 Val Loss:0.3723 Val F1:0.7572\n",
      "Epoch:208/500 Train Loss:0.319 Val Loss:0.3714 Val F1:0.7429\n",
      "Epoch:209/500 Train Loss:0.3189 Val Loss:0.3695 Val F1:0.7474\n",
      "Epoch:210/500 Train Loss:0.319 Val Loss:0.3678 Val F1:0.7596\n",
      "Epoch:211/500 Train Loss:0.3192 Val Loss:0.3692 Val F1:0.765\n",
      "Epoch:212/500 Train Loss:0.319 Val Loss:0.3704 Val F1:0.7712\n",
      "Epoch:213/500 Train Loss:0.3191 Val Loss:0.3648 Val F1:0.7913\n",
      "Epoch:214/500 Train Loss:0.3191 Val Loss:0.3697 Val F1:0.769\n",
      "Epoch:215/500 Train Loss:0.3191 Val Loss:0.3675 Val F1:0.7537\n",
      "Epoch:216/500 Train Loss:0.319 Val Loss:0.3667 Val F1:0.7843\n",
      "Epoch:217/500 Train Loss:0.3189 Val Loss:0.367 Val F1:0.8028\n",
      "Epoch:218/500 Train Loss:0.3189 Val Loss:0.3643 Val F1:0.7967\n",
      "Epoch:219/500 Train Loss:0.3189 Val Loss:0.3662 Val F1:0.7753\n",
      "Epoch:220/500 Train Loss:0.3189 Val Loss:0.3661 Val F1:0.7822\n",
      "Epoch:221/500 Train Loss:0.3189 Val Loss:0.369 Val F1:0.7632\n",
      "Epoch:222/500 Train Loss:0.3189 Val Loss:0.3672 Val F1:0.7695\n",
      "Epoch:223/500 Train Loss:0.3189 Val Loss:0.3713 Val F1:0.751\n",
      "Epoch:224/500 Train Loss:0.319 Val Loss:0.3717 Val F1:0.7388\n",
      "Epoch:225/500 Train Loss:0.3192 Val Loss:0.3707 Val F1:0.7339\n",
      "Epoch:226/500 Train Loss:0.3189 Val Loss:0.3704 Val F1:0.7491\n",
      "Epoch:227/500 Train Loss:0.3189 Val Loss:0.3695 Val F1:0.7613\n",
      "Epoch:228/500 Train Loss:0.319 Val Loss:0.3656 Val F1:0.776\n",
      "Epoch:229/500 Train Loss:0.3188 Val Loss:0.3666 Val F1:0.7791\n",
      "Epoch:230/500 Train Loss:0.3189 Val Loss:0.3655 Val F1:0.78\n",
      "Epoch:231/500 Train Loss:0.3189 Val Loss:0.368 Val F1:0.7631\n",
      "Epoch:232/500 Train Loss:0.3189 Val Loss:0.3655 Val F1:0.775\n",
      "Epoch:233/500 Train Loss:0.3189 Val Loss:0.3651 Val F1:0.7806\n",
      "Epoch:234/500 Train Loss:0.3189 Val Loss:0.3632 Val F1:0.7925\n",
      "Epoch:235/500 Train Loss:0.3189 Val Loss:0.3681 Val F1:0.7871\n",
      "Epoch:236/500 Train Loss:0.3189 Val Loss:0.3635 Val F1:0.7814\n",
      "Epoch:237/500 Train Loss:0.3189 Val Loss:0.3649 Val F1:0.7578\n",
      "Epoch:238/500 Train Loss:0.319 Val Loss:0.3657 Val F1:0.7585\n",
      "Epoch:239/500 Train Loss:0.3189 Val Loss:0.3646 Val F1:0.776\n",
      "Epoch:240/500 Train Loss:0.319 Val Loss:0.37 Val F1:0.7967\n",
      "Epoch:241/500 Train Loss:0.319 Val Loss:0.3649 Val F1:0.7916\n",
      "Epoch:242/500 Train Loss:0.319 Val Loss:0.3638 Val F1:0.7764\n",
      "Epoch:243/500 Train Loss:0.3189 Val Loss:0.3679 Val F1:0.7582\n",
      "Epoch:244/500 Train Loss:0.3191 Val Loss:0.3654 Val F1:0.7634\n",
      "Epoch:245/500 Train Loss:0.319 Val Loss:0.3678 Val F1:0.7845\n",
      "Epoch:246/500 Train Loss:0.319 Val Loss:0.3646 Val F1:0.7873\n",
      "Epoch:247/500 Train Loss:0.319 Val Loss:0.3679 Val F1:0.7742\n",
      "Epoch:248/500 Train Loss:0.3189 Val Loss:0.3646 Val F1:0.7669\n",
      "Epoch:249/500 Train Loss:0.3189 Val Loss:0.3656 Val F1:0.768\n",
      "Epoch:250/500 Train Loss:0.3188 Val Loss:0.3634 Val F1:0.791\n",
      "Epoch:251/500 Train Loss:0.3188 Val Loss:0.3675 Val F1:0.7677\n",
      "Epoch:252/500 Train Loss:0.3189 Val Loss:0.3678 Val F1:0.7764\n",
      "Epoch:253/500 Train Loss:0.3188 Val Loss:0.3682 Val F1:0.751\n",
      "Epoch:254/500 Train Loss:0.3188 Val Loss:0.3677 Val F1:0.7478\n",
      "Epoch:255/500 Train Loss:0.3188 Val Loss:0.3622 Val F1:0.7775\n",
      "Epoch:256/500 Train Loss:0.3189 Val Loss:0.3685 Val F1:0.7626\n",
      "Epoch:257/500 Train Loss:0.3189 Val Loss:0.3701 Val F1:0.7504\n",
      "Epoch:258/500 Train Loss:0.319 Val Loss:0.3715 Val F1:0.7463\n",
      "Epoch:259/500 Train Loss:0.3194 Val Loss:0.3736 Val F1:0.7329\n",
      "Epoch:260/500 Train Loss:0.3195 Val Loss:0.365 Val F1:0.7961\n",
      "Epoch:261/500 Train Loss:0.3216 Val Loss:0.3637 Val F1:0.7883\n",
      "Epoch:262/500 Train Loss:0.3231 Val Loss:0.3907 Val F1:0.6445\n",
      "Epoch:263/500 Train Loss:0.3216 Val Loss:0.3716 Val F1:0.7448\n",
      "Epoch:264/500 Train Loss:0.321 Val Loss:0.3702 Val F1:0.7693\n",
      "Epoch:265/500 Train Loss:0.3204 Val Loss:0.3724 Val F1:0.7475\n",
      "Epoch:266/500 Train Loss:0.3201 Val Loss:0.3693 Val F1:0.7684\n",
      "Epoch:267/500 Train Loss:0.3198 Val Loss:0.3689 Val F1:0.7772\n",
      "Epoch:268/500 Train Loss:0.3197 Val Loss:0.3683 Val F1:0.7711\n",
      "Epoch:269/500 Train Loss:0.3195 Val Loss:0.3701 Val F1:0.7306\n",
      "Epoch:270/500 Train Loss:0.3195 Val Loss:0.3743 Val F1:0.7336\n",
      "Epoch:271/500 Train Loss:0.3189 Val Loss:0.3701 Val F1:0.7731\n",
      "Epoch:272/500 Train Loss:0.3198 Val Loss:0.3749 Val F1:0.7316\n",
      "Epoch:273/500 Train Loss:0.3191 Val Loss:0.3705 Val F1:0.7502\n",
      "Epoch:274/500 Train Loss:0.3191 Val Loss:0.3715 Val F1:0.7351\n",
      "Epoch:275/500 Train Loss:0.3193 Val Loss:0.3688 Val F1:0.7449\n",
      "Epoch:276/500 Train Loss:0.3192 Val Loss:0.37 Val F1:0.7771\n",
      "Epoch:277/500 Train Loss:0.3199 Val Loss:0.3715 Val F1:0.7896\n",
      "Epoch:278/500 Train Loss:0.319 Val Loss:0.3654 Val F1:0.7826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:279/500 Train Loss:0.3189 Val Loss:0.3672 Val F1:0.7635\n",
      "Epoch:280/500 Train Loss:0.319 Val Loss:0.3648 Val F1:0.7834\n",
      "Epoch:281/500 Train Loss:0.3191 Val Loss:0.3679 Val F1:0.7734\n",
      "Epoch:282/500 Train Loss:0.3186 Val Loss:0.3668 Val F1:0.7695\n",
      "Epoch:283/500 Train Loss:0.3188 Val Loss:0.3683 Val F1:0.77\n",
      "Epoch:284/500 Train Loss:0.3189 Val Loss:0.3662 Val F1:0.7661\n",
      "Epoch:285/500 Train Loss:0.3187 Val Loss:0.3648 Val F1:0.7641\n",
      "Epoch:286/500 Train Loss:0.3188 Val Loss:0.3705 Val F1:0.7179\n",
      "Epoch:287/500 Train Loss:0.3188 Val Loss:0.3738 Val F1:0.7013\n",
      "Epoch:288/500 Train Loss:0.3186 Val Loss:0.3806 Val F1:0.697\n",
      "Epoch:289/500 Train Loss:0.3186 Val Loss:0.3814 Val F1:0.6963\n",
      "Epoch:290/500 Train Loss:0.3187 Val Loss:0.3759 Val F1:0.7185\n",
      "Epoch:291/500 Train Loss:0.3188 Val Loss:0.375 Val F1:0.7288\n",
      "Epoch:292/500 Train Loss:0.3188 Val Loss:0.3762 Val F1:0.7081\n",
      "Epoch:293/500 Train Loss:0.3188 Val Loss:0.3763 Val F1:0.7071\n",
      "Epoch:294/500 Train Loss:0.3189 Val Loss:0.3756 Val F1:0.7182\n",
      "Epoch:295/500 Train Loss:0.3188 Val Loss:0.3693 Val F1:0.7476\n",
      "Epoch:296/500 Train Loss:0.3188 Val Loss:0.3614 Val F1:0.7811\n",
      "Epoch:297/500 Train Loss:0.3189 Val Loss:0.3674 Val F1:0.765\n",
      "Epoch:298/500 Train Loss:0.3187 Val Loss:0.3634 Val F1:0.7901\n",
      "Epoch:299/500 Train Loss:0.3189 Val Loss:0.3647 Val F1:0.771\n",
      "Epoch:300/500 Train Loss:0.3188 Val Loss:0.3658 Val F1:0.7866\n",
      "Epoch:301/500 Train Loss:0.3187 Val Loss:0.3678 Val F1:0.7709\n",
      "Epoch:302/500 Train Loss:0.3187 Val Loss:0.3637 Val F1:0.7796\n",
      "Epoch:303/500 Train Loss:0.3187 Val Loss:0.3666 Val F1:0.7806\n",
      "Epoch:304/500 Train Loss:0.3188 Val Loss:0.3644 Val F1:0.8014\n",
      "Epoch:305/500 Train Loss:0.3186 Val Loss:0.3664 Val F1:0.7866\n",
      "Epoch:306/500 Train Loss:0.3187 Val Loss:0.3673 Val F1:0.7524\n",
      "Epoch:307/500 Train Loss:0.3187 Val Loss:0.3684 Val F1:0.7728\n",
      "Epoch:308/500 Train Loss:0.3186 Val Loss:0.3646 Val F1:0.7759\n",
      "Epoch:309/500 Train Loss:0.3186 Val Loss:0.3633 Val F1:0.7757\n",
      "Epoch:310/500 Train Loss:0.3186 Val Loss:0.3616 Val F1:0.813\n",
      "Epoch:311/500 Train Loss:0.3185 Val Loss:0.3595 Val F1:0.8142\n",
      "Epoch:312/500 Train Loss:0.3186 Val Loss:0.358 Val F1:0.8214\n",
      "Epoch:313/500 Train Loss:0.3184 Val Loss:0.3634 Val F1:0.7916\n",
      "Epoch:314/500 Train Loss:0.3188 Val Loss:0.3652 Val F1:0.7759\n",
      "Epoch:315/500 Train Loss:0.3184 Val Loss:0.3607 Val F1:0.8014\n",
      "Epoch:316/500 Train Loss:0.3186 Val Loss:0.3669 Val F1:0.7828\n",
      "Epoch:317/500 Train Loss:0.3189 Val Loss:0.367 Val F1:0.776\n",
      "Epoch:318/500 Train Loss:0.3183 Val Loss:0.3631 Val F1:0.7844\n",
      "Epoch:319/500 Train Loss:0.3183 Val Loss:0.3624 Val F1:0.7933\n",
      "Epoch:320/500 Train Loss:0.319 Val Loss:0.3629 Val F1:0.788\n",
      "Epoch:321/500 Train Loss:0.3191 Val Loss:0.363 Val F1:0.7763\n",
      "Epoch:322/500 Train Loss:0.3187 Val Loss:0.365 Val F1:0.7637\n",
      "Epoch:323/500 Train Loss:0.3186 Val Loss:0.3668 Val F1:0.7758\n",
      "Epoch:324/500 Train Loss:0.3185 Val Loss:0.3664 Val F1:0.7854\n",
      "Epoch:325/500 Train Loss:0.3185 Val Loss:0.3684 Val F1:0.7519\n",
      "Epoch:326/500 Train Loss:0.3191 Val Loss:0.3717 Val F1:0.7301\n",
      "Epoch:327/500 Train Loss:0.3189 Val Loss:0.3687 Val F1:0.7852\n",
      "Epoch:328/500 Train Loss:0.3201 Val Loss:0.3696 Val F1:0.7948\n",
      "Epoch:329/500 Train Loss:0.3205 Val Loss:0.3676 Val F1:0.7768\n",
      "Epoch:330/500 Train Loss:0.3216 Val Loss:0.3785 Val F1:0.6875\n",
      "Epoch:331/500 Train Loss:0.3207 Val Loss:0.3698 Val F1:0.7807\n",
      "Epoch:332/500 Train Loss:0.3228 Val Loss:0.3751 Val F1:0.7688\n",
      "Epoch:333/500 Train Loss:0.3201 Val Loss:0.3757 Val F1:0.727\n",
      "Epoch:334/500 Train Loss:0.3232 Val Loss:0.3856 Val F1:0.7009\n",
      "Epoch:335/500 Train Loss:0.3218 Val Loss:0.3639 Val F1:0.7888\n",
      "Epoch:336/500 Train Loss:0.3206 Val Loss:0.3647 Val F1:0.8247\n",
      "Epoch:337/500 Train Loss:0.3195 Val Loss:0.3605 Val F1:0.8296\n",
      "Epoch:338/500 Train Loss:0.3197 Val Loss:0.3628 Val F1:0.7976\n",
      "Epoch:339/500 Train Loss:0.3191 Val Loss:0.3619 Val F1:0.8178\n",
      "Epoch:340/500 Train Loss:0.319 Val Loss:0.3628 Val F1:0.8007\n",
      "Epoch:341/500 Train Loss:0.3187 Val Loss:0.3629 Val F1:0.8011\n",
      "Epoch:342/500 Train Loss:0.3187 Val Loss:0.3635 Val F1:0.8106\n",
      "Epoch:343/500 Train Loss:0.319 Val Loss:0.3654 Val F1:0.7941\n",
      "Epoch:344/500 Train Loss:0.3189 Val Loss:0.365 Val F1:0.788\n",
      "Epoch:345/500 Train Loss:0.3183 Val Loss:0.3604 Val F1:0.7995\n",
      "Epoch:346/500 Train Loss:0.3185 Val Loss:0.3612 Val F1:0.7927\n",
      "Epoch:347/500 Train Loss:0.3185 Val Loss:0.3575 Val F1:0.8235\n",
      "Epoch:348/500 Train Loss:0.3183 Val Loss:0.358 Val F1:0.8118\n",
      "Epoch:349/500 Train Loss:0.3184 Val Loss:0.3574 Val F1:0.8318\n",
      "Epoch:350/500 Train Loss:0.3181 Val Loss:0.3595 Val F1:0.8184\n",
      "Epoch:351/500 Train Loss:0.3186 Val Loss:0.3612 Val F1:0.8201\n",
      "Epoch:352/500 Train Loss:0.3183 Val Loss:0.3589 Val F1:0.821\n",
      "Epoch:353/500 Train Loss:0.3184 Val Loss:0.365 Val F1:0.7837\n",
      "Epoch:354/500 Train Loss:0.3183 Val Loss:0.3645 Val F1:0.7786\n",
      "Epoch:355/500 Train Loss:0.3181 Val Loss:0.3658 Val F1:0.7833\n",
      "Epoch:356/500 Train Loss:0.3181 Val Loss:0.364 Val F1:0.7981\n",
      "Epoch:357/500 Train Loss:0.3184 Val Loss:0.3657 Val F1:0.7866\n",
      "Epoch:358/500 Train Loss:0.3182 Val Loss:0.3621 Val F1:0.7999\n",
      "Epoch:359/500 Train Loss:0.3184 Val Loss:0.3638 Val F1:0.7818\n",
      "Epoch:360/500 Train Loss:0.318 Val Loss:0.3644 Val F1:0.7831\n",
      "Epoch:361/500 Train Loss:0.3182 Val Loss:0.3628 Val F1:0.7914\n",
      "Epoch:362/500 Train Loss:0.3183 Val Loss:0.3629 Val F1:0.8044\n",
      "Epoch:363/500 Train Loss:0.3181 Val Loss:0.3651 Val F1:0.8036\n",
      "Epoch:364/500 Train Loss:0.3181 Val Loss:0.362 Val F1:0.8004\n",
      "Epoch:365/500 Train Loss:0.318 Val Loss:0.3627 Val F1:0.7743\n",
      "Epoch:366/500 Train Loss:0.3182 Val Loss:0.3648 Val F1:0.797\n",
      "Epoch:367/500 Train Loss:0.3181 Val Loss:0.3678 Val F1:0.7739\n",
      "Epoch:368/500 Train Loss:0.3181 Val Loss:0.3655 Val F1:0.7799\n",
      "Epoch:369/500 Train Loss:0.3182 Val Loss:0.3633 Val F1:0.7861\n",
      "Epoch:370/500 Train Loss:0.3182 Val Loss:0.3658 Val F1:0.7876\n",
      "Epoch:371/500 Train Loss:0.3181 Val Loss:0.3658 Val F1:0.7747\n",
      "Epoch:372/500 Train Loss:0.318 Val Loss:0.3649 Val F1:0.7781\n",
      "Epoch:373/500 Train Loss:0.318 Val Loss:0.3645 Val F1:0.767\n",
      "Epoch:374/500 Train Loss:0.3181 Val Loss:0.3664 Val F1:0.7503\n",
      "Epoch:375/500 Train Loss:0.3182 Val Loss:0.3659 Val F1:0.7612\n",
      "Epoch:376/500 Train Loss:0.3181 Val Loss:0.3644 Val F1:0.7671\n",
      "Epoch:377/500 Train Loss:0.3181 Val Loss:0.3648 Val F1:0.7752\n",
      "Epoch:378/500 Train Loss:0.3181 Val Loss:0.3628 Val F1:0.7819\n",
      "Epoch:379/500 Train Loss:0.3181 Val Loss:0.365 Val F1:0.7716\n",
      "Epoch:380/500 Train Loss:0.318 Val Loss:0.3625 Val F1:0.788\n",
      "Epoch:381/500 Train Loss:0.318 Val Loss:0.3669 Val F1:0.762\n",
      "Epoch:382/500 Train Loss:0.318 Val Loss:0.3651 Val F1:0.7747\n",
      "Epoch:383/500 Train Loss:0.318 Val Loss:0.3627 Val F1:0.7762\n",
      "Epoch:384/500 Train Loss:0.318 Val Loss:0.364 Val F1:0.7728\n",
      "Epoch:385/500 Train Loss:0.318 Val Loss:0.3663 Val F1:0.7772\n",
      "Epoch:386/500 Train Loss:0.318 Val Loss:0.3614 Val F1:0.7951\n",
      "Epoch:387/500 Train Loss:0.3181 Val Loss:0.3658 Val F1:0.7639\n",
      "Epoch:388/500 Train Loss:0.318 Val Loss:0.3639 Val F1:0.7803\n",
      "Epoch:389/500 Train Loss:0.318 Val Loss:0.3628 Val F1:0.781\n",
      "Epoch:390/500 Train Loss:0.318 Val Loss:0.3645 Val F1:0.7913\n",
      "Epoch:391/500 Train Loss:0.318 Val Loss:0.3651 Val F1:0.7645\n",
      "Epoch:392/500 Train Loss:0.318 Val Loss:0.3648 Val F1:0.7505\n",
      "Epoch:393/500 Train Loss:0.318 Val Loss:0.3646 Val F1:0.7729\n",
      "Epoch:394/500 Train Loss:0.318 Val Loss:0.3632 Val F1:0.7713\n",
      "Epoch:395/500 Train Loss:0.318 Val Loss:0.3654 Val F1:0.7666\n",
      "Epoch:396/500 Train Loss:0.318 Val Loss:0.368 Val F1:0.7634\n",
      "Epoch:397/500 Train Loss:0.3181 Val Loss:0.362 Val F1:0.7686\n",
      "Epoch:398/500 Train Loss:0.318 Val Loss:0.3659 Val F1:0.7604\n",
      "Epoch:399/500 Train Loss:0.318 Val Loss:0.3642 Val F1:0.7902\n",
      "Epoch:400/500 Train Loss:0.318 Val Loss:0.3638 Val F1:0.792\n",
      "Epoch:401/500 Train Loss:0.318 Val Loss:0.359 Val F1:0.8063\n",
      "Epoch:402/500 Train Loss:0.3181 Val Loss:0.3617 Val F1:0.7822\n",
      "Epoch:403/500 Train Loss:0.318 Val Loss:0.3638 Val F1:0.7884\n",
      "Epoch:404/500 Train Loss:0.318 Val Loss:0.3667 Val F1:0.7729\n",
      "Epoch:405/500 Train Loss:0.3181 Val Loss:0.365 Val F1:0.78\n",
      "Epoch:406/500 Train Loss:0.318 Val Loss:0.3675 Val F1:0.7707\n",
      "Epoch:407/500 Train Loss:0.318 Val Loss:0.3687 Val F1:0.7746\n",
      "Epoch:408/500 Train Loss:0.318 Val Loss:0.3666 Val F1:0.7717\n",
      "Epoch:409/500 Train Loss:0.3181 Val Loss:0.3683 Val F1:0.7741\n",
      "Epoch:410/500 Train Loss:0.3181 Val Loss:0.367 Val F1:0.7876\n",
      "Epoch:411/500 Train Loss:0.318 Val Loss:0.3676 Val F1:0.7669\n",
      "Epoch:412/500 Train Loss:0.3181 Val Loss:0.368 Val F1:0.7605\n",
      "Epoch:413/500 Train Loss:0.318 Val Loss:0.3647 Val F1:0.7628\n",
      "Epoch:414/500 Train Loss:0.3181 Val Loss:0.3664 Val F1:0.7663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:415/500 Train Loss:0.318 Val Loss:0.3686 Val F1:0.7612\n",
      "Epoch:416/500 Train Loss:0.318 Val Loss:0.3659 Val F1:0.7664\n",
      "Epoch:417/500 Train Loss:0.318 Val Loss:0.3652 Val F1:0.7757\n",
      "Epoch:418/500 Train Loss:0.3183 Val Loss:0.3644 Val F1:0.7831\n",
      "Epoch:419/500 Train Loss:0.318 Val Loss:0.3665 Val F1:0.7743\n",
      "Epoch:420/500 Train Loss:0.3181 Val Loss:0.364 Val F1:0.7804\n",
      "Epoch:421/500 Train Loss:0.318 Val Loss:0.3682 Val F1:0.7638\n",
      "Epoch:422/500 Train Loss:0.318 Val Loss:0.3642 Val F1:0.7842\n",
      "Epoch:423/500 Train Loss:0.318 Val Loss:0.3646 Val F1:0.7847\n",
      "Epoch:424/500 Train Loss:0.3181 Val Loss:0.3652 Val F1:0.7859\n",
      "Epoch:425/500 Train Loss:0.318 Val Loss:0.3651 Val F1:0.7657\n",
      "Epoch:426/500 Train Loss:0.3181 Val Loss:0.3708 Val F1:0.7317\n",
      "Epoch:427/500 Train Loss:0.3181 Val Loss:0.3669 Val F1:0.7603\n",
      "Epoch:428/500 Train Loss:0.318 Val Loss:0.3678 Val F1:0.7857\n",
      "Epoch:429/500 Train Loss:0.319 Val Loss:0.3679 Val F1:0.7685\n",
      "Epoch:430/500 Train Loss:0.318 Val Loss:0.3706 Val F1:0.7473\n",
      "Epoch:431/500 Train Loss:0.319 Val Loss:0.3665 Val F1:0.7814\n",
      "Epoch:432/500 Train Loss:0.3183 Val Loss:0.367 Val F1:0.8046\n",
      "Epoch:433/500 Train Loss:0.3189 Val Loss:0.3689 Val F1:0.7924\n",
      "Epoch:434/500 Train Loss:0.3187 Val Loss:0.3636 Val F1:0.7866\n",
      "Epoch:435/500 Train Loss:0.3184 Val Loss:0.3669 Val F1:0.7446\n",
      "Epoch:436/500 Train Loss:0.3186 Val Loss:0.3752 Val F1:0.71\n",
      "Epoch:437/500 Train Loss:0.3184 Val Loss:0.3722 Val F1:0.7341\n",
      "Epoch:438/500 Train Loss:0.3181 Val Loss:0.3719 Val F1:0.7416\n",
      "Epoch:439/500 Train Loss:0.3182 Val Loss:0.3661 Val F1:0.7645\n",
      "Epoch:440/500 Train Loss:0.3182 Val Loss:0.3679 Val F1:0.7664\n",
      "Epoch:441/500 Train Loss:0.3183 Val Loss:0.3648 Val F1:0.7688\n",
      "Epoch:442/500 Train Loss:0.318 Val Loss:0.367 Val F1:0.7674\n",
      "Epoch:443/500 Train Loss:0.318 Val Loss:0.3605 Val F1:0.7836\n",
      "Epoch:444/500 Train Loss:0.318 Val Loss:0.3677 Val F1:0.7598\n",
      "Epoch:445/500 Train Loss:0.318 Val Loss:0.3607 Val F1:0.7742\n",
      "Epoch:446/500 Train Loss:0.3181 Val Loss:0.3653 Val F1:0.7769\n",
      "Epoch:447/500 Train Loss:0.318 Val Loss:0.3668 Val F1:0.7305\n",
      "Epoch:448/500 Train Loss:0.3182 Val Loss:0.3652 Val F1:0.7541\n",
      "Epoch:449/500 Train Loss:0.318 Val Loss:0.3676 Val F1:0.7611\n",
      "Epoch:450/500 Train Loss:0.3182 Val Loss:0.3686 Val F1:0.7523\n",
      "Epoch:451/500 Train Loss:0.3184 Val Loss:0.3699 Val F1:0.7383\n",
      "Epoch:452/500 Train Loss:0.3181 Val Loss:0.3658 Val F1:0.7662\n",
      "Epoch:453/500 Train Loss:0.318 Val Loss:0.3636 Val F1:0.7674\n",
      "Epoch:454/500 Train Loss:0.318 Val Loss:0.3659 Val F1:0.7595\n",
      "Epoch:455/500 Train Loss:0.3181 Val Loss:0.3653 Val F1:0.7578\n",
      "Epoch:456/500 Train Loss:0.318 Val Loss:0.3661 Val F1:0.7665\n",
      "Epoch:457/500 Train Loss:0.3181 Val Loss:0.3631 Val F1:0.7667\n",
      "Epoch:458/500 Train Loss:0.318 Val Loss:0.3623 Val F1:0.7645\n",
      "Epoch:459/500 Train Loss:0.318 Val Loss:0.363 Val F1:0.7612\n",
      "Epoch:460/500 Train Loss:0.318 Val Loss:0.3683 Val F1:0.7485\n",
      "Epoch:461/500 Train Loss:0.318 Val Loss:0.3671 Val F1:0.7544\n",
      "Epoch:462/500 Train Loss:0.318 Val Loss:0.3628 Val F1:0.7626\n",
      "Epoch:463/500 Train Loss:0.318 Val Loss:0.3626 Val F1:0.7579\n",
      "Epoch:464/500 Train Loss:0.318 Val Loss:0.363 Val F1:0.7553\n",
      "Epoch:465/500 Train Loss:0.318 Val Loss:0.3649 Val F1:0.7574\n",
      "Epoch:466/500 Train Loss:0.318 Val Loss:0.3658 Val F1:0.7596\n",
      "Epoch:467/500 Train Loss:0.318 Val Loss:0.3656 Val F1:0.7602\n",
      "Epoch:468/500 Train Loss:0.318 Val Loss:0.3597 Val F1:0.7845\n",
      "Epoch:469/500 Train Loss:0.3181 Val Loss:0.3662 Val F1:0.7476\n",
      "Epoch:470/500 Train Loss:0.318 Val Loss:0.3658 Val F1:0.7546\n",
      "Epoch:471/500 Train Loss:0.318 Val Loss:0.3653 Val F1:0.7488\n",
      "Epoch:472/500 Train Loss:0.318 Val Loss:0.3632 Val F1:0.7773\n",
      "Epoch:473/500 Train Loss:0.318 Val Loss:0.361 Val F1:0.7728\n",
      "Epoch:474/500 Train Loss:0.318 Val Loss:0.3645 Val F1:0.7633\n",
      "Epoch:475/500 Train Loss:0.318 Val Loss:0.366 Val F1:0.7565\n",
      "Epoch:476/500 Train Loss:0.318 Val Loss:0.3649 Val F1:0.7598\n",
      "Epoch:477/500 Train Loss:0.318 Val Loss:0.3636 Val F1:0.7678\n",
      "Epoch:478/500 Train Loss:0.318 Val Loss:0.3658 Val F1:0.7481\n",
      "Epoch:479/500 Train Loss:0.318 Val Loss:0.3642 Val F1:0.7601\n",
      "Epoch:480/500 Train Loss:0.318 Val Loss:0.365 Val F1:0.7682\n",
      "Epoch:481/500 Train Loss:0.318 Val Loss:0.3639 Val F1:0.7629\n",
      "Epoch:482/500 Train Loss:0.318 Val Loss:0.3636 Val F1:0.7773\n",
      "Epoch:483/500 Train Loss:0.318 Val Loss:0.366 Val F1:0.7529\n",
      "Epoch:484/500 Train Loss:0.318 Val Loss:0.3661 Val F1:0.7664\n",
      "Epoch:485/500 Train Loss:0.318 Val Loss:0.3674 Val F1:0.7564\n",
      "Epoch:486/500 Train Loss:0.318 Val Loss:0.3684 Val F1:0.7425\n",
      "Epoch:487/500 Train Loss:0.318 Val Loss:0.3664 Val F1:0.7477\n",
      "Epoch:488/500 Train Loss:0.318 Val Loss:0.3644 Val F1:0.7656\n",
      "Epoch:489/500 Train Loss:0.318 Val Loss:0.3646 Val F1:0.7709\n",
      "Epoch:490/500 Train Loss:0.318 Val Loss:0.364 Val F1:0.763\n",
      "Epoch:491/500 Train Loss:0.318 Val Loss:0.3644 Val F1:0.7559\n",
      "Epoch:492/500 Train Loss:0.318 Val Loss:0.365 Val F1:0.7745\n",
      "Epoch:493/500 Train Loss:0.318 Val Loss:0.3659 Val F1:0.7583\n",
      "Epoch:494/500 Train Loss:0.318 Val Loss:0.3625 Val F1:0.7836\n",
      "Epoch:495/500 Train Loss:0.318 Val Loss:0.3667 Val F1:0.7681\n",
      "Epoch:496/500 Train Loss:0.318 Val Loss:0.3666 Val F1:0.7629\n",
      "Epoch:497/500 Train Loss:0.318 Val Loss:0.3675 Val F1:0.7628\n",
      "Epoch:498/500 Train Loss:0.318 Val Loss:0.3661 Val F1:0.7546\n",
      "Epoch:499/500 Train Loss:0.318 Val Loss:0.3666 Val F1:0.7496\n",
      "Epoch:500/500 Train Loss:0.318 Val Loss:0.3656 Val F1:0.757\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_encoder = None\n",
    "best_mlp = None\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = train(encoder, mlp, optimizer, train_loader, loss_fn, device)\n",
    "    val_loss, val_f1 = test(encoder, mlp, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch:{e}/{epochs} Train Loss:{round(train_loss,4)} Val Loss:{round(val_loss,4)} Val F1:{round(val_f1, 4)}\")\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_encoder = deepcopy(encoder)\n",
    "        best_mlp = deepcopy(mlp)\n",
    "\n",
    "encoder = deepcopy(best_encoder)\n",
    "mlp = deepcopy(best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d8b5ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8317731833793521\n"
     ]
    }
   ],
   "source": [
    "print(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc978347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8ea56b5",
   "metadata": {},
   "source": [
    "## Prequential Evaluation on Subsequent Time Steps ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a967c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list = []\n",
    "f1_list.append(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6720d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_classifier = DomainClassifier(emb_dim, emb_dim // 4, 2).to(device)\n",
    "loss_fn_class = nn.CrossEntropyLoss()\n",
    "loss_fn_domain = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903eab0d",
   "metadata": {},
   "source": [
    "### 9-13 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af0b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [get_data(data_dir, \"elliptic\", i) for i in range(9,14)]\n",
    "test_loader_9_14 = DataLoader(dataset=test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98177228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.361, Test F1: 0.7124\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = test(encoder, mlp, test_loader_9_14, loss_fn, device)\n",
    "f1_list.append(test_f1)\n",
    "print(f\"Test Loss: {round(test_loss,4)}, Test F1: {round(test_f1,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe39ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_9_14, mlp_9_14, domain_classifier_9_14 = deepcopy(encoder), deepcopy(mlp), deepcopy(domain_classifier)\n",
    "optimizer = torch.optim.Adam(list(encoder_9_14.parameters()) + list(mlp_9_14.parameters()) + list(domain_classifier_9_14.parameters()), lr=1e-3)\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aad4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0/1000 Source Label Loss:0.318 Source Domain Loss:0.645 Target Domain Loss:0.746\n",
      "Epoch:1/1000 Source Label Loss:0.317 Source Domain Loss:0.541 Target Domain Loss:0.813\n",
      "Epoch:2/1000 Source Label Loss:0.319 Source Domain Loss:0.526 Target Domain Loss:0.852\n",
      "Epoch:3/1000 Source Label Loss:0.318 Source Domain Loss:0.601 Target Domain Loss:0.814\n",
      "Epoch:4/1000 Source Label Loss:0.317 Source Domain Loss:0.652 Target Domain Loss:0.792\n",
      "Epoch:5/1000 Source Label Loss:0.317 Source Domain Loss:0.665 Target Domain Loss:0.805\n",
      "Epoch:6/1000 Source Label Loss:0.318 Source Domain Loss:0.643 Target Domain Loss:0.831\n",
      "Epoch:7/1000 Source Label Loss:0.318 Source Domain Loss:0.628 Target Domain Loss:0.846\n",
      "Epoch:8/1000 Source Label Loss:0.317 Source Domain Loss:0.602 Target Domain Loss:0.834\n",
      "Epoch:9/1000 Source Label Loss:0.319 Source Domain Loss:0.628 Target Domain Loss:0.791\n",
      "Epoch:10/1000 Source Label Loss:0.319 Source Domain Loss:0.662 Target Domain Loss:0.724\n",
      "Epoch:11/1000 Source Label Loss:0.319 Source Domain Loss:0.699 Target Domain Loss:0.677\n",
      "Epoch:12/1000 Source Label Loss:0.32 Source Domain Loss:0.71 Target Domain Loss:0.673\n",
      "Epoch:13/1000 Source Label Loss:0.32 Source Domain Loss:0.708 Target Domain Loss:0.69\n",
      "Epoch:14/1000 Source Label Loss:0.319 Source Domain Loss:0.704 Target Domain Loss:0.715\n",
      "Epoch:15/1000 Source Label Loss:0.317 Source Domain Loss:0.683 Target Domain Loss:0.747\n",
      "Epoch:16/1000 Source Label Loss:0.317 Source Domain Loss:0.66 Target Domain Loss:0.776\n",
      "Epoch:17/1000 Source Label Loss:0.318 Source Domain Loss:0.624 Target Domain Loss:0.808\n",
      "Epoch:18/1000 Source Label Loss:0.322 Source Domain Loss:0.575 Target Domain Loss:0.838\n",
      "Epoch:19/1000 Source Label Loss:0.322 Source Domain Loss:0.596 Target Domain Loss:0.842\n",
      "Epoch:20/1000 Source Label Loss:0.321 Source Domain Loss:0.698 Target Domain Loss:0.788\n",
      "Epoch:21/1000 Source Label Loss:0.318 Source Domain Loss:0.836 Target Domain Loss:0.702\n",
      "Epoch:22/1000 Source Label Loss:0.32 Source Domain Loss:0.913 Target Domain Loss:0.641\n",
      "Epoch:23/1000 Source Label Loss:0.32 Source Domain Loss:1.005 Target Domain Loss:0.623\n",
      "Epoch:24/1000 Source Label Loss:0.321 Source Domain Loss:1.063 Target Domain Loss:0.629\n",
      "Epoch:25/1000 Source Label Loss:0.32 Source Domain Loss:1.111 Target Domain Loss:0.626\n",
      "Epoch:26/1000 Source Label Loss:0.319 Source Domain Loss:1.139 Target Domain Loss:0.618\n",
      "Epoch:27/1000 Source Label Loss:0.32 Source Domain Loss:1.158 Target Domain Loss:0.607\n",
      "Epoch:28/1000 Source Label Loss:0.319 Source Domain Loss:1.175 Target Domain Loss:0.593\n",
      "Epoch:29/1000 Source Label Loss:0.319 Source Domain Loss:1.188 Target Domain Loss:0.578\n",
      "Epoch:30/1000 Source Label Loss:0.319 Source Domain Loss:1.207 Target Domain Loss:0.568\n",
      "Epoch:31/1000 Source Label Loss:0.32 Source Domain Loss:1.196 Target Domain Loss:0.557\n",
      "Epoch:32/1000 Source Label Loss:0.319 Source Domain Loss:1.161 Target Domain Loss:0.554\n",
      "Epoch:33/1000 Source Label Loss:0.319 Source Domain Loss:1.072 Target Domain Loss:0.583\n",
      "Epoch:34/1000 Source Label Loss:0.319 Source Domain Loss:0.78 Target Domain Loss:0.683\n",
      "Epoch:35/1000 Source Label Loss:0.317 Source Domain Loss:0.484 Target Domain Loss:0.854\n",
      "Epoch:36/1000 Source Label Loss:0.323 Source Domain Loss:0.402 Target Domain Loss:0.97\n",
      "Epoch:37/1000 Source Label Loss:0.323 Source Domain Loss:0.383 Target Domain Loss:1.035\n",
      "Epoch:38/1000 Source Label Loss:0.322 Source Domain Loss:0.385 Target Domain Loss:1.091\n",
      "Epoch:39/1000 Source Label Loss:0.32 Source Domain Loss:0.392 Target Domain Loss:1.126\n",
      "Epoch:40/1000 Source Label Loss:0.32 Source Domain Loss:0.4 Target Domain Loss:1.141\n",
      "Epoch:41/1000 Source Label Loss:0.32 Source Domain Loss:0.42 Target Domain Loss:1.151\n",
      "Epoch:42/1000 Source Label Loss:0.318 Source Domain Loss:0.399 Target Domain Loss:1.166\n",
      "Epoch:43/1000 Source Label Loss:0.318 Source Domain Loss:0.391 Target Domain Loss:1.182\n",
      "Epoch:44/1000 Source Label Loss:0.319 Source Domain Loss:0.387 Target Domain Loss:1.198\n",
      "Epoch:45/1000 Source Label Loss:0.318 Source Domain Loss:0.384 Target Domain Loss:1.214\n",
      "Epoch:46/1000 Source Label Loss:0.319 Source Domain Loss:0.37 Target Domain Loss:1.227\n",
      "Epoch:47/1000 Source Label Loss:0.317 Source Domain Loss:0.362 Target Domain Loss:1.236\n",
      "Epoch:48/1000 Source Label Loss:0.319 Source Domain Loss:0.364 Target Domain Loss:1.244\n",
      "Epoch:49/1000 Source Label Loss:0.318 Source Domain Loss:0.37 Target Domain Loss:1.251\n",
      "Epoch:50/1000 Source Label Loss:0.319 Source Domain Loss:0.367 Target Domain Loss:1.254\n",
      "Epoch:51/1000 Source Label Loss:0.318 Source Domain Loss:0.351 Target Domain Loss:1.254\n",
      "Epoch:52/1000 Source Label Loss:0.32 Source Domain Loss:0.372 Target Domain Loss:1.251\n",
      "Epoch:53/1000 Source Label Loss:0.319 Source Domain Loss:0.376 Target Domain Loss:1.244\n",
      "Epoch:54/1000 Source Label Loss:0.32 Source Domain Loss:0.383 Target Domain Loss:1.229\n",
      "Epoch:55/1000 Source Label Loss:0.319 Source Domain Loss:0.417 Target Domain Loss:1.194\n",
      "Epoch:56/1000 Source Label Loss:0.32 Source Domain Loss:0.452 Target Domain Loss:1.151\n",
      "Epoch:57/1000 Source Label Loss:0.323 Source Domain Loss:0.465 Target Domain Loss:1.122\n",
      "Epoch:58/1000 Source Label Loss:0.322 Source Domain Loss:0.481 Target Domain Loss:1.017\n",
      "Epoch:59/1000 Source Label Loss:0.323 Source Domain Loss:0.602 Target Domain Loss:0.75\n",
      "Epoch:60/1000 Source Label Loss:0.322 Source Domain Loss:0.712 Target Domain Loss:0.61\n",
      "Epoch:61/1000 Source Label Loss:0.322 Source Domain Loss:0.712 Target Domain Loss:0.637\n",
      "Epoch:62/1000 Source Label Loss:0.323 Source Domain Loss:0.709 Target Domain Loss:0.701\n",
      "Epoch:63/1000 Source Label Loss:0.318 Source Domain Loss:0.692 Target Domain Loss:0.759\n",
      "Epoch:64/1000 Source Label Loss:0.326 Source Domain Loss:0.696 Target Domain Loss:0.816\n",
      "Epoch:65/1000 Source Label Loss:0.326 Source Domain Loss:0.681 Target Domain Loss:0.897\n",
      "Epoch:66/1000 Source Label Loss:0.318 Source Domain Loss:0.651 Target Domain Loss:0.979\n",
      "Epoch:67/1000 Source Label Loss:0.327 Source Domain Loss:0.662 Target Domain Loss:1.067\n",
      "Epoch:68/1000 Source Label Loss:0.327 Source Domain Loss:0.644 Target Domain Loss:1.124\n",
      "Epoch:69/1000 Source Label Loss:0.319 Source Domain Loss:0.642 Target Domain Loss:1.172\n",
      "Epoch:70/1000 Source Label Loss:0.329 Source Domain Loss:0.579 Target Domain Loss:1.211\n",
      "Epoch:71/1000 Source Label Loss:0.319 Source Domain Loss:0.585 Target Domain Loss:1.231\n",
      "Epoch:72/1000 Source Label Loss:0.319 Source Domain Loss:0.526 Target Domain Loss:1.249\n",
      "Epoch:73/1000 Source Label Loss:0.332 Source Domain Loss:0.429 Target Domain Loss:1.266\n",
      "Epoch:74/1000 Source Label Loss:0.332 Source Domain Loss:0.349 Target Domain Loss:1.282\n",
      "Epoch:75/1000 Source Label Loss:0.331 Source Domain Loss:0.329 Target Domain Loss:1.29\n",
      "Epoch:76/1000 Source Label Loss:0.331 Source Domain Loss:0.327 Target Domain Loss:1.294\n",
      "Epoch:77/1000 Source Label Loss:0.319 Source Domain Loss:0.326 Target Domain Loss:1.297\n",
      "Epoch:78/1000 Source Label Loss:0.33 Source Domain Loss:0.326 Target Domain Loss:1.299\n",
      "Epoch:79/1000 Source Label Loss:0.33 Source Domain Loss:0.325 Target Domain Loss:1.301\n",
      "Epoch:80/1000 Source Label Loss:0.329 Source Domain Loss:0.325 Target Domain Loss:1.302\n",
      "Epoch:81/1000 Source Label Loss:0.319 Source Domain Loss:0.324 Target Domain Loss:1.303\n",
      "Epoch:82/1000 Source Label Loss:0.328 Source Domain Loss:0.321 Target Domain Loss:1.305\n",
      "Epoch:83/1000 Source Label Loss:0.319 Source Domain Loss:0.32 Target Domain Loss:1.306\n",
      "Epoch:84/1000 Source Label Loss:0.329 Source Domain Loss:0.32 Target Domain Loss:1.307\n",
      "Epoch:85/1000 Source Label Loss:0.328 Source Domain Loss:0.319 Target Domain Loss:1.308\n",
      "Epoch:86/1000 Source Label Loss:0.328 Source Domain Loss:0.32 Target Domain Loss:1.308\n",
      "Epoch:87/1000 Source Label Loss:0.318 Source Domain Loss:0.32 Target Domain Loss:1.309\n",
      "Epoch:88/1000 Source Label Loss:0.327 Source Domain Loss:0.32 Target Domain Loss:1.309\n",
      "Epoch:89/1000 Source Label Loss:0.319 Source Domain Loss:0.32 Target Domain Loss:1.309\n",
      "Epoch:90/1000 Source Label Loss:0.318 Source Domain Loss:0.322 Target Domain Loss:1.309\n",
      "Epoch:91/1000 Source Label Loss:0.318 Source Domain Loss:0.32 Target Domain Loss:1.31\n",
      "Epoch:92/1000 Source Label Loss:0.326 Source Domain Loss:0.322 Target Domain Loss:1.31\n",
      "Epoch:93/1000 Source Label Loss:0.326 Source Domain Loss:0.323 Target Domain Loss:1.31\n",
      "Epoch:94/1000 Source Label Loss:0.326 Source Domain Loss:0.321 Target Domain Loss:1.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:95/1000 Source Label Loss:0.325 Source Domain Loss:0.323 Target Domain Loss:1.309\n",
      "Epoch:96/1000 Source Label Loss:0.327 Source Domain Loss:0.325 Target Domain Loss:1.309\n",
      "Epoch:97/1000 Source Label Loss:0.326 Source Domain Loss:0.326 Target Domain Loss:1.309\n",
      "Epoch:98/1000 Source Label Loss:0.318 Source Domain Loss:0.327 Target Domain Loss:1.309\n",
      "Epoch:99/1000 Source Label Loss:0.318 Source Domain Loss:0.33 Target Domain Loss:1.308\n",
      "Epoch:100/1000 Source Label Loss:0.319 Source Domain Loss:0.33 Target Domain Loss:1.308\n",
      "Epoch:101/1000 Source Label Loss:0.325 Source Domain Loss:0.332 Target Domain Loss:1.308\n",
      "Epoch:102/1000 Source Label Loss:0.324 Source Domain Loss:0.331 Target Domain Loss:1.308\n",
      "Epoch:103/1000 Source Label Loss:0.325 Source Domain Loss:0.336 Target Domain Loss:1.307\n",
      "Epoch:104/1000 Source Label Loss:0.325 Source Domain Loss:0.337 Target Domain Loss:1.306\n",
      "Epoch:105/1000 Source Label Loss:0.326 Source Domain Loss:0.34 Target Domain Loss:1.305\n",
      "Epoch:106/1000 Source Label Loss:0.319 Source Domain Loss:0.35 Target Domain Loss:1.304\n",
      "Epoch:107/1000 Source Label Loss:0.326 Source Domain Loss:0.352 Target Domain Loss:1.303\n",
      "Epoch:108/1000 Source Label Loss:0.324 Source Domain Loss:0.345 Target Domain Loss:1.302\n",
      "Epoch:109/1000 Source Label Loss:0.324 Source Domain Loss:0.354 Target Domain Loss:1.301\n",
      "Epoch:110/1000 Source Label Loss:0.326 Source Domain Loss:0.364 Target Domain Loss:1.296\n",
      "Epoch:111/1000 Source Label Loss:0.324 Source Domain Loss:0.37 Target Domain Loss:1.294\n",
      "Epoch:112/1000 Source Label Loss:0.328 Source Domain Loss:0.368 Target Domain Loss:1.294\n",
      "Epoch:113/1000 Source Label Loss:0.329 Source Domain Loss:0.352 Target Domain Loss:1.295\n",
      "Epoch:114/1000 Source Label Loss:0.331 Source Domain Loss:0.354 Target Domain Loss:1.298\n",
      "Epoch:115/1000 Source Label Loss:0.332 Source Domain Loss:0.337 Target Domain Loss:1.303\n",
      "Epoch:116/1000 Source Label Loss:0.33 Source Domain Loss:0.329 Target Domain Loss:1.306\n",
      "Epoch:117/1000 Source Label Loss:0.327 Source Domain Loss:0.325 Target Domain Loss:1.308\n",
      "Epoch:118/1000 Source Label Loss:0.325 Source Domain Loss:0.323 Target Domain Loss:1.308\n",
      "Epoch:119/1000 Source Label Loss:0.326 Source Domain Loss:0.329 Target Domain Loss:1.308\n",
      "Epoch:120/1000 Source Label Loss:0.32 Source Domain Loss:0.333 Target Domain Loss:1.308\n",
      "Epoch:121/1000 Source Label Loss:0.319 Source Domain Loss:0.335 Target Domain Loss:1.308\n",
      "Epoch:122/1000 Source Label Loss:0.323 Source Domain Loss:0.34 Target Domain Loss:1.307\n",
      "Epoch:123/1000 Source Label Loss:0.318 Source Domain Loss:0.339 Target Domain Loss:1.305\n",
      "Epoch:124/1000 Source Label Loss:0.319 Source Domain Loss:0.35 Target Domain Loss:1.302\n",
      "Epoch:125/1000 Source Label Loss:0.324 Source Domain Loss:0.355 Target Domain Loss:1.3\n",
      "Epoch:126/1000 Source Label Loss:0.324 Source Domain Loss:0.379 Target Domain Loss:1.299\n",
      "Epoch:127/1000 Source Label Loss:0.328 Source Domain Loss:0.376 Target Domain Loss:1.298\n",
      "Epoch:128/1000 Source Label Loss:0.326 Source Domain Loss:0.354 Target Domain Loss:1.301\n",
      "Epoch:129/1000 Source Label Loss:0.323 Source Domain Loss:0.343 Target Domain Loss:1.303\n",
      "Epoch:130/1000 Source Label Loss:0.325 Source Domain Loss:0.343 Target Domain Loss:1.303\n",
      "Epoch:131/1000 Source Label Loss:0.325 Source Domain Loss:0.35 Target Domain Loss:1.303\n",
      "Epoch:132/1000 Source Label Loss:0.32 Source Domain Loss:0.356 Target Domain Loss:1.304\n",
      "Epoch:133/1000 Source Label Loss:0.318 Source Domain Loss:0.367 Target Domain Loss:1.303\n",
      "Epoch:134/1000 Source Label Loss:0.323 Source Domain Loss:0.356 Target Domain Loss:1.302\n",
      "Epoch:135/1000 Source Label Loss:0.322 Source Domain Loss:0.383 Target Domain Loss:1.297\n",
      "Epoch:136/1000 Source Label Loss:0.324 Source Domain Loss:0.387 Target Domain Loss:1.291\n",
      "Epoch:137/1000 Source Label Loss:0.328 Source Domain Loss:0.416 Target Domain Loss:1.286\n",
      "Epoch:138/1000 Source Label Loss:0.33 Source Domain Loss:0.394 Target Domain Loss:1.292\n",
      "Epoch:139/1000 Source Label Loss:0.328 Source Domain Loss:0.345 Target Domain Loss:1.302\n",
      "Epoch:140/1000 Source Label Loss:0.318 Source Domain Loss:0.33 Target Domain Loss:1.308\n",
      "Epoch:141/1000 Source Label Loss:0.318 Source Domain Loss:0.329 Target Domain Loss:1.31\n",
      "Epoch:142/1000 Source Label Loss:0.325 Source Domain Loss:0.331 Target Domain Loss:1.309\n",
      "Epoch:143/1000 Source Label Loss:0.322 Source Domain Loss:0.337 Target Domain Loss:1.308\n",
      "Epoch:144/1000 Source Label Loss:0.325 Source Domain Loss:0.355 Target Domain Loss:1.305\n",
      "Epoch:145/1000 Source Label Loss:0.321 Source Domain Loss:0.36 Target Domain Loss:1.302\n",
      "Epoch:146/1000 Source Label Loss:0.318 Source Domain Loss:0.405 Target Domain Loss:1.297\n",
      "Epoch:147/1000 Source Label Loss:0.32 Source Domain Loss:0.401 Target Domain Loss:1.292\n",
      "Epoch:148/1000 Source Label Loss:0.32 Source Domain Loss:0.4 Target Domain Loss:1.293\n",
      "Epoch:149/1000 Source Label Loss:0.326 Source Domain Loss:0.36 Target Domain Loss:1.301\n",
      "Epoch:150/1000 Source Label Loss:0.322 Source Domain Loss:0.339 Target Domain Loss:1.306\n",
      "Epoch:151/1000 Source Label Loss:0.32 Source Domain Loss:0.34 Target Domain Loss:1.308\n",
      "Epoch:152/1000 Source Label Loss:0.319 Source Domain Loss:0.364 Target Domain Loss:1.304\n",
      "Epoch:153/1000 Source Label Loss:0.32 Source Domain Loss:0.391 Target Domain Loss:1.298\n",
      "Epoch:154/1000 Source Label Loss:0.321 Source Domain Loss:0.413 Target Domain Loss:1.289\n",
      "Epoch:155/1000 Source Label Loss:0.322 Source Domain Loss:0.434 Target Domain Loss:1.292\n",
      "Epoch:156/1000 Source Label Loss:0.327 Source Domain Loss:0.438 Target Domain Loss:1.288\n",
      "Epoch:157/1000 Source Label Loss:0.333 Source Domain Loss:0.473 Target Domain Loss:1.284\n",
      "Epoch:158/1000 Source Label Loss:0.327 Source Domain Loss:0.49 Target Domain Loss:1.285\n",
      "Epoch:159/1000 Source Label Loss:0.326 Source Domain Loss:0.522 Target Domain Loss:1.281\n",
      "Epoch:160/1000 Source Label Loss:0.333 Source Domain Loss:0.371 Target Domain Loss:1.298\n",
      "Epoch:161/1000 Source Label Loss:0.326 Source Domain Loss:0.327 Target Domain Loss:1.308\n",
      "Epoch:162/1000 Source Label Loss:0.319 Source Domain Loss:0.322 Target Domain Loss:1.309\n",
      "Epoch:163/1000 Source Label Loss:0.325 Source Domain Loss:0.321 Target Domain Loss:1.31\n",
      "Epoch:164/1000 Source Label Loss:0.323 Source Domain Loss:0.329 Target Domain Loss:1.309\n",
      "Epoch:165/1000 Source Label Loss:0.32 Source Domain Loss:0.342 Target Domain Loss:1.307\n",
      "Epoch:166/1000 Source Label Loss:0.322 Source Domain Loss:0.36 Target Domain Loss:1.304\n",
      "Epoch:167/1000 Source Label Loss:0.321 Source Domain Loss:0.38 Target Domain Loss:1.301\n",
      "Epoch:168/1000 Source Label Loss:0.321 Source Domain Loss:0.361 Target Domain Loss:1.303\n",
      "Epoch:169/1000 Source Label Loss:0.317 Source Domain Loss:0.336 Target Domain Loss:1.307\n",
      "Epoch:170/1000 Source Label Loss:0.317 Source Domain Loss:0.324 Target Domain Loss:1.31\n",
      "Epoch:171/1000 Source Label Loss:0.321 Source Domain Loss:0.319 Target Domain Loss:1.311\n",
      "Epoch:172/1000 Source Label Loss:0.318 Source Domain Loss:0.319 Target Domain Loss:1.311\n",
      "Epoch:173/1000 Source Label Loss:0.321 Source Domain Loss:0.318 Target Domain Loss:1.312\n",
      "Epoch:174/1000 Source Label Loss:0.317 Source Domain Loss:0.319 Target Domain Loss:1.312\n",
      "Epoch:175/1000 Source Label Loss:0.32 Source Domain Loss:0.319 Target Domain Loss:1.312\n",
      "Epoch:176/1000 Source Label Loss:0.32 Source Domain Loss:0.322 Target Domain Loss:1.311\n",
      "Epoch:177/1000 Source Label Loss:0.32 Source Domain Loss:0.326 Target Domain Loss:1.311\n",
      "Epoch:178/1000 Source Label Loss:0.32 Source Domain Loss:0.329 Target Domain Loss:1.309\n",
      "Epoch:179/1000 Source Label Loss:0.321 Source Domain Loss:0.334 Target Domain Loss:1.308\n",
      "Epoch:180/1000 Source Label Loss:0.319 Source Domain Loss:0.355 Target Domain Loss:1.307\n",
      "Epoch:181/1000 Source Label Loss:0.324 Source Domain Loss:0.384 Target Domain Loss:1.304\n",
      "Epoch:182/1000 Source Label Loss:0.325 Source Domain Loss:0.41 Target Domain Loss:1.302\n",
      "Epoch:183/1000 Source Label Loss:0.322 Source Domain Loss:0.391 Target Domain Loss:1.302\n",
      "Epoch:184/1000 Source Label Loss:0.322 Source Domain Loss:0.37 Target Domain Loss:1.305\n",
      "Epoch:185/1000 Source Label Loss:0.324 Source Domain Loss:0.377 Target Domain Loss:1.305\n",
      "Epoch:186/1000 Source Label Loss:0.324 Source Domain Loss:0.376 Target Domain Loss:1.305\n",
      "Epoch:187/1000 Source Label Loss:0.325 Source Domain Loss:0.345 Target Domain Loss:1.309\n",
      "Epoch:188/1000 Source Label Loss:0.319 Source Domain Loss:0.329 Target Domain Loss:1.311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:189/1000 Source Label Loss:0.321 Source Domain Loss:0.322 Target Domain Loss:1.311\n",
      "Epoch:190/1000 Source Label Loss:0.321 Source Domain Loss:0.325 Target Domain Loss:1.311\n",
      "Epoch:191/1000 Source Label Loss:0.319 Source Domain Loss:0.329 Target Domain Loss:1.311\n",
      "Epoch:192/1000 Source Label Loss:0.322 Source Domain Loss:0.331 Target Domain Loss:1.311\n",
      "Epoch:193/1000 Source Label Loss:0.322 Source Domain Loss:0.342 Target Domain Loss:1.31\n",
      "Epoch:194/1000 Source Label Loss:0.324 Source Domain Loss:0.349 Target Domain Loss:1.309\n",
      "Epoch:195/1000 Source Label Loss:0.322 Source Domain Loss:0.363 Target Domain Loss:1.306\n",
      "Epoch:196/1000 Source Label Loss:0.323 Source Domain Loss:0.388 Target Domain Loss:1.303\n",
      "Epoch:197/1000 Source Label Loss:0.321 Source Domain Loss:0.391 Target Domain Loss:1.302\n",
      "Epoch:198/1000 Source Label Loss:0.322 Source Domain Loss:0.375 Target Domain Loss:1.304\n",
      "Epoch:199/1000 Source Label Loss:0.321 Source Domain Loss:0.35 Target Domain Loss:1.306\n",
      "Epoch:200/1000 Source Label Loss:0.322 Source Domain Loss:0.332 Target Domain Loss:1.309\n",
      "Epoch:201/1000 Source Label Loss:0.318 Source Domain Loss:0.324 Target Domain Loss:1.31\n",
      "Epoch:202/1000 Source Label Loss:0.321 Source Domain Loss:0.323 Target Domain Loss:1.311\n",
      "Epoch:203/1000 Source Label Loss:0.32 Source Domain Loss:0.323 Target Domain Loss:1.311\n",
      "Epoch:204/1000 Source Label Loss:0.32 Source Domain Loss:0.329 Target Domain Loss:1.31\n",
      "Epoch:205/1000 Source Label Loss:0.321 Source Domain Loss:0.34 Target Domain Loss:1.308\n",
      "Epoch:206/1000 Source Label Loss:0.319 Source Domain Loss:0.36 Target Domain Loss:1.304\n",
      "Epoch:207/1000 Source Label Loss:0.32 Source Domain Loss:0.393 Target Domain Loss:1.3\n",
      "Epoch:208/1000 Source Label Loss:0.317 Source Domain Loss:0.407 Target Domain Loss:1.302\n",
      "Epoch:209/1000 Source Label Loss:0.322 Source Domain Loss:0.389 Target Domain Loss:1.306\n",
      "Epoch:210/1000 Source Label Loss:0.325 Source Domain Loss:0.353 Target Domain Loss:1.308\n",
      "Epoch:211/1000 Source Label Loss:0.324 Source Domain Loss:0.33 Target Domain Loss:1.31\n",
      "Epoch:212/1000 Source Label Loss:0.321 Source Domain Loss:0.325 Target Domain Loss:1.311\n",
      "Epoch:213/1000 Source Label Loss:0.321 Source Domain Loss:0.324 Target Domain Loss:1.311\n",
      "Epoch:214/1000 Source Label Loss:0.321 Source Domain Loss:0.332 Target Domain Loss:1.311\n",
      "Epoch:215/1000 Source Label Loss:0.321 Source Domain Loss:0.344 Target Domain Loss:1.311\n",
      "Epoch:216/1000 Source Label Loss:0.324 Source Domain Loss:0.353 Target Domain Loss:1.311\n",
      "Epoch:217/1000 Source Label Loss:0.323 Source Domain Loss:0.34 Target Domain Loss:1.312\n",
      "Epoch:218/1000 Source Label Loss:0.322 Source Domain Loss:0.33 Target Domain Loss:1.312\n",
      "Epoch:219/1000 Source Label Loss:0.321 Source Domain Loss:0.321 Target Domain Loss:1.312\n",
      "Epoch:220/1000 Source Label Loss:0.32 Source Domain Loss:0.32 Target Domain Loss:1.312\n",
      "Epoch:221/1000 Source Label Loss:0.32 Source Domain Loss:0.327 Target Domain Loss:1.312\n",
      "Epoch:222/1000 Source Label Loss:0.32 Source Domain Loss:0.335 Target Domain Loss:1.312\n",
      "Epoch:223/1000 Source Label Loss:0.319 Source Domain Loss:0.338 Target Domain Loss:1.311\n",
      "Epoch:224/1000 Source Label Loss:0.319 Source Domain Loss:0.358 Target Domain Loss:1.31\n",
      "Epoch:225/1000 Source Label Loss:0.318 Source Domain Loss:0.334 Target Domain Loss:1.31\n",
      "Epoch:226/1000 Source Label Loss:0.321 Source Domain Loss:0.338 Target Domain Loss:1.31\n",
      "Epoch:227/1000 Source Label Loss:0.32 Source Domain Loss:0.332 Target Domain Loss:1.311\n",
      "Epoch:228/1000 Source Label Loss:0.321 Source Domain Loss:0.331 Target Domain Loss:1.311\n",
      "Epoch:229/1000 Source Label Loss:0.319 Source Domain Loss:0.336 Target Domain Loss:1.311\n",
      "Epoch:230/1000 Source Label Loss:0.319 Source Domain Loss:0.34 Target Domain Loss:1.311\n",
      "Epoch:231/1000 Source Label Loss:0.321 Source Domain Loss:0.345 Target Domain Loss:1.311\n",
      "Epoch:232/1000 Source Label Loss:0.322 Source Domain Loss:0.343 Target Domain Loss:1.311\n",
      "Epoch:233/1000 Source Label Loss:0.32 Source Domain Loss:0.34 Target Domain Loss:1.312\n",
      "Epoch:234/1000 Source Label Loss:0.319 Source Domain Loss:0.329 Target Domain Loss:1.312\n",
      "Epoch:235/1000 Source Label Loss:0.32 Source Domain Loss:0.323 Target Domain Loss:1.312\n",
      "Epoch:236/1000 Source Label Loss:0.32 Source Domain Loss:0.319 Target Domain Loss:1.313\n",
      "Epoch:237/1000 Source Label Loss:0.319 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:238/1000 Source Label Loss:0.319 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:239/1000 Source Label Loss:0.317 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:240/1000 Source Label Loss:0.317 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:241/1000 Source Label Loss:0.319 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:242/1000 Source Label Loss:0.317 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:243/1000 Source Label Loss:0.32 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:244/1000 Source Label Loss:0.32 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:245/1000 Source Label Loss:0.319 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:246/1000 Source Label Loss:0.318 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:247/1000 Source Label Loss:0.318 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:248/1000 Source Label Loss:0.317 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:249/1000 Source Label Loss:0.318 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:250/1000 Source Label Loss:0.319 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:251/1000 Source Label Loss:0.318 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:252/1000 Source Label Loss:0.316 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:253/1000 Source Label Loss:0.318 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:254/1000 Source Label Loss:0.319 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:255/1000 Source Label Loss:0.317 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:256/1000 Source Label Loss:0.319 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:257/1000 Source Label Loss:0.319 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:258/1000 Source Label Loss:0.319 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:259/1000 Source Label Loss:0.319 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:260/1000 Source Label Loss:0.319 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:261/1000 Source Label Loss:0.316 Source Domain Loss:0.319 Target Domain Loss:1.313\n",
      "Epoch:262/1000 Source Label Loss:0.319 Source Domain Loss:0.319 Target Domain Loss:1.313\n",
      "Epoch:263/1000 Source Label Loss:0.318 Source Domain Loss:0.32 Target Domain Loss:1.313\n",
      "Epoch:264/1000 Source Label Loss:0.318 Source Domain Loss:0.322 Target Domain Loss:1.313\n",
      "Epoch:265/1000 Source Label Loss:0.319 Source Domain Loss:0.326 Target Domain Loss:1.313\n",
      "Epoch:266/1000 Source Label Loss:0.319 Source Domain Loss:0.33 Target Domain Loss:1.312\n",
      "Epoch:267/1000 Source Label Loss:0.321 Source Domain Loss:0.326 Target Domain Loss:1.312\n",
      "Epoch:268/1000 Source Label Loss:0.321 Source Domain Loss:0.319 Target Domain Loss:1.312\n",
      "Epoch:269/1000 Source Label Loss:0.32 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:270/1000 Source Label Loss:0.32 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:271/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:272/1000 Source Label Loss:0.319 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:273/1000 Source Label Loss:0.319 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:274/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:275/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:276/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:277/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:278/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:279/1000 Source Label Loss:0.319 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:280/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:281/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:282/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:283/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:284/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:285/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:286/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:287/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:288/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:289/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:290/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:291/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:292/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:293/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:294/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:295/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:296/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:297/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:298/1000 Source Label Loss:0.318 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:299/1000 Source Label Loss:0.318 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:300/1000 Source Label Loss:0.317 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:301/1000 Source Label Loss:0.318 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:302/1000 Source Label Loss:0.317 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:303/1000 Source Label Loss:0.317 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:304/1000 Source Label Loss:0.318 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:305/1000 Source Label Loss:0.318 Source Domain Loss:0.319 Target Domain Loss:1.312\n",
      "Epoch:306/1000 Source Label Loss:0.318 Source Domain Loss:0.32 Target Domain Loss:1.312\n",
      "Epoch:307/1000 Source Label Loss:0.318 Source Domain Loss:0.319 Target Domain Loss:1.312\n",
      "Epoch:308/1000 Source Label Loss:0.317 Source Domain Loss:0.319 Target Domain Loss:1.313\n",
      "Epoch:309/1000 Source Label Loss:0.318 Source Domain Loss:0.319 Target Domain Loss:1.313\n",
      "Epoch:310/1000 Source Label Loss:0.317 Source Domain Loss:0.317 Target Domain Loss:1.312\n",
      "Epoch:311/1000 Source Label Loss:0.318 Source Domain Loss:0.318 Target Domain Loss:1.313\n",
      "Epoch:312/1000 Source Label Loss:0.317 Source Domain Loss:0.317 Target Domain Loss:1.313\n",
      "Epoch:313/1000 Source Label Loss:0.317 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:314/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:315/1000 Source Label Loss:0.317 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:316/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:317/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:318/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:319/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:320/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:321/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:322/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:323/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:324/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:325/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:326/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:327/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:328/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:329/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:330/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:331/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:332/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:333/1000 Source Label Loss:0.315 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:334/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:335/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:336/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:337/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:338/1000 Source Label Loss:0.317 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:339/1000 Source Label Loss:0.317 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:340/1000 Source Label Loss:0.318 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:341/1000 Source Label Loss:0.318 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:342/1000 Source Label Loss:0.316 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:343/1000 Source Label Loss:0.317 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:344/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:345/1000 Source Label Loss:0.316 Source Domain Loss:0.316 Target Domain Loss:1.313\n",
      "Epoch:346/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:347/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:348/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:349/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:350/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:351/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:352/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:353/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:354/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:355/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:356/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:357/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:358/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:359/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:360/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:361/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:362/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:363/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:364/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:365/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:366/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:367/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:368/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:369/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:370/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:371/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:372/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:373/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:374/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:375/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:376/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:377/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:378/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:379/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:380/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:381/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:382/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:383/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:384/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:385/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:386/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:387/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:388/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:389/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:390/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:391/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:392/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:393/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:394/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:395/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:396/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:397/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:398/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:399/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:400/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:401/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:402/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:403/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:404/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:405/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:406/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:407/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:408/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:409/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:410/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:411/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:412/1000 Source Label Loss:0.316 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:413/1000 Source Label Loss:0.318 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:414/1000 Source Label Loss:0.317 Source Domain Loss:0.315 Target Domain Loss:1.313\n",
      "Epoch:415/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:416/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:417/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:418/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:419/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:420/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:421/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:422/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:423/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:424/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:425/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:426/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:427/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:428/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:429/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:430/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:431/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:432/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:433/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:434/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:435/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:436/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:437/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:438/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:439/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:440/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:441/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:442/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:443/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:444/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:445/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:446/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:447/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:448/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:449/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:450/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:451/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:452/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:453/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:454/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:455/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:456/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:457/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:458/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:459/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:460/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:461/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:462/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:463/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:464/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:465/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:466/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:467/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:468/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:469/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:470/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:471/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:472/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:473/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:474/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:475/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:476/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:477/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:478/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:479/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:480/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:481/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:482/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:483/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:484/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:485/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:486/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:487/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:488/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:489/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:490/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:491/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:492/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:493/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:494/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:495/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:496/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:497/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:498/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:499/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:500/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:501/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:502/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:503/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:504/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:505/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:506/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:507/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:508/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:509/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:510/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:511/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:512/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:513/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:514/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:515/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:516/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:517/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:518/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:519/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:520/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:521/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:522/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:523/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:524/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:525/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:526/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:527/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:528/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:529/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:530/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:531/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:532/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:533/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:534/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:535/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:536/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:537/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:538/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:539/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:540/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:541/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:542/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:543/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:544/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:545/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:546/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:547/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:548/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:549/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:550/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:551/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:552/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:553/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:554/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:555/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:556/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:557/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:558/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:559/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:560/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:561/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:562/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:563/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:564/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:565/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:566/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:567/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:568/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:569/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:570/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:571/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:572/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:573/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:574/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:575/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:576/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:577/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:578/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:579/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:580/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:581/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:582/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:583/1000 Source Label Loss:0.318 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:584/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:585/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:586/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:587/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:588/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:589/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:590/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:591/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:592/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:593/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:594/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:595/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:596/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:597/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:598/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:599/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:600/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:601/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:602/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:603/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:604/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:605/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:606/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:607/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:608/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:609/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:610/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:611/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:612/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:613/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:614/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:615/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:616/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:617/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:618/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:619/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:620/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:621/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:622/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:623/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:624/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:625/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:626/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:627/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:628/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:629/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:630/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:631/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:632/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:633/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:634/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:635/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:636/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:637/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:638/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:639/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:640/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:641/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:642/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:643/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:644/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:645/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:646/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:647/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:648/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:649/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:650/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:651/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:652/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:653/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:654/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:655/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:656/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:657/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:658/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:659/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:660/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:661/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:662/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:663/1000 Source Label Loss:0.315 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:664/1000 Source Label Loss:0.317 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:665/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:666/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:667/1000 Source Label Loss:0.316 Source Domain Loss:0.314 Target Domain Loss:1.313\n",
      "Epoch:668/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:669/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:670/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:671/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:672/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:673/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:674/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:675/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:676/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:677/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:678/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:679/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:680/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:681/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:682/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:683/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:684/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:685/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:686/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:687/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:688/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:689/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:690/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:691/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:692/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:693/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:694/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:695/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:696/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:697/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:698/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:699/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:700/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:701/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:702/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:703/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:704/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:705/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:706/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:707/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:708/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:709/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:710/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:711/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:712/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:713/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:714/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:715/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:716/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:717/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:718/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:719/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:720/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:721/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:722/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:723/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:724/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:725/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:726/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:727/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:728/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:729/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:730/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:731/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:732/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:733/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:734/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:735/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:736/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:737/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:738/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:739/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:740/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:741/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:742/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:743/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:744/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:745/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:746/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:747/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:748/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:749/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:750/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:751/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:752/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:753/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:754/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:755/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:756/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:757/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:758/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:759/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:760/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:761/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:762/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:763/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:764/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:765/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:766/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:767/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:768/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:769/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:770/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:771/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:772/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:773/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:774/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:775/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:776/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:777/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:778/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:779/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:780/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:781/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:782/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:783/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:784/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:785/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:786/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:787/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:788/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:789/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:790/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:791/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:792/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:793/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:794/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:795/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:796/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:797/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:798/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:799/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:800/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:801/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:802/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:803/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:804/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:805/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:806/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:807/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:808/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:809/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:810/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:811/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:812/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:813/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:814/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:815/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:816/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:817/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:818/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:819/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:820/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:821/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:822/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:823/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:824/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:825/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:826/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:827/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:828/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:829/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:830/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:831/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:832/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:833/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:834/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:835/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:836/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:837/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:838/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:839/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:840/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:841/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:842/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:843/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:844/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:845/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:846/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:847/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:848/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:849/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:850/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:851/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:852/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:853/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:854/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:855/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:856/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:857/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:858/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:859/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:860/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:861/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:862/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:863/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:864/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:865/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:866/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:867/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:868/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:869/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:870/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:871/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:872/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:873/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:874/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:875/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:876/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:877/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:878/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:879/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:880/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:881/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:882/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:883/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:884/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:885/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:886/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:887/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:888/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:889/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:890/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:891/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:892/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:893/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:894/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:895/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:896/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:897/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:898/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:899/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:900/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:901/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:902/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:903/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:904/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:905/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:906/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:907/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:908/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:909/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:910/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:911/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:912/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:913/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:914/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:915/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:916/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:917/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:918/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:919/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:920/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:921/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:922/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:923/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:924/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:925/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:926/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:927/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:928/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:929/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:930/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:931/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:932/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:933/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:934/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:935/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:936/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:937/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:938/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:939/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:940/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:941/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:942/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:943/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:944/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:945/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:946/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:947/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:948/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:949/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:950/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:951/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:952/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:953/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:954/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:955/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:956/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:957/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:958/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:959/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:960/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:961/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:962/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:963/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:964/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:965/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:966/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:967/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:968/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:969/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:970/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:971/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:972/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:973/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:974/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:975/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:976/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:977/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:978/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:979/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:980/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:981/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:982/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:983/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:984/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:985/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:986/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:987/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:988/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:989/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:990/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:991/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:992/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:993/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:994/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:995/1000 Source Label Loss:0.316 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:996/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:997/1000 Source Label Loss:0.315 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:998/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Epoch:999/1000 Source Label Loss:0.317 Source Domain Loss:0.313 Target Domain Loss:1.313\n",
      "Best epoch: 71 Best domain loss: 1.8158150613307953\n"
     ]
    }
   ],
   "source": [
    "best_domain_loss = 0\n",
    "best_encoder = None\n",
    "best_mlp = None\n",
    "best_domain_classifier = None\n",
    "best_epoch = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    total_src_label_loss, total_src_domain_loss, total_tgt_domain_loss = \\\n",
    "adapt(encoder_9_14, mlp_9_14, domain_classifier_9_14, ReverseLayerF, optimizer, train_loader, test_loader_9_14, e, epochs, loss_fn_class, loss_fn_domain, device)\n",
    "    print(f'Epoch:{e}/{epochs} Source Label Loss:{round(total_src_label_loss,3)} Source Domain Loss:{round(total_src_domain_loss,3)} Target Domain Loss:{round(total_tgt_domain_loss,3)}')\n",
    "    if total_src_domain_loss + total_tgt_domain_loss > best_domain_loss:\n",
    "        best_domain_loss = total_src_domain_loss + total_tgt_domain_loss\n",
    "        best_encoder = deepcopy(encoder_9_14)\n",
    "        best_mlp = deepcopy(mlp_9_14)\n",
    "        best_domain_classifier = deepcopy(domain_classifier_9_14)\n",
    "        best_epoch = e\n",
    "\n",
    "print(\"Best epoch:\", best_epoch, \"Best domain loss:\", best_domain_loss)\n",
    "encoder_9_14 = deepcopy(best_encoder)\n",
    "mlp_9_14 = deepcopy(best_mlp)\n",
    "domain_classifier_9_14 = deepcopy(best_domain_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db88096",
   "metadata": {},
   "source": [
    "### 14-18 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55243d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [get_data(data_dir, \"elliptic\", i) for i in range(14,19)]\n",
    "test_loader_14_19 = DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bfdc043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4197, Test F1: 0.5342\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = test(encoder_9_14, mlp_9_14, test_loader_14_19, loss_fn, device)\n",
    "f1_list.append(test_f1)\n",
    "print(f\"Test Loss: {round(test_loss,4)}, Test F1: {round(test_f1,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd480c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_14_19, mlp_14_19, domain_classifier_14_19 = deepcopy(encoder_9_14), deepcopy(mlp_9_14), deepcopy(domain_classifier_9_14)\n",
    "optimizer = torch.optim.Adam(list(encoder_14_19.parameters()) + list(mlp_14_19.parameters()) + list(domain_classifier_14_19.parameters()), lr=1e-3)\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61062acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0/1000 Source Label Loss:0.329 Source Domain Loss:0.475 Target Domain Loss:1.249\n",
      "Epoch:1/1000 Source Label Loss:0.329 Source Domain Loss:0.378 Target Domain Loss:1.28\n",
      "Epoch:2/1000 Source Label Loss:0.319 Source Domain Loss:0.348 Target Domain Loss:1.294\n",
      "Epoch:3/1000 Source Label Loss:0.327 Source Domain Loss:0.328 Target Domain Loss:1.3\n",
      "Epoch:4/1000 Source Label Loss:0.318 Source Domain Loss:0.32 Target Domain Loss:1.302\n",
      "Epoch:5/1000 Source Label Loss:0.318 Source Domain Loss:0.319 Target Domain Loss:1.299\n",
      "Epoch:6/1000 Source Label Loss:0.317 Source Domain Loss:0.319 Target Domain Loss:1.293\n",
      "Epoch:7/1000 Source Label Loss:0.326 Source Domain Loss:0.325 Target Domain Loss:1.284\n",
      "Epoch:8/1000 Source Label Loss:0.326 Source Domain Loss:0.335 Target Domain Loss:1.253\n",
      "Epoch:9/1000 Source Label Loss:0.326 Source Domain Loss:0.37 Target Domain Loss:1.154\n",
      "Epoch:10/1000 Source Label Loss:0.326 Source Domain Loss:0.531 Target Domain Loss:0.871\n",
      "Epoch:11/1000 Source Label Loss:0.317 Source Domain Loss:0.733 Target Domain Loss:0.572\n",
      "Epoch:12/1000 Source Label Loss:0.325 Source Domain Loss:0.902 Target Domain Loss:0.484\n",
      "Epoch:13/1000 Source Label Loss:0.324 Source Domain Loss:0.908 Target Domain Loss:0.501\n",
      "Epoch:14/1000 Source Label Loss:0.323 Source Domain Loss:0.853 Target Domain Loss:0.554\n",
      "Epoch:15/1000 Source Label Loss:0.318 Source Domain Loss:0.863 Target Domain Loss:0.605\n",
      "Epoch:16/1000 Source Label Loss:0.323 Source Domain Loss:0.876 Target Domain Loss:0.632\n",
      "Epoch:17/1000 Source Label Loss:0.317 Source Domain Loss:0.915 Target Domain Loss:0.634\n",
      "Epoch:18/1000 Source Label Loss:0.317 Source Domain Loss:1.055 Target Domain Loss:0.611\n",
      "Epoch:19/1000 Source Label Loss:0.323 Source Domain Loss:1.151 Target Domain Loss:0.566\n",
      "Epoch:20/1000 Source Label Loss:0.322 Source Domain Loss:1.175 Target Domain Loss:0.495\n",
      "Epoch:21/1000 Source Label Loss:0.32 Source Domain Loss:1.219 Target Domain Loss:0.454\n",
      "Epoch:22/1000 Source Label Loss:0.318 Source Domain Loss:1.261 Target Domain Loss:0.44\n",
      "Epoch:23/1000 Source Label Loss:0.32 Source Domain Loss:1.253 Target Domain Loss:0.434\n",
      "Epoch:24/1000 Source Label Loss:0.32 Source Domain Loss:1.284 Target Domain Loss:0.391\n",
      "Epoch:25/1000 Source Label Loss:0.32 Source Domain Loss:1.31 Target Domain Loss:0.333\n",
      "Epoch:26/1000 Source Label Loss:0.32 Source Domain Loss:1.312 Target Domain Loss:0.319\n",
      "Epoch:27/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:28/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:29/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:30/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:31/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:32/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:33/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:34/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:35/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:36/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:37/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:38/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:39/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:40/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:41/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:42/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:43/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:44/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:45/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:46/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:47/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:48/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:49/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:50/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:51/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:52/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:53/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:54/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:55/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:56/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:57/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:58/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:59/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:60/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:61/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:62/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:63/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:64/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:65/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:66/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:67/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:68/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:69/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:70/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:71/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:72/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:73/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:74/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:75/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:76/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:77/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:78/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:79/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:80/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:81/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:82/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:83/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:84/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:85/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:86/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:87/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:88/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:89/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:90/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:91/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:92/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:93/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:94/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:95/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:96/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:97/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:98/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:99/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:100/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:101/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:102/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:103/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:104/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:105/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:106/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:107/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:108/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:109/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:110/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:111/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:112/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:113/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:114/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:115/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:116/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:117/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:118/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:119/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:120/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:121/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:122/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:123/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:124/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:125/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:126/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:127/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:128/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:129/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:130/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:131/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:132/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:133/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:134/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:135/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:136/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:137/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:138/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.318\n",
      "Epoch:139/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.32\n",
      "Epoch:140/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.322\n",
      "Epoch:141/1000 Source Label Loss:0.318 Source Domain Loss:1.312 Target Domain Loss:0.323\n",
      "Epoch:142/1000 Source Label Loss:0.316 Source Domain Loss:1.311 Target Domain Loss:0.323\n",
      "Epoch:143/1000 Source Label Loss:0.318 Source Domain Loss:1.311 Target Domain Loss:0.322\n",
      "Epoch:144/1000 Source Label Loss:0.318 Source Domain Loss:1.311 Target Domain Loss:0.322\n",
      "Epoch:145/1000 Source Label Loss:0.316 Source Domain Loss:1.312 Target Domain Loss:0.322\n",
      "Epoch:146/1000 Source Label Loss:0.316 Source Domain Loss:1.311 Target Domain Loss:0.321\n",
      "Epoch:147/1000 Source Label Loss:0.318 Source Domain Loss:1.312 Target Domain Loss:0.32\n",
      "Epoch:148/1000 Source Label Loss:0.318 Source Domain Loss:1.312 Target Domain Loss:0.317\n",
      "Epoch:149/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:150/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:151/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:152/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:153/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:154/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:155/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:156/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:157/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:158/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:159/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:160/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:161/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:162/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:163/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:164/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.319\n",
      "Epoch:165/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.324\n",
      "Epoch:166/1000 Source Label Loss:0.319 Source Domain Loss:1.312 Target Domain Loss:0.335\n",
      "Epoch:167/1000 Source Label Loss:0.318 Source Domain Loss:1.31 Target Domain Loss:0.359\n",
      "Epoch:168/1000 Source Label Loss:0.321 Source Domain Loss:1.313 Target Domain Loss:0.37\n",
      "Epoch:169/1000 Source Label Loss:0.321 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:170/1000 Source Label Loss:0.321 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:171/1000 Source Label Loss:0.322 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:172/1000 Source Label Loss:0.321 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:173/1000 Source Label Loss:0.32 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:174/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:175/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:176/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:177/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:178/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:179/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:180/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:181/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:182/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:183/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:184/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:185/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.318\n",
      "Epoch:186/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.319\n",
      "Epoch:187/1000 Source Label Loss:0.318 Source Domain Loss:1.312 Target Domain Loss:0.318\n",
      "Epoch:188/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:189/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:190/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:191/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:192/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:193/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:194/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:195/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:196/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:197/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:198/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:199/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:200/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:201/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:202/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:203/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:204/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:205/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:206/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:207/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:208/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:209/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:210/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:211/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:212/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:213/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:214/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:215/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:216/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.318\n",
      "Epoch:217/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.318\n",
      "Epoch:218/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:219/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:220/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:221/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:222/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:223/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:224/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:225/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:226/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:227/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:228/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:229/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:230/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:231/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.318\n",
      "Epoch:232/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.319\n",
      "Epoch:233/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.319\n",
      "Epoch:234/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.318\n",
      "Epoch:235/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:236/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:237/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:238/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:239/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:240/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:241/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:242/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:243/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:244/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:245/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:246/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:247/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:248/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:249/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:250/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:251/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.317\n",
      "Epoch:252/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:253/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:254/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:255/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:256/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.319\n",
      "Epoch:257/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.322\n",
      "Epoch:258/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.322\n",
      "Epoch:259/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.318\n",
      "Epoch:260/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:261/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:262/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:263/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:264/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:265/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:266/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:267/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:268/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:269/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:270/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:271/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:272/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:273/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:274/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:275/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:276/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:277/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:278/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:279/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:280/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:281/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:282/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:283/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:284/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:285/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:286/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:287/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:288/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:289/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:290/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:291/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:292/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:293/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:294/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:295/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:296/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:297/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:298/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.316\n",
      "Epoch:299/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:300/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.315\n",
      "Epoch:301/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:302/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:303/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:304/1000 Source Label Loss:0.319 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:305/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:306/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:307/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:308/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:309/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:310/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:311/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:312/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:313/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:314/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:315/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:316/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:317/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:318/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:319/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:320/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:321/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:322/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:323/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:324/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:325/1000 Source Label Loss:0.318 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:326/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:327/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:328/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:329/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:330/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:331/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:332/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:333/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:334/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:335/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:336/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:337/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:338/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:339/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:340/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:341/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:342/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:343/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:344/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:345/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:346/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:347/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:348/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:349/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:350/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:351/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:352/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:353/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:354/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:355/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:356/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:357/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:358/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:359/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:360/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:361/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:362/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:363/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:364/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:365/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:366/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:367/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:368/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:369/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:370/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:371/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:372/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:373/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:374/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:375/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:376/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:377/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:378/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:379/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:380/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:381/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:382/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:383/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:384/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:385/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:386/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:387/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:388/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:389/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:390/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:391/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:392/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:393/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:394/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:395/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:396/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:397/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:398/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:399/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:400/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:401/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:402/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:403/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:404/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:405/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:406/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:407/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:408/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:409/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:410/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:411/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:412/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:413/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:414/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:415/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:416/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:417/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:418/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:419/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:420/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:421/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:422/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:423/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:424/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:425/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:426/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:427/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:428/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:429/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:430/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:431/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:432/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:433/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:434/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:435/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:436/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:437/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:438/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:439/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:440/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:441/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:442/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:443/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:444/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:445/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:446/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:447/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:448/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:449/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:450/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:451/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:452/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:453/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:454/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:455/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:456/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:457/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:458/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:459/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:460/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:461/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:462/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:463/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:464/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:465/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:466/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:467/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:468/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:469/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:470/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:471/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:472/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:473/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:474/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:475/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:476/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:477/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:478/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:479/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:480/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:481/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:482/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:483/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:484/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:485/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:486/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:487/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:488/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:489/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:490/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:491/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:492/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:493/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:494/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:495/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:496/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:497/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:498/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:499/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:500/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:501/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:502/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:503/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:504/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:505/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:506/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:507/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:508/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:509/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:510/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:511/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:512/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:513/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:514/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:515/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:516/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:517/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:518/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:519/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:520/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:521/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:522/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:523/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:524/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:525/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:526/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:527/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:528/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:529/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:530/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:531/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:532/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:533/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:534/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:535/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:536/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:537/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:538/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:539/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:540/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:541/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:542/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:543/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:544/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:545/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:546/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:547/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:548/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:549/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:550/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:551/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:552/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:553/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:554/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:555/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:556/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:557/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:558/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.314\n",
      "Epoch:559/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:560/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:561/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:562/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:563/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:564/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:565/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:566/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:567/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:568/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:569/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:570/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:571/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:572/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:573/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:574/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:575/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:576/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:577/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:578/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:579/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:580/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:581/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:582/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:583/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:584/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:585/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:586/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:587/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:588/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:589/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:590/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:591/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:592/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:593/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:594/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:595/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:596/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:597/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:598/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:599/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:600/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:601/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:602/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:603/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:604/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:605/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:606/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:607/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:608/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:609/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:610/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:611/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:612/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:613/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:614/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:615/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:616/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:617/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:618/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:619/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:620/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:621/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:622/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:623/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:624/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:625/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:626/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:627/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:628/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:629/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:630/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:631/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:632/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:633/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:634/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:635/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:636/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:637/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:638/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:639/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:640/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:641/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:642/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:643/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:644/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:645/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:646/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:647/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:648/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:649/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:650/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:651/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:652/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:653/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:654/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:655/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:656/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:657/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:658/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:659/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:660/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:661/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:662/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:663/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:664/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:665/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:666/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:667/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:668/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:669/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:670/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:671/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:672/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:673/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:674/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:675/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:676/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:677/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:678/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:679/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:680/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:681/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:682/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:683/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:684/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:685/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:686/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:687/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:688/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:689/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:690/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:691/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:692/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:693/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:694/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:695/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:696/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:697/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:698/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:699/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:700/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:701/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:702/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:703/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:704/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:705/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:706/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:707/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:708/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:709/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:710/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:711/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:712/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:713/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:714/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:715/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:716/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:717/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:718/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:719/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:720/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:721/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:722/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:723/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:724/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:725/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:726/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:727/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:728/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:729/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:730/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:731/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:732/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:733/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:734/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:735/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:736/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:737/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:738/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:739/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:740/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:741/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:742/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:743/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:744/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:745/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:746/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:747/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:748/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:749/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:750/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:751/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:752/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:753/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:754/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:755/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:756/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:757/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:758/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:759/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:760/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:761/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:762/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:763/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:764/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:765/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:766/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:767/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:768/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:769/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:770/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:771/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:772/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:773/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:774/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:775/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:776/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:777/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:778/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:779/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:780/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:781/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:782/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:783/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:784/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:785/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:786/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:787/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:788/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:789/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:790/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:791/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:792/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:793/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:794/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:795/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:796/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:797/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:798/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:799/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:800/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:801/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:802/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:803/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:804/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:805/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:806/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:807/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:808/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:809/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:810/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:811/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:812/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:813/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:814/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:815/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:816/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:817/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:818/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:819/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:820/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:821/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:822/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:823/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:824/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:825/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:826/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:827/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:828/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:829/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:830/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:831/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:832/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:833/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:834/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:835/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:836/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:837/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:838/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:839/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:840/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:841/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:842/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:843/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:844/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:845/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:846/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:847/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:848/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:849/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:850/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:851/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:852/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:853/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:854/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:855/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:856/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:857/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:858/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:859/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:860/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:861/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:862/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:863/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:864/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:865/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:866/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:867/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:868/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:869/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:870/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:871/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:872/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:873/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:874/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:875/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:876/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:877/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:878/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:879/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:880/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:881/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:882/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:883/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:884/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:885/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:886/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:887/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:888/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:889/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:890/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:891/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:892/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:893/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:894/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:895/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:896/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:897/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:898/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:899/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:900/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:901/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:902/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:903/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:904/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:905/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:906/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:907/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:908/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:909/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:910/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:911/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:912/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:913/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:914/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:915/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:916/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:917/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:918/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:919/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:920/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:921/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:922/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:923/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:924/1000 Source Label Loss:0.317 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:925/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:926/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:927/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:928/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:929/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:930/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:931/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:932/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:933/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:934/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:935/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:936/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:937/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:938/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:939/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:940/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:941/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:942/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:943/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:944/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:945/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:946/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:947/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:948/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:949/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:950/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:951/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:952/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:953/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:954/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:955/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:956/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:957/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:958/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:959/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:960/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:961/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:962/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:963/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:964/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:965/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:966/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:967/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:968/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:969/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:970/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:971/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:972/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:973/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:974/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:975/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:976/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:977/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:978/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:979/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:980/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:981/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:982/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:983/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:984/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:985/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:986/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:987/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:988/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:989/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:990/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:991/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:992/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:993/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:994/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:995/1000 Source Label Loss:0.315 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:996/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:997/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:998/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Epoch:999/1000 Source Label Loss:0.316 Source Domain Loss:1.313 Target Domain Loss:0.313\n",
      "Best epoch: 0 Best domain loss: 1.7240363776683807\n"
     ]
    }
   ],
   "source": [
    "best_domain_loss = 0\n",
    "best_encoder = None\n",
    "best_mlp = None\n",
    "best_domain_classifier = None\n",
    "best_epoch = 0\n",
    "for e in range(epochs):\n",
    "    total_src_label_loss, total_src_domain_loss, total_tgt_domain_loss = \\\n",
    "adapt(encoder_14_19, mlp_14_19, domain_classifier_14_19, ReverseLayerF, optimizer, train_loader, test_loader_14_19, e, epochs, loss_fn_class, loss_fn_domain, device)\n",
    "    print(f'Epoch:{e}/{epochs} Source Label Loss:{round(total_src_label_loss,3)} Source Domain Loss:{round(total_src_domain_loss,3)} Target Domain Loss:{round(total_tgt_domain_loss,3)}')\n",
    "    if total_src_domain_loss + total_tgt_domain_loss > best_domain_loss:\n",
    "        best_domain_loss = total_src_domain_loss + total_tgt_domain_loss\n",
    "        best_encoder = deepcopy(encoder_14_19)\n",
    "        best_mlp = deepcopy(mlp_14_19)\n",
    "        best_domain_classifier = deepcopy(domain_classifier_14_19)\n",
    "        best_epoch = e\n",
    "\n",
    "print(\"Best epoch:\", best_epoch, \"Best domain loss:\", best_domain_loss)\n",
    "encoder_14_19 = deepcopy(best_encoder)\n",
    "mlp_14_19 = deepcopy(best_mlp)\n",
    "domain_classifier_14_19 = deepcopy(best_domain_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fe1ff7",
   "metadata": {},
   "source": [
    "### 19-23 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7550e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [get_data(data_dir, \"elliptic\", i) for i in range(19,24)]\n",
    "test_loader_19_24 = DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea43f843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4393, Test F1: 0.3798\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = test(encoder_14_19, mlp_14_19, test_loader_19_24, loss_fn, device)\n",
    "f1_list.append(test_f1)\n",
    "print(f\"Test Loss: {round(test_loss,4)}, Test F1: {round(test_f1,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236f388",
   "metadata": {},
   "source": [
    "### 24-28 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cd2200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [get_data(data_dir, \"elliptic\", i) for i in range(24,29)]\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82862669",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loss, test_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m f1_list\u001b[38;5;241m.\u001b[39mappend(test_f1)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(test_loss,\u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(test_f1,\u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dyngraph-uda/.venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 86\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(encoder, mlp, loader, loss_fn, device)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     85\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 86\u001b[0m     out \u001b[38;5;241m=\u001b[39m mlp(\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     87\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(out[data\u001b[38;5;241m.\u001b[39mmask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mmask])\n\u001b[1;32m     88\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/dyngraph-uda/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [6], line 9\u001b[0m, in \u001b[0;36mTwoLayerGraphSAGE.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout)\n",
      "File \u001b[0;32m~/dyngraph-uda/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dyngraph-uda/.venv/lib/python3.8/site-packages/torch_geometric/nn/conv/sage_conv.py:132\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m--> 132\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_l\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight \u001b[38;5;129;01mand\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dyngraph-uda/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dyngraph-uda/.venv/lib/python3.8/site-packages/torch_geometric/nn/dense/linear.py:118\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m        x (Tensor): The features.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = test(encoder, mlp, test_loader, loss_fn)\n",
    "f1_list.append(test_f1)\n",
    "print(f\"Test Loss: {round(test_loss,4)}, Test F1: {round(test_f1,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df94c7",
   "metadata": {},
   "source": [
    "### 29-33 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4543d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [get_data(data_dir, \"elliptic\", i) for i in range(29,34)]\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "530a9b43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6201, Test F1: 0.3769\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = test(encoder, mlp, test_loader, loss_fn)\n",
    "f1_list.append(test_f1)\n",
    "print(f\"Test Loss: {round(test_loss,4)}, Test F1: {round(test_f1,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd1e7a",
   "metadata": {},
   "source": [
    "### 34-38 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6c4e83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [get_data(data_dir, \"elliptic\", i) for i in range(34,39)]\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3f318972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6685, Test F1: 0.3045\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = test(encoder, mlp, test_loader, loss_fn)\n",
    "f1_list.append(test_f1)\n",
    "print(f\"Test Loss: {round(test_loss,4)}, Test F1: {round(test_f1,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cdc8a9",
   "metadata": {},
   "source": [
    "### 39-43 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "31c5db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [get_data(data_dir, \"elliptic\", i) for i in range(39,44)]\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d838a3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.936, Test F1: 0.1487\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = test(encoder, mlp, test_loader, loss_fn)\n",
    "f1_list.append(test_f1)\n",
    "print(f\"Test Loss: {round(test_loss,4)}, Test F1: {round(test_f1,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27ea87",
   "metadata": {},
   "source": [
    "### 44-48 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "42abac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [get_data(data_dir, \"elliptic\", i) for i in range(44,49)]\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "94b732a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7313, Test F1: 0.1614\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_f1 = test(encoder, mlp, test_loader, loss_fn)\n",
    "f1_list.append(test_f1)\n",
    "print(f\"Test Loss: {round(test_loss,4)}, Test F1: {round(test_f1,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742b342",
   "metadata": {},
   "source": [
    "* 9-13, 14-18, 19-23, 24-28, 29-33, 34-38, 39-43, 44-48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "74820680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f1_list)\n",
    "plt.title(\"Performance Degradation Over Time\")\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"Evaluation Time Step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280f42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
