{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2a7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from typing import Any, Tuple, Optional, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f993cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhchung/dyngraph-uda/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbc0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da93a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from ogb.nodeproppred.evaluate import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa60451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import temp_partition_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cbb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ogbn-arxiv'\n",
    "dataset = PygNodePropPredDataset(name = dataset_name, root='/home/hhchung/data/ogb-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe7aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b2bf2",
   "metadata": {},
   "source": [
    "## Model Structure ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0423e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ThreeLayerGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.linear1(x)\n",
    "        x1 = F.elu(x1)\n",
    "        x1 = F.dropout(x1, p=self.dropout)\n",
    "        output = self.linear2(x1)\n",
    "        # x = F.softmax(x, dim=1)\n",
    "        features = x1\n",
    "        return output, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f76cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, mlp, optimizer, data):\n",
    "    encoder.train()\n",
    "    mlp.train()\n",
    "    \n",
    "    out, _ = mlp(encoder(data.x, data.edge_index))\n",
    "    out = F.log_softmax(out, dim=1)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test(encoder, mlp, data, evaluator):\n",
    "    encoder.eval()\n",
    "    mlp.eval()\n",
    "    \n",
    "    out, _ = mlp(encoder(data.x, data.edge_index))\n",
    "    out = F.log_softmax(out, dim=1)\n",
    "    val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask]).item()\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    val_acc = evaluator.eval({\n",
    "        'y_true': data.y[data.val_mask].unsqueeze(1),\n",
    "        'y_pred': y_pred[data.val_mask],\n",
    "    })['acc']\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a14358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_label(encoder, classifier, tgt_data):\n",
    "    pseudo_y, _ = classifier(encoder(tgt_data.x, tgt_data.edge_index))\n",
    "    pseudo_y = F.log_softmax(pseudo_y, dim=1).argmax(dim=-1, keepdim=False)\n",
    "    return pseudo_y\n",
    "\n",
    "def adapt_train(encoder, classifier, src_data, tgt_data, optimizer):\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    pseudo_tgt_label = pseudo_label(encoder, classifier, tgt_data)\n",
    "    src_y, _ = classifier(encoder(src_data.x, src_data.edge_index))\n",
    "    tgt_y, _ = classifier(encoder(tgt_data.x, tgt_data.edge_index))\n",
    "    src_loss = F.nll_loss(F.log_softmax(src_y[src_data.train_mask], dim=1), src_data.y[src_data.train_mask])\n",
    "    tgt_loss = F.nll_loss(F.log_softmax(tgt_y[tgt_data.train_mask], dim=1), pseudo_tgt_label[tgt_data.train_mask])\n",
    "    loss = src_loss + tgt_loss \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), src_loss.item(), tgt_loss.item()\n",
    "    \n",
    "@torch.no_grad()\n",
    "def adapt_val(encoder, classifier, src_data, tgt_data):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    pseudo_tgt_label = pseudo_label(encoder, classifier, tgt_data)\n",
    "    src_y, _ = classifier(encoder(src_data.x, src_data.edge_index))\n",
    "    tgt_y, _ = classifier(encoder(tgt_data.x, tgt_data.edge_index))\n",
    "    src_loss = F.nll_loss(F.log_softmax(src_y[src_data.val_mask], dim=1), src_data.y[src_data.val_mask])\n",
    "    tgt_loss = F.nll_loss(F.log_softmax(tgt_y[tgt_data.val_mask], dim=1), pseudo_tgt_label[tgt_data.val_mask])\n",
    "    loss = src_loss + tgt_loss\n",
    "    return loss.item(), src_loss.item(), tgt_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c47f3e",
   "metadata": {},
   "source": [
    "## Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9af34247",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygNodePropPredDataset(name = dataset_name, root='/home/hhchung/data/ogb-data')\n",
    "data = dataset[0]\n",
    "data.edge_index = to_undirected(data.edge_index, data.num_nodes) # mimicking barlow twins repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6e29e",
   "metadata": {},
   "source": [
    "## Data Partition ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525c0ad",
   "metadata": {},
   "source": [
    "* Train: 0-2011\n",
    "* Val: 2012\n",
    "* Test:\n",
    "** 2013-2014 (then adapt 2012-2013 adapt-val 2014)\n",
    "** 2015-2016 (then adapt 2014-2015 adapt-val 2016)\n",
    "** 2017-2018 (then adapt 2016-2017 adapt-val 2018)\n",
    "** 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0230e3f",
   "metadata": {},
   "source": [
    "## Source Training Stage ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0515f9",
   "metadata": {},
   "source": [
    "* Train: 0-2011\n",
    "* Val: 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a633f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = data.x.shape[1]\n",
    "class_dim = data.y\n",
    "hidden_dim = 128\n",
    "emb_dim = 256\n",
    "encoder = TwoLayerGraphSAGE(feat_dim, hidden_dim, emb_dim)\n",
    "mlp = MLPHead(emb_dim, emb_dim // 4, 40)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(mlp.parameters()), lr=1e-3)\n",
    "epochs = 500\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = encoder.to(device)\n",
    "mlp = mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd5e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_2013 = temp_partition_arxiv(data, year_bound=[-1,2012,2013], proportion=1.0)\n",
    "data_2012_2013 = data_2012_2013.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "373ed8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/500 Train Loss:3.7146 Val Loss:3.6349 Val Acc:0.1565\n",
      "Epoch:2/500 Train Loss:3.6308 Val Loss:3.5643 Val Acc:0.1977\n",
      "Epoch:3/500 Train Loss:3.5491 Val Loss:3.4867 Val Acc:0.2036\n",
      "Epoch:4/500 Train Loss:3.4588 Val Loss:3.4016 Val Acc:0.2045\n",
      "Epoch:5/500 Train Loss:3.3554 Val Loss:3.3091 Val Acc:0.2045\n",
      "Epoch:6/500 Train Loss:3.2474 Val Loss:3.2373 Val Acc:0.2045\n",
      "Epoch:7/500 Train Loss:3.159 Val Loss:3.2137 Val Acc:0.2045\n",
      "Epoch:8/500 Train Loss:3.1197 Val Loss:3.2375 Val Acc:0.2045\n",
      "Epoch:9/500 Train Loss:3.1278 Val Loss:3.2435 Val Acc:0.2045\n",
      "Epoch:10/500 Train Loss:3.1227 Val Loss:3.1977 Val Acc:0.2045\n",
      "Epoch:11/500 Train Loss:3.0835 Val Loss:3.1572 Val Acc:0.2034\n",
      "Epoch:12/500 Train Loss:3.0444 Val Loss:3.1318 Val Acc:0.2026\n",
      "Epoch:13/500 Train Loss:3.014 Val Loss:3.1054 Val Acc:0.1984\n",
      "Epoch:14/500 Train Loss:3.0002 Val Loss:3.1003 Val Acc:0.1997\n",
      "Epoch:15/500 Train Loss:2.985 Val Loss:3.0831 Val Acc:0.2009\n",
      "Epoch:16/500 Train Loss:2.9754 Val Loss:3.0729 Val Acc:0.2068\n",
      "Epoch:17/500 Train Loss:2.9613 Val Loss:3.0626 Val Acc:0.207\n",
      "Epoch:18/500 Train Loss:2.9419 Val Loss:3.0551 Val Acc:0.205\n",
      "Epoch:19/500 Train Loss:2.9214 Val Loss:3.0459 Val Acc:0.2048\n",
      "Epoch:20/500 Train Loss:2.906 Val Loss:3.049 Val Acc:0.2053\n",
      "Epoch:21/500 Train Loss:2.8953 Val Loss:3.04 Val Acc:0.2047\n",
      "Epoch:22/500 Train Loss:2.8782 Val Loss:3.0268 Val Acc:0.2047\n",
      "Epoch:23/500 Train Loss:2.8634 Val Loss:3.0 Val Acc:0.205\n",
      "Epoch:24/500 Train Loss:2.8341 Val Loss:2.9766 Val Acc:0.2056\n",
      "Epoch:25/500 Train Loss:2.8117 Val Loss:2.9472 Val Acc:0.2089\n",
      "Epoch:26/500 Train Loss:2.7811 Val Loss:2.9274 Val Acc:0.2154\n",
      "Epoch:27/500 Train Loss:2.7661 Val Loss:2.9087 Val Acc:0.223\n",
      "Epoch:28/500 Train Loss:2.7452 Val Loss:2.8845 Val Acc:0.2323\n",
      "Epoch:29/500 Train Loss:2.7206 Val Loss:2.8627 Val Acc:0.2336\n",
      "Epoch:30/500 Train Loss:2.6849 Val Loss:2.8377 Val Acc:0.234\n",
      "Epoch:31/500 Train Loss:2.6561 Val Loss:2.8239 Val Acc:0.2364\n",
      "Epoch:32/500 Train Loss:2.6327 Val Loss:2.8059 Val Acc:0.2427\n",
      "Epoch:33/500 Train Loss:2.6152 Val Loss:2.7712 Val Acc:0.2514\n",
      "Epoch:34/500 Train Loss:2.5896 Val Loss:2.7462 Val Acc:0.2617\n",
      "Epoch:35/500 Train Loss:2.5623 Val Loss:2.7241 Val Acc:0.2707\n",
      "Epoch:36/500 Train Loss:2.5438 Val Loss:2.7076 Val Acc:0.2779\n",
      "Epoch:37/500 Train Loss:2.5284 Val Loss:2.6854 Val Acc:0.2811\n",
      "Epoch:38/500 Train Loss:2.4995 Val Loss:2.663 Val Acc:0.28\n",
      "Epoch:39/500 Train Loss:2.4703 Val Loss:2.654 Val Acc:0.2816\n",
      "Epoch:40/500 Train Loss:2.4514 Val Loss:2.6352 Val Acc:0.2841\n",
      "Epoch:41/500 Train Loss:2.4253 Val Loss:2.6044 Val Acc:0.2936\n",
      "Epoch:42/500 Train Loss:2.4065 Val Loss:2.5893 Val Acc:0.3005\n",
      "Epoch:43/500 Train Loss:2.3828 Val Loss:2.5678 Val Acc:0.3066\n",
      "Epoch:44/500 Train Loss:2.3625 Val Loss:2.5535 Val Acc:0.3054\n",
      "Epoch:45/500 Train Loss:2.3444 Val Loss:2.5384 Val Acc:0.3086\n",
      "Epoch:46/500 Train Loss:2.3257 Val Loss:2.5292 Val Acc:0.3075\n",
      "Epoch:47/500 Train Loss:2.3075 Val Loss:2.5186 Val Acc:0.3144\n",
      "Epoch:48/500 Train Loss:2.2894 Val Loss:2.5064 Val Acc:0.306\n",
      "Epoch:49/500 Train Loss:2.2709 Val Loss:2.4829 Val Acc:0.3144\n",
      "Epoch:50/500 Train Loss:2.2592 Val Loss:2.4763 Val Acc:0.3117\n",
      "Epoch:51/500 Train Loss:2.2438 Val Loss:2.467 Val Acc:0.3139\n",
      "Epoch:52/500 Train Loss:2.2297 Val Loss:2.4648 Val Acc:0.3139\n",
      "Epoch:53/500 Train Loss:2.2215 Val Loss:2.4456 Val Acc:0.3226\n",
      "Epoch:54/500 Train Loss:2.2054 Val Loss:2.4308 Val Acc:0.3228\n",
      "Epoch:55/500 Train Loss:2.1957 Val Loss:2.4168 Val Acc:0.3232\n",
      "Epoch:56/500 Train Loss:2.185 Val Loss:2.4127 Val Acc:0.3282\n",
      "Epoch:57/500 Train Loss:2.174 Val Loss:2.404 Val Acc:0.3293\n",
      "Epoch:58/500 Train Loss:2.1672 Val Loss:2.3976 Val Acc:0.3284\n",
      "Epoch:59/500 Train Loss:2.1559 Val Loss:2.3872 Val Acc:0.3312\n",
      "Epoch:60/500 Train Loss:2.143 Val Loss:2.3662 Val Acc:0.3358\n",
      "Epoch:61/500 Train Loss:2.1323 Val Loss:2.3571 Val Acc:0.3372\n",
      "Epoch:62/500 Train Loss:2.1204 Val Loss:2.3413 Val Acc:0.3473\n",
      "Epoch:63/500 Train Loss:2.1097 Val Loss:2.3396 Val Acc:0.3467\n",
      "Epoch:64/500 Train Loss:2.0979 Val Loss:2.3202 Val Acc:0.3475\n",
      "Epoch:65/500 Train Loss:2.0923 Val Loss:2.3143 Val Acc:0.3447\n",
      "Epoch:66/500 Train Loss:2.0795 Val Loss:2.3036 Val Acc:0.3534\n",
      "Epoch:67/500 Train Loss:2.0741 Val Loss:2.2908 Val Acc:0.356\n",
      "Epoch:68/500 Train Loss:2.0557 Val Loss:2.2748 Val Acc:0.363\n",
      "Epoch:69/500 Train Loss:2.0429 Val Loss:2.2594 Val Acc:0.3674\n",
      "Epoch:70/500 Train Loss:2.0397 Val Loss:2.2583 Val Acc:0.3643\n",
      "Epoch:71/500 Train Loss:2.0282 Val Loss:2.2361 Val Acc:0.3709\n",
      "Epoch:72/500 Train Loss:2.0135 Val Loss:2.2245 Val Acc:0.3747\n",
      "Epoch:73/500 Train Loss:2.005 Val Loss:2.2009 Val Acc:0.3796\n",
      "Epoch:74/500 Train Loss:1.9926 Val Loss:2.1983 Val Acc:0.3849\n",
      "Epoch:75/500 Train Loss:1.9791 Val Loss:2.1898 Val Acc:0.3857\n",
      "Epoch:76/500 Train Loss:1.9682 Val Loss:2.1742 Val Acc:0.389\n",
      "Epoch:77/500 Train Loss:1.9599 Val Loss:2.1574 Val Acc:0.3941\n",
      "Epoch:78/500 Train Loss:1.9486 Val Loss:2.1483 Val Acc:0.3983\n",
      "Epoch:79/500 Train Loss:1.9327 Val Loss:2.1357 Val Acc:0.4034\n",
      "Epoch:80/500 Train Loss:1.9259 Val Loss:2.1224 Val Acc:0.4068\n",
      "Epoch:81/500 Train Loss:1.9176 Val Loss:2.1058 Val Acc:0.4101\n",
      "Epoch:82/500 Train Loss:1.9003 Val Loss:2.0915 Val Acc:0.4162\n",
      "Epoch:83/500 Train Loss:1.8947 Val Loss:2.0852 Val Acc:0.4183\n",
      "Epoch:84/500 Train Loss:1.8846 Val Loss:2.0606 Val Acc:0.4252\n",
      "Epoch:85/500 Train Loss:1.8697 Val Loss:2.0548 Val Acc:0.4266\n",
      "Epoch:86/500 Train Loss:1.8647 Val Loss:2.0377 Val Acc:0.4319\n",
      "Epoch:87/500 Train Loss:1.8522 Val Loss:2.0334 Val Acc:0.4357\n",
      "Epoch:88/500 Train Loss:1.846 Val Loss:2.0199 Val Acc:0.4385\n",
      "Epoch:89/500 Train Loss:1.8312 Val Loss:2.0066 Val Acc:0.4404\n",
      "Epoch:90/500 Train Loss:1.8254 Val Loss:1.9964 Val Acc:0.4448\n",
      "Epoch:91/500 Train Loss:1.8148 Val Loss:1.9956 Val Acc:0.4449\n",
      "Epoch:92/500 Train Loss:1.8065 Val Loss:1.9786 Val Acc:0.4538\n",
      "Epoch:93/500 Train Loss:1.7974 Val Loss:1.9786 Val Acc:0.4527\n",
      "Epoch:94/500 Train Loss:1.7923 Val Loss:1.9674 Val Acc:0.4589\n",
      "Epoch:95/500 Train Loss:1.7871 Val Loss:1.9624 Val Acc:0.4545\n",
      "Epoch:96/500 Train Loss:1.7709 Val Loss:1.9582 Val Acc:0.4581\n",
      "Epoch:97/500 Train Loss:1.7706 Val Loss:1.951 Val Acc:0.4597\n",
      "Epoch:98/500 Train Loss:1.7596 Val Loss:1.9456 Val Acc:0.465\n",
      "Epoch:99/500 Train Loss:1.7528 Val Loss:1.9449 Val Acc:0.4583\n",
      "Epoch:100/500 Train Loss:1.7426 Val Loss:1.9311 Val Acc:0.4597\n",
      "Epoch:101/500 Train Loss:1.7379 Val Loss:1.9264 Val Acc:0.4682\n",
      "Epoch:102/500 Train Loss:1.732 Val Loss:1.918 Val Acc:0.4695\n",
      "Epoch:103/500 Train Loss:1.7216 Val Loss:1.9151 Val Acc:0.4659\n",
      "Epoch:104/500 Train Loss:1.7176 Val Loss:1.9008 Val Acc:0.474\n",
      "Epoch:105/500 Train Loss:1.712 Val Loss:1.8961 Val Acc:0.4726\n",
      "Epoch:106/500 Train Loss:1.7006 Val Loss:1.8869 Val Acc:0.48\n",
      "Epoch:107/500 Train Loss:1.6926 Val Loss:1.8825 Val Acc:0.4839\n",
      "Epoch:108/500 Train Loss:1.6865 Val Loss:1.876 Val Acc:0.4811\n",
      "Epoch:109/500 Train Loss:1.6796 Val Loss:1.8736 Val Acc:0.4827\n",
      "Epoch:110/500 Train Loss:1.675 Val Loss:1.8642 Val Acc:0.487\n",
      "Epoch:111/500 Train Loss:1.6701 Val Loss:1.8546 Val Acc:0.4923\n",
      "Epoch:112/500 Train Loss:1.6576 Val Loss:1.8546 Val Acc:0.4848\n",
      "Epoch:113/500 Train Loss:1.651 Val Loss:1.8455 Val Acc:0.488\n",
      "Epoch:114/500 Train Loss:1.6467 Val Loss:1.8481 Val Acc:0.487\n",
      "Epoch:115/500 Train Loss:1.6415 Val Loss:1.8376 Val Acc:0.4866\n",
      "Epoch:116/500 Train Loss:1.6319 Val Loss:1.8364 Val Acc:0.4887\n",
      "Epoch:117/500 Train Loss:1.6275 Val Loss:1.8156 Val Acc:0.4923\n",
      "Epoch:118/500 Train Loss:1.6186 Val Loss:1.8152 Val Acc:0.4951\n",
      "Epoch:119/500 Train Loss:1.6128 Val Loss:1.8204 Val Acc:0.4953\n",
      "Epoch:120/500 Train Loss:1.6062 Val Loss:1.8128 Val Acc:0.4956\n",
      "Epoch:121/500 Train Loss:1.6013 Val Loss:1.81 Val Acc:0.4939\n",
      "Epoch:122/500 Train Loss:1.5966 Val Loss:1.8062 Val Acc:0.497\n",
      "Epoch:123/500 Train Loss:1.5952 Val Loss:1.7983 Val Acc:0.5004\n",
      "Epoch:124/500 Train Loss:1.5872 Val Loss:1.7961 Val Acc:0.5032\n",
      "Epoch:125/500 Train Loss:1.5829 Val Loss:1.7975 Val Acc:0.4956\n",
      "Epoch:126/500 Train Loss:1.5797 Val Loss:1.7878 Val Acc:0.5005\n",
      "Epoch:127/500 Train Loss:1.5692 Val Loss:1.7908 Val Acc:0.5018\n",
      "Epoch:128/500 Train Loss:1.568 Val Loss:1.7781 Val Acc:0.5047\n",
      "Epoch:129/500 Train Loss:1.5659 Val Loss:1.7777 Val Acc:0.5082\n",
      "Epoch:130/500 Train Loss:1.5625 Val Loss:1.7708 Val Acc:0.5023\n",
      "Epoch:131/500 Train Loss:1.5599 Val Loss:1.7651 Val Acc:0.5019\n",
      "Epoch:132/500 Train Loss:1.5549 Val Loss:1.765 Val Acc:0.5049\n",
      "Epoch:133/500 Train Loss:1.5466 Val Loss:1.7661 Val Acc:0.5054\n",
      "Epoch:134/500 Train Loss:1.5442 Val Loss:1.755 Val Acc:0.5077\n",
      "Epoch:135/500 Train Loss:1.5409 Val Loss:1.7545 Val Acc:0.5117\n",
      "Epoch:136/500 Train Loss:1.5378 Val Loss:1.7477 Val Acc:0.4998\n",
      "Epoch:137/500 Train Loss:1.5349 Val Loss:1.7459 Val Acc:0.5122\n",
      "Epoch:138/500 Train Loss:1.5291 Val Loss:1.7466 Val Acc:0.5043\n",
      "Epoch:139/500 Train Loss:1.5236 Val Loss:1.7382 Val Acc:0.5161\n",
      "Epoch:140/500 Train Loss:1.5244 Val Loss:1.7433 Val Acc:0.5106\n",
      "Epoch:141/500 Train Loss:1.5167 Val Loss:1.7368 Val Acc:0.5099\n",
      "Epoch:142/500 Train Loss:1.5141 Val Loss:1.739 Val Acc:0.5114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:143/500 Train Loss:1.5089 Val Loss:1.7335 Val Acc:0.511\n",
      "Epoch:144/500 Train Loss:1.5028 Val Loss:1.7282 Val Acc:0.5117\n",
      "Epoch:145/500 Train Loss:1.5014 Val Loss:1.7215 Val Acc:0.5094\n",
      "Epoch:146/500 Train Loss:1.4982 Val Loss:1.7283 Val Acc:0.5172\n",
      "Epoch:147/500 Train Loss:1.5045 Val Loss:1.7207 Val Acc:0.5159\n",
      "Epoch:148/500 Train Loss:1.4934 Val Loss:1.7122 Val Acc:0.5207\n",
      "Epoch:149/500 Train Loss:1.4897 Val Loss:1.7171 Val Acc:0.5167\n",
      "Epoch:150/500 Train Loss:1.4878 Val Loss:1.7082 Val Acc:0.5201\n",
      "Epoch:151/500 Train Loss:1.4852 Val Loss:1.7078 Val Acc:0.5161\n",
      "Epoch:152/500 Train Loss:1.4839 Val Loss:1.7085 Val Acc:0.5173\n",
      "Epoch:153/500 Train Loss:1.4802 Val Loss:1.7029 Val Acc:0.5253\n",
      "Epoch:154/500 Train Loss:1.4737 Val Loss:1.705 Val Acc:0.5186\n",
      "Epoch:155/500 Train Loss:1.474 Val Loss:1.7022 Val Acc:0.5197\n",
      "Epoch:156/500 Train Loss:1.4681 Val Loss:1.6938 Val Acc:0.5277\n",
      "Epoch:157/500 Train Loss:1.4676 Val Loss:1.6962 Val Acc:0.5221\n",
      "Epoch:158/500 Train Loss:1.4619 Val Loss:1.6918 Val Acc:0.5231\n",
      "Epoch:159/500 Train Loss:1.4582 Val Loss:1.6931 Val Acc:0.5218\n",
      "Epoch:160/500 Train Loss:1.4549 Val Loss:1.6857 Val Acc:0.5242\n",
      "Epoch:161/500 Train Loss:1.4557 Val Loss:1.6804 Val Acc:0.5285\n",
      "Epoch:162/500 Train Loss:1.4558 Val Loss:1.6859 Val Acc:0.5254\n",
      "Epoch:163/500 Train Loss:1.4474 Val Loss:1.6823 Val Acc:0.5228\n",
      "Epoch:164/500 Train Loss:1.4473 Val Loss:1.6767 Val Acc:0.5304\n",
      "Epoch:165/500 Train Loss:1.4429 Val Loss:1.6726 Val Acc:0.5246\n",
      "Epoch:166/500 Train Loss:1.4498 Val Loss:1.6752 Val Acc:0.5239\n",
      "Epoch:167/500 Train Loss:1.4367 Val Loss:1.6776 Val Acc:0.5206\n",
      "Epoch:168/500 Train Loss:1.438 Val Loss:1.6723 Val Acc:0.5274\n",
      "Epoch:169/500 Train Loss:1.4341 Val Loss:1.6684 Val Acc:0.5321\n",
      "Epoch:170/500 Train Loss:1.4334 Val Loss:1.6668 Val Acc:0.5333\n",
      "Epoch:171/500 Train Loss:1.43 Val Loss:1.6723 Val Acc:0.5291\n",
      "Epoch:172/500 Train Loss:1.4291 Val Loss:1.6727 Val Acc:0.5315\n",
      "Epoch:173/500 Train Loss:1.4248 Val Loss:1.6522 Val Acc:0.5313\n",
      "Epoch:174/500 Train Loss:1.4151 Val Loss:1.6631 Val Acc:0.5304\n",
      "Epoch:175/500 Train Loss:1.4153 Val Loss:1.6567 Val Acc:0.5308\n",
      "Epoch:176/500 Train Loss:1.4159 Val Loss:1.6581 Val Acc:0.5336\n",
      "Epoch:177/500 Train Loss:1.4161 Val Loss:1.6649 Val Acc:0.5326\n",
      "Epoch:178/500 Train Loss:1.409 Val Loss:1.6533 Val Acc:0.5291\n",
      "Epoch:179/500 Train Loss:1.4075 Val Loss:1.6487 Val Acc:0.5363\n",
      "Epoch:180/500 Train Loss:1.4027 Val Loss:1.6487 Val Acc:0.535\n",
      "Epoch:181/500 Train Loss:1.4073 Val Loss:1.6467 Val Acc:0.5358\n",
      "Epoch:182/500 Train Loss:1.4052 Val Loss:1.6521 Val Acc:0.5321\n",
      "Epoch:183/500 Train Loss:1.403 Val Loss:1.6357 Val Acc:0.5395\n",
      "Epoch:184/500 Train Loss:1.3971 Val Loss:1.6399 Val Acc:0.5386\n",
      "Epoch:185/500 Train Loss:1.3964 Val Loss:1.6465 Val Acc:0.5333\n",
      "Epoch:186/500 Train Loss:1.3946 Val Loss:1.6394 Val Acc:0.5392\n",
      "Epoch:187/500 Train Loss:1.3934 Val Loss:1.6352 Val Acc:0.536\n",
      "Epoch:188/500 Train Loss:1.3878 Val Loss:1.6394 Val Acc:0.5406\n",
      "Epoch:189/500 Train Loss:1.3868 Val Loss:1.6343 Val Acc:0.5388\n",
      "Epoch:190/500 Train Loss:1.3851 Val Loss:1.6314 Val Acc:0.5385\n",
      "Epoch:191/500 Train Loss:1.3852 Val Loss:1.6317 Val Acc:0.542\n",
      "Epoch:192/500 Train Loss:1.3794 Val Loss:1.6227 Val Acc:0.5419\n",
      "Epoch:193/500 Train Loss:1.3802 Val Loss:1.6288 Val Acc:0.5414\n",
      "Epoch:194/500 Train Loss:1.3782 Val Loss:1.6291 Val Acc:0.5403\n",
      "Epoch:195/500 Train Loss:1.3765 Val Loss:1.621 Val Acc:0.5406\n",
      "Epoch:196/500 Train Loss:1.3677 Val Loss:1.6121 Val Acc:0.5422\n",
      "Epoch:197/500 Train Loss:1.3682 Val Loss:1.6134 Val Acc:0.5444\n",
      "Epoch:198/500 Train Loss:1.3668 Val Loss:1.6147 Val Acc:0.5437\n",
      "Epoch:199/500 Train Loss:1.3707 Val Loss:1.6151 Val Acc:0.542\n",
      "Epoch:200/500 Train Loss:1.3654 Val Loss:1.6215 Val Acc:0.5433\n",
      "Epoch:201/500 Train Loss:1.3614 Val Loss:1.6128 Val Acc:0.5448\n",
      "Epoch:202/500 Train Loss:1.3589 Val Loss:1.6099 Val Acc:0.5453\n",
      "Epoch:203/500 Train Loss:1.3555 Val Loss:1.6184 Val Acc:0.5422\n",
      "Epoch:204/500 Train Loss:1.3563 Val Loss:1.601 Val Acc:0.549\n",
      "Epoch:205/500 Train Loss:1.3557 Val Loss:1.5976 Val Acc:0.5453\n",
      "Epoch:206/500 Train Loss:1.3549 Val Loss:1.6055 Val Acc:0.5441\n",
      "Epoch:207/500 Train Loss:1.3499 Val Loss:1.5973 Val Acc:0.5528\n",
      "Epoch:208/500 Train Loss:1.3489 Val Loss:1.6038 Val Acc:0.5436\n",
      "Epoch:209/500 Train Loss:1.3497 Val Loss:1.6018 Val Acc:0.5467\n",
      "Epoch:210/500 Train Loss:1.3454 Val Loss:1.6005 Val Acc:0.5479\n",
      "Epoch:211/500 Train Loss:1.3438 Val Loss:1.6003 Val Acc:0.5531\n",
      "Epoch:212/500 Train Loss:1.3351 Val Loss:1.6026 Val Acc:0.5484\n",
      "Epoch:213/500 Train Loss:1.3425 Val Loss:1.598 Val Acc:0.5481\n",
      "Epoch:214/500 Train Loss:1.3353 Val Loss:1.5964 Val Acc:0.5493\n",
      "Epoch:215/500 Train Loss:1.3395 Val Loss:1.6035 Val Acc:0.5462\n",
      "Epoch:216/500 Train Loss:1.3316 Val Loss:1.591 Val Acc:0.55\n",
      "Epoch:217/500 Train Loss:1.3288 Val Loss:1.594 Val Acc:0.5554\n",
      "Epoch:218/500 Train Loss:1.3313 Val Loss:1.5876 Val Acc:0.5467\n",
      "Epoch:219/500 Train Loss:1.3295 Val Loss:1.5966 Val Acc:0.5531\n",
      "Epoch:220/500 Train Loss:1.3297 Val Loss:1.5899 Val Acc:0.5526\n",
      "Epoch:221/500 Train Loss:1.3232 Val Loss:1.5806 Val Acc:0.5543\n",
      "Epoch:222/500 Train Loss:1.3199 Val Loss:1.5853 Val Acc:0.5501\n",
      "Epoch:223/500 Train Loss:1.3208 Val Loss:1.5899 Val Acc:0.5507\n",
      "Epoch:224/500 Train Loss:1.3201 Val Loss:1.577 Val Acc:0.5534\n",
      "Epoch:225/500 Train Loss:1.3168 Val Loss:1.5784 Val Acc:0.5556\n",
      "Epoch:226/500 Train Loss:1.3205 Val Loss:1.5742 Val Acc:0.5528\n",
      "Epoch:227/500 Train Loss:1.315 Val Loss:1.5792 Val Acc:0.5535\n",
      "Epoch:228/500 Train Loss:1.3154 Val Loss:1.5693 Val Acc:0.5596\n",
      "Epoch:229/500 Train Loss:1.3091 Val Loss:1.5748 Val Acc:0.5563\n",
      "Epoch:230/500 Train Loss:1.3081 Val Loss:1.579 Val Acc:0.5556\n",
      "Epoch:231/500 Train Loss:1.3044 Val Loss:1.5806 Val Acc:0.5574\n",
      "Epoch:232/500 Train Loss:1.3023 Val Loss:1.5776 Val Acc:0.5559\n",
      "Epoch:233/500 Train Loss:1.3093 Val Loss:1.5789 Val Acc:0.5532\n",
      "Epoch:234/500 Train Loss:1.3002 Val Loss:1.5683 Val Acc:0.5521\n",
      "Epoch:235/500 Train Loss:1.3032 Val Loss:1.5692 Val Acc:0.5585\n",
      "Epoch:236/500 Train Loss:1.2941 Val Loss:1.5687 Val Acc:0.556\n",
      "Epoch:237/500 Train Loss:1.2964 Val Loss:1.5659 Val Acc:0.5602\n",
      "Epoch:238/500 Train Loss:1.2949 Val Loss:1.5711 Val Acc:0.5579\n",
      "Epoch:239/500 Train Loss:1.2913 Val Loss:1.5633 Val Acc:0.5594\n",
      "Epoch:240/500 Train Loss:1.295 Val Loss:1.5628 Val Acc:0.5618\n",
      "Epoch:241/500 Train Loss:1.2865 Val Loss:1.5567 Val Acc:0.5579\n",
      "Epoch:242/500 Train Loss:1.2864 Val Loss:1.5672 Val Acc:0.5574\n",
      "Epoch:243/500 Train Loss:1.2864 Val Loss:1.5673 Val Acc:0.5607\n",
      "Epoch:244/500 Train Loss:1.2827 Val Loss:1.5664 Val Acc:0.5616\n",
      "Epoch:245/500 Train Loss:1.2792 Val Loss:1.5703 Val Acc:0.5582\n",
      "Epoch:246/500 Train Loss:1.2785 Val Loss:1.5621 Val Acc:0.5591\n",
      "Epoch:247/500 Train Loss:1.284 Val Loss:1.5688 Val Acc:0.5535\n",
      "Epoch:248/500 Train Loss:1.2747 Val Loss:1.5625 Val Acc:0.5632\n",
      "Epoch:249/500 Train Loss:1.2774 Val Loss:1.5643 Val Acc:0.5574\n",
      "Epoch:250/500 Train Loss:1.2738 Val Loss:1.5564 Val Acc:0.5577\n",
      "Epoch:251/500 Train Loss:1.2701 Val Loss:1.5614 Val Acc:0.5608\n",
      "Epoch:252/500 Train Loss:1.2706 Val Loss:1.5562 Val Acc:0.5625\n",
      "Epoch:253/500 Train Loss:1.2663 Val Loss:1.5547 Val Acc:0.5607\n",
      "Epoch:254/500 Train Loss:1.2702 Val Loss:1.5591 Val Acc:0.5661\n",
      "Epoch:255/500 Train Loss:1.2734 Val Loss:1.5607 Val Acc:0.5632\n",
      "Epoch:256/500 Train Loss:1.268 Val Loss:1.5459 Val Acc:0.5685\n",
      "Epoch:257/500 Train Loss:1.2589 Val Loss:1.5365 Val Acc:0.5643\n",
      "Epoch:258/500 Train Loss:1.2643 Val Loss:1.5528 Val Acc:0.5655\n",
      "Epoch:259/500 Train Loss:1.2643 Val Loss:1.5486 Val Acc:0.5598\n",
      "Epoch:260/500 Train Loss:1.2639 Val Loss:1.5548 Val Acc:0.5638\n",
      "Epoch:261/500 Train Loss:1.2628 Val Loss:1.5524 Val Acc:0.5618\n",
      "Epoch:262/500 Train Loss:1.259 Val Loss:1.5556 Val Acc:0.5677\n",
      "Epoch:263/500 Train Loss:1.2611 Val Loss:1.5425 Val Acc:0.5652\n",
      "Epoch:264/500 Train Loss:1.2511 Val Loss:1.5444 Val Acc:0.5638\n",
      "Epoch:265/500 Train Loss:1.2539 Val Loss:1.5428 Val Acc:0.5664\n",
      "Epoch:266/500 Train Loss:1.2502 Val Loss:1.5521 Val Acc:0.5666\n",
      "Epoch:267/500 Train Loss:1.2535 Val Loss:1.5428 Val Acc:0.566\n",
      "Epoch:268/500 Train Loss:1.2464 Val Loss:1.5475 Val Acc:0.5649\n",
      "Epoch:269/500 Train Loss:1.2426 Val Loss:1.5351 Val Acc:0.5624\n",
      "Epoch:270/500 Train Loss:1.2431 Val Loss:1.541 Val Acc:0.5649\n",
      "Epoch:271/500 Train Loss:1.2408 Val Loss:1.5367 Val Acc:0.5692\n",
      "Epoch:272/500 Train Loss:1.2416 Val Loss:1.5341 Val Acc:0.5669\n",
      "Epoch:273/500 Train Loss:1.2412 Val Loss:1.5417 Val Acc:0.568\n",
      "Epoch:274/500 Train Loss:1.2379 Val Loss:1.5384 Val Acc:0.5683\n",
      "Epoch:275/500 Train Loss:1.2412 Val Loss:1.5456 Val Acc:0.5666\n",
      "Epoch:276/500 Train Loss:1.2392 Val Loss:1.5434 Val Acc:0.5624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:277/500 Train Loss:1.2404 Val Loss:1.5355 Val Acc:0.5667\n",
      "Epoch:278/500 Train Loss:1.2371 Val Loss:1.5374 Val Acc:0.5652\n",
      "Epoch:279/500 Train Loss:1.2351 Val Loss:1.5383 Val Acc:0.5649\n",
      "Epoch:280/500 Train Loss:1.236 Val Loss:1.5319 Val Acc:0.5652\n",
      "Epoch:281/500 Train Loss:1.2323 Val Loss:1.5374 Val Acc:0.5649\n",
      "Epoch:282/500 Train Loss:1.2312 Val Loss:1.5346 Val Acc:0.5672\n",
      "Epoch:283/500 Train Loss:1.2324 Val Loss:1.5378 Val Acc:0.5697\n",
      "Epoch:284/500 Train Loss:1.2283 Val Loss:1.5345 Val Acc:0.5644\n",
      "Epoch:285/500 Train Loss:1.2241 Val Loss:1.5422 Val Acc:0.5692\n",
      "Epoch:286/500 Train Loss:1.2288 Val Loss:1.5324 Val Acc:0.5719\n",
      "Epoch:287/500 Train Loss:1.2237 Val Loss:1.5325 Val Acc:0.5716\n",
      "Epoch:288/500 Train Loss:1.2263 Val Loss:1.5329 Val Acc:0.5709\n",
      "Epoch:289/500 Train Loss:1.2195 Val Loss:1.537 Val Acc:0.572\n",
      "Epoch:290/500 Train Loss:1.2249 Val Loss:1.5207 Val Acc:0.574\n",
      "Epoch:291/500 Train Loss:1.2193 Val Loss:1.5284 Val Acc:0.5717\n",
      "Epoch:292/500 Train Loss:1.219 Val Loss:1.547 Val Acc:0.5695\n",
      "Epoch:293/500 Train Loss:1.2172 Val Loss:1.5275 Val Acc:0.5697\n",
      "Epoch:294/500 Train Loss:1.2225 Val Loss:1.5292 Val Acc:0.5685\n",
      "Epoch:295/500 Train Loss:1.2144 Val Loss:1.5323 Val Acc:0.5669\n",
      "Epoch:296/500 Train Loss:1.2115 Val Loss:1.5335 Val Acc:0.5691\n",
      "Epoch:297/500 Train Loss:1.2103 Val Loss:1.537 Val Acc:0.5703\n",
      "Epoch:298/500 Train Loss:1.2123 Val Loss:1.5268 Val Acc:0.5692\n",
      "Epoch:299/500 Train Loss:1.2106 Val Loss:1.5247 Val Acc:0.5709\n",
      "Epoch:300/500 Train Loss:1.2097 Val Loss:1.5251 Val Acc:0.5706\n",
      "Epoch:301/500 Train Loss:1.2112 Val Loss:1.5229 Val Acc:0.5669\n",
      "Epoch:302/500 Train Loss:1.2096 Val Loss:1.5351 Val Acc:0.572\n",
      "Epoch:303/500 Train Loss:1.2014 Val Loss:1.5283 Val Acc:0.5685\n",
      "Epoch:304/500 Train Loss:1.2054 Val Loss:1.5254 Val Acc:0.5685\n",
      "Epoch:305/500 Train Loss:1.2005 Val Loss:1.5201 Val Acc:0.5709\n",
      "Epoch:306/500 Train Loss:1.204 Val Loss:1.5154 Val Acc:0.5748\n",
      "Epoch:307/500 Train Loss:1.2003 Val Loss:1.5169 Val Acc:0.5706\n",
      "Epoch:308/500 Train Loss:1.1989 Val Loss:1.5318 Val Acc:0.5702\n",
      "Epoch:309/500 Train Loss:1.1942 Val Loss:1.5142 Val Acc:0.5789\n",
      "Epoch:310/500 Train Loss:1.2001 Val Loss:1.5287 Val Acc:0.5674\n",
      "Epoch:311/500 Train Loss:1.1966 Val Loss:1.5212 Val Acc:0.575\n",
      "Epoch:312/500 Train Loss:1.198 Val Loss:1.5189 Val Acc:0.5728\n",
      "Epoch:313/500 Train Loss:1.1914 Val Loss:1.5185 Val Acc:0.5695\n",
      "Epoch:314/500 Train Loss:1.1961 Val Loss:1.5257 Val Acc:0.5768\n",
      "Epoch:315/500 Train Loss:1.1871 Val Loss:1.5191 Val Acc:0.5773\n",
      "Epoch:316/500 Train Loss:1.1937 Val Loss:1.5173 Val Acc:0.5779\n",
      "Epoch:317/500 Train Loss:1.1873 Val Loss:1.519 Val Acc:0.5672\n",
      "Epoch:318/500 Train Loss:1.1881 Val Loss:1.5059 Val Acc:0.5759\n",
      "Epoch:319/500 Train Loss:1.1908 Val Loss:1.5194 Val Acc:0.575\n",
      "Epoch:320/500 Train Loss:1.1902 Val Loss:1.5342 Val Acc:0.5708\n",
      "Epoch:321/500 Train Loss:1.1835 Val Loss:1.5272 Val Acc:0.572\n",
      "Epoch:322/500 Train Loss:1.1817 Val Loss:1.5226 Val Acc:0.5716\n",
      "Epoch:323/500 Train Loss:1.1876 Val Loss:1.5091 Val Acc:0.5663\n",
      "Epoch:324/500 Train Loss:1.1825 Val Loss:1.5207 Val Acc:0.5716\n",
      "Epoch:325/500 Train Loss:1.1837 Val Loss:1.5184 Val Acc:0.5723\n",
      "Epoch:326/500 Train Loss:1.1824 Val Loss:1.5211 Val Acc:0.5761\n",
      "Epoch:327/500 Train Loss:1.1788 Val Loss:1.5091 Val Acc:0.5784\n",
      "Epoch:328/500 Train Loss:1.1786 Val Loss:1.5177 Val Acc:0.5784\n",
      "Epoch:329/500 Train Loss:1.1798 Val Loss:1.5207 Val Acc:0.5772\n",
      "Epoch:330/500 Train Loss:1.1772 Val Loss:1.5272 Val Acc:0.5699\n",
      "Epoch:331/500 Train Loss:1.1787 Val Loss:1.521 Val Acc:0.5725\n",
      "Epoch:332/500 Train Loss:1.1778 Val Loss:1.5139 Val Acc:0.5733\n",
      "Epoch:333/500 Train Loss:1.1699 Val Loss:1.5049 Val Acc:0.5779\n",
      "Epoch:334/500 Train Loss:1.1755 Val Loss:1.5187 Val Acc:0.5736\n",
      "Epoch:335/500 Train Loss:1.1759 Val Loss:1.5107 Val Acc:0.5775\n",
      "Epoch:336/500 Train Loss:1.1713 Val Loss:1.5094 Val Acc:0.5775\n",
      "Epoch:337/500 Train Loss:1.1685 Val Loss:1.5153 Val Acc:0.5726\n",
      "Epoch:338/500 Train Loss:1.1724 Val Loss:1.5111 Val Acc:0.5747\n",
      "Epoch:339/500 Train Loss:1.1664 Val Loss:1.5159 Val Acc:0.5716\n",
      "Epoch:340/500 Train Loss:1.1602 Val Loss:1.5136 Val Acc:0.5685\n",
      "Epoch:341/500 Train Loss:1.1661 Val Loss:1.501 Val Acc:0.5787\n",
      "Epoch:342/500 Train Loss:1.1634 Val Loss:1.5137 Val Acc:0.5759\n",
      "Epoch:343/500 Train Loss:1.1684 Val Loss:1.5259 Val Acc:0.5744\n",
      "Epoch:344/500 Train Loss:1.1656 Val Loss:1.5053 Val Acc:0.5744\n",
      "Epoch:345/500 Train Loss:1.1643 Val Loss:1.5103 Val Acc:0.5768\n",
      "Epoch:346/500 Train Loss:1.1592 Val Loss:1.5078 Val Acc:0.5778\n",
      "Epoch:347/500 Train Loss:1.1616 Val Loss:1.5097 Val Acc:0.5739\n",
      "Epoch:348/500 Train Loss:1.1611 Val Loss:1.5109 Val Acc:0.5772\n",
      "Epoch:349/500 Train Loss:1.1599 Val Loss:1.5103 Val Acc:0.5772\n",
      "Epoch:350/500 Train Loss:1.1562 Val Loss:1.5192 Val Acc:0.5751\n",
      "Epoch:351/500 Train Loss:1.1545 Val Loss:1.5183 Val Acc:0.5747\n",
      "Epoch:352/500 Train Loss:1.1592 Val Loss:1.5244 Val Acc:0.5775\n",
      "Epoch:353/500 Train Loss:1.1573 Val Loss:1.495 Val Acc:0.582\n",
      "Epoch:354/500 Train Loss:1.1521 Val Loss:1.5024 Val Acc:0.5782\n",
      "Epoch:355/500 Train Loss:1.1571 Val Loss:1.5177 Val Acc:0.5744\n",
      "Epoch:356/500 Train Loss:1.155 Val Loss:1.5116 Val Acc:0.5756\n",
      "Epoch:357/500 Train Loss:1.1524 Val Loss:1.5167 Val Acc:0.5726\n",
      "Epoch:358/500 Train Loss:1.1493 Val Loss:1.5104 Val Acc:0.5736\n",
      "Epoch:359/500 Train Loss:1.1467 Val Loss:1.5074 Val Acc:0.5789\n",
      "Epoch:360/500 Train Loss:1.1515 Val Loss:1.5103 Val Acc:0.5754\n",
      "Epoch:361/500 Train Loss:1.1489 Val Loss:1.5116 Val Acc:0.5801\n",
      "Epoch:362/500 Train Loss:1.1444 Val Loss:1.5187 Val Acc:0.5739\n",
      "Epoch:363/500 Train Loss:1.1458 Val Loss:1.5178 Val Acc:0.5747\n",
      "Epoch:364/500 Train Loss:1.141 Val Loss:1.511 Val Acc:0.5817\n",
      "Epoch:365/500 Train Loss:1.148 Val Loss:1.5023 Val Acc:0.5784\n",
      "Epoch:366/500 Train Loss:1.1467 Val Loss:1.516 Val Acc:0.5786\n",
      "Epoch:367/500 Train Loss:1.1432 Val Loss:1.504 Val Acc:0.5765\n",
      "Epoch:368/500 Train Loss:1.143 Val Loss:1.5122 Val Acc:0.5768\n",
      "Epoch:369/500 Train Loss:1.1396 Val Loss:1.519 Val Acc:0.5733\n",
      "Epoch:370/500 Train Loss:1.1422 Val Loss:1.5115 Val Acc:0.5765\n",
      "Epoch:371/500 Train Loss:1.1452 Val Loss:1.5147 Val Acc:0.5725\n",
      "Epoch:372/500 Train Loss:1.1423 Val Loss:1.5151 Val Acc:0.5745\n",
      "Epoch:373/500 Train Loss:1.1412 Val Loss:1.5163 Val Acc:0.5787\n",
      "Epoch:374/500 Train Loss:1.1331 Val Loss:1.4988 Val Acc:0.5796\n",
      "Epoch:375/500 Train Loss:1.1382 Val Loss:1.4951 Val Acc:0.5837\n",
      "Epoch:376/500 Train Loss:1.1342 Val Loss:1.5062 Val Acc:0.5789\n",
      "Epoch:377/500 Train Loss:1.1323 Val Loss:1.5169 Val Acc:0.5737\n",
      "Epoch:378/500 Train Loss:1.1351 Val Loss:1.5102 Val Acc:0.5767\n",
      "Epoch:379/500 Train Loss:1.1314 Val Loss:1.4956 Val Acc:0.5849\n",
      "Epoch:380/500 Train Loss:1.1295 Val Loss:1.504 Val Acc:0.5792\n",
      "Epoch:381/500 Train Loss:1.134 Val Loss:1.5114 Val Acc:0.5748\n",
      "Epoch:382/500 Train Loss:1.1252 Val Loss:1.4969 Val Acc:0.5828\n",
      "Epoch:383/500 Train Loss:1.1333 Val Loss:1.5068 Val Acc:0.5782\n",
      "Epoch:384/500 Train Loss:1.1301 Val Loss:1.5112 Val Acc:0.5779\n",
      "Epoch:385/500 Train Loss:1.1334 Val Loss:1.5081 Val Acc:0.5767\n",
      "Epoch:386/500 Train Loss:1.1285 Val Loss:1.5053 Val Acc:0.5772\n",
      "Epoch:387/500 Train Loss:1.1287 Val Loss:1.502 Val Acc:0.5789\n",
      "Epoch:388/500 Train Loss:1.1322 Val Loss:1.5038 Val Acc:0.5737\n",
      "Epoch:389/500 Train Loss:1.1283 Val Loss:1.4956 Val Acc:0.5784\n",
      "Epoch:390/500 Train Loss:1.1224 Val Loss:1.5001 Val Acc:0.58\n",
      "Epoch:391/500 Train Loss:1.1216 Val Loss:1.5133 Val Acc:0.5792\n",
      "Epoch:392/500 Train Loss:1.1232 Val Loss:1.5116 Val Acc:0.577\n",
      "Epoch:393/500 Train Loss:1.1191 Val Loss:1.5028 Val Acc:0.5787\n",
      "Epoch:394/500 Train Loss:1.1238 Val Loss:1.5084 Val Acc:0.5758\n",
      "Epoch:395/500 Train Loss:1.1216 Val Loss:1.5089 Val Acc:0.5801\n",
      "Epoch:396/500 Train Loss:1.1243 Val Loss:1.5073 Val Acc:0.5756\n",
      "Epoch:397/500 Train Loss:1.1208 Val Loss:1.5058 Val Acc:0.5789\n",
      "Epoch:398/500 Train Loss:1.1155 Val Loss:1.5074 Val Acc:0.5787\n",
      "Epoch:399/500 Train Loss:1.1215 Val Loss:1.5127 Val Acc:0.5815\n",
      "Epoch:400/500 Train Loss:1.1186 Val Loss:1.5043 Val Acc:0.5753\n",
      "Epoch:401/500 Train Loss:1.1211 Val Loss:1.5079 Val Acc:0.5781\n",
      "Epoch:402/500 Train Loss:1.1181 Val Loss:1.5134 Val Acc:0.5768\n",
      "Epoch:403/500 Train Loss:1.1173 Val Loss:1.5005 Val Acc:0.5779\n",
      "Epoch:404/500 Train Loss:1.1092 Val Loss:1.5061 Val Acc:0.5761\n",
      "Epoch:405/500 Train Loss:1.111 Val Loss:1.5119 Val Acc:0.5753\n",
      "Epoch:406/500 Train Loss:1.1139 Val Loss:1.5077 Val Acc:0.5767\n",
      "Epoch:407/500 Train Loss:1.1158 Val Loss:1.5071 Val Acc:0.5795\n",
      "Epoch:408/500 Train Loss:1.1172 Val Loss:1.5031 Val Acc:0.5779\n",
      "Epoch:409/500 Train Loss:1.1061 Val Loss:1.5181 Val Acc:0.5784\n",
      "Epoch:410/500 Train Loss:1.1102 Val Loss:1.4989 Val Acc:0.5761\n",
      "Epoch:411/500 Train Loss:1.1136 Val Loss:1.5007 Val Acc:0.5792\n",
      "Epoch:412/500 Train Loss:1.1035 Val Loss:1.5003 Val Acc:0.5814\n",
      "Epoch:413/500 Train Loss:1.1034 Val Loss:1.5179 Val Acc:0.5818\n",
      "Epoch:414/500 Train Loss:1.1062 Val Loss:1.5035 Val Acc:0.584\n",
      "Epoch:415/500 Train Loss:1.1108 Val Loss:1.4955 Val Acc:0.5768\n",
      "Epoch:416/500 Train Loss:1.102 Val Loss:1.5096 Val Acc:0.5801\n",
      "Epoch:417/500 Train Loss:1.1003 Val Loss:1.5047 Val Acc:0.582\n",
      "Epoch:418/500 Train Loss:1.1062 Val Loss:1.5067 Val Acc:0.5754\n",
      "Epoch:419/500 Train Loss:1.0994 Val Loss:1.5102 Val Acc:0.582\n",
      "Epoch:420/500 Train Loss:1.0991 Val Loss:1.5105 Val Acc:0.5809\n",
      "Epoch:421/500 Train Loss:1.1007 Val Loss:1.502 Val Acc:0.5818\n",
      "Epoch:422/500 Train Loss:1.1004 Val Loss:1.5084 Val Acc:0.5773\n",
      "Epoch:423/500 Train Loss:1.1003 Val Loss:1.4965 Val Acc:0.5812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:424/500 Train Loss:1.0951 Val Loss:1.4919 Val Acc:0.5851\n",
      "Epoch:425/500 Train Loss:1.0918 Val Loss:1.5123 Val Acc:0.5754\n",
      "Epoch:426/500 Train Loss:1.098 Val Loss:1.4917 Val Acc:0.584\n",
      "Epoch:427/500 Train Loss:1.0969 Val Loss:1.495 Val Acc:0.5824\n",
      "Epoch:428/500 Train Loss:1.0977 Val Loss:1.5085 Val Acc:0.582\n",
      "Epoch:429/500 Train Loss:1.0968 Val Loss:1.5057 Val Acc:0.581\n",
      "Epoch:430/500 Train Loss:1.0992 Val Loss:1.5102 Val Acc:0.5823\n",
      "Epoch:431/500 Train Loss:1.0925 Val Loss:1.5002 Val Acc:0.5848\n",
      "Epoch:432/500 Train Loss:1.0905 Val Loss:1.5101 Val Acc:0.5765\n",
      "Epoch:433/500 Train Loss:1.0896 Val Loss:1.5148 Val Acc:0.5773\n",
      "Epoch:434/500 Train Loss:1.0907 Val Loss:1.5079 Val Acc:0.5812\n",
      "Epoch:435/500 Train Loss:1.0899 Val Loss:1.504 Val Acc:0.5812\n",
      "Epoch:436/500 Train Loss:1.0882 Val Loss:1.5054 Val Acc:0.5837\n",
      "Epoch:437/500 Train Loss:1.0912 Val Loss:1.5079 Val Acc:0.581\n",
      "Epoch:438/500 Train Loss:1.0877 Val Loss:1.519 Val Acc:0.5737\n",
      "Epoch:439/500 Train Loss:1.0879 Val Loss:1.5084 Val Acc:0.5843\n",
      "Epoch:440/500 Train Loss:1.0883 Val Loss:1.5021 Val Acc:0.5838\n",
      "Epoch:441/500 Train Loss:1.0863 Val Loss:1.5165 Val Acc:0.574\n",
      "Epoch:442/500 Train Loss:1.0929 Val Loss:1.5062 Val Acc:0.5817\n",
      "Epoch:443/500 Train Loss:1.084 Val Loss:1.5097 Val Acc:0.5807\n",
      "Epoch:444/500 Train Loss:1.0844 Val Loss:1.4936 Val Acc:0.582\n",
      "Epoch:445/500 Train Loss:1.0843 Val Loss:1.5014 Val Acc:0.5787\n",
      "Epoch:446/500 Train Loss:1.0837 Val Loss:1.5051 Val Acc:0.5786\n",
      "Epoch:447/500 Train Loss:1.0786 Val Loss:1.5107 Val Acc:0.5807\n",
      "Epoch:448/500 Train Loss:1.0792 Val Loss:1.5047 Val Acc:0.5767\n",
      "Epoch:449/500 Train Loss:1.08 Val Loss:1.5109 Val Acc:0.58\n",
      "Epoch:450/500 Train Loss:1.0821 Val Loss:1.5141 Val Acc:0.5798\n",
      "Epoch:451/500 Train Loss:1.0839 Val Loss:1.5116 Val Acc:0.5846\n",
      "Epoch:452/500 Train Loss:1.073 Val Loss:1.4963 Val Acc:0.5835\n",
      "Epoch:453/500 Train Loss:1.0772 Val Loss:1.4941 Val Acc:0.5841\n",
      "Epoch:454/500 Train Loss:1.0836 Val Loss:1.5134 Val Acc:0.5772\n",
      "Epoch:455/500 Train Loss:1.0745 Val Loss:1.4939 Val Acc:0.5835\n",
      "Epoch:456/500 Train Loss:1.0771 Val Loss:1.504 Val Acc:0.5762\n",
      "Epoch:457/500 Train Loss:1.0735 Val Loss:1.5112 Val Acc:0.5739\n",
      "Epoch:458/500 Train Loss:1.0782 Val Loss:1.5069 Val Acc:0.5841\n",
      "Epoch:459/500 Train Loss:1.0769 Val Loss:1.5114 Val Acc:0.5829\n",
      "Epoch:460/500 Train Loss:1.0744 Val Loss:1.5017 Val Acc:0.5865\n",
      "Epoch:461/500 Train Loss:1.0718 Val Loss:1.5025 Val Acc:0.5809\n",
      "Epoch:462/500 Train Loss:1.0682 Val Loss:1.506 Val Acc:0.5798\n",
      "Epoch:463/500 Train Loss:1.0739 Val Loss:1.5114 Val Acc:0.5789\n",
      "Epoch:464/500 Train Loss:1.0753 Val Loss:1.4942 Val Acc:0.5796\n",
      "Epoch:465/500 Train Loss:1.0695 Val Loss:1.4959 Val Acc:0.5838\n",
      "Epoch:466/500 Train Loss:1.0629 Val Loss:1.4971 Val Acc:0.5838\n",
      "Epoch:467/500 Train Loss:1.0673 Val Loss:1.5051 Val Acc:0.5826\n",
      "Epoch:468/500 Train Loss:1.0684 Val Loss:1.4953 Val Acc:0.5814\n",
      "Epoch:469/500 Train Loss:1.0718 Val Loss:1.5109 Val Acc:0.5812\n",
      "Epoch:470/500 Train Loss:1.0655 Val Loss:1.4947 Val Acc:0.5815\n",
      "Epoch:471/500 Train Loss:1.0682 Val Loss:1.5031 Val Acc:0.5765\n",
      "Epoch:472/500 Train Loss:1.0635 Val Loss:1.5119 Val Acc:0.5803\n",
      "Epoch:473/500 Train Loss:1.0612 Val Loss:1.5086 Val Acc:0.5835\n",
      "Epoch:474/500 Train Loss:1.0645 Val Loss:1.498 Val Acc:0.5817\n",
      "Epoch:475/500 Train Loss:1.0668 Val Loss:1.4919 Val Acc:0.5841\n",
      "Epoch:476/500 Train Loss:1.0638 Val Loss:1.515 Val Acc:0.5789\n",
      "Epoch:477/500 Train Loss:1.0613 Val Loss:1.5088 Val Acc:0.5804\n",
      "Epoch:478/500 Train Loss:1.0556 Val Loss:1.4878 Val Acc:0.584\n",
      "Epoch:479/500 Train Loss:1.0615 Val Loss:1.4956 Val Acc:0.5829\n",
      "Epoch:480/500 Train Loss:1.0614 Val Loss:1.5081 Val Acc:0.5852\n",
      "Epoch:481/500 Train Loss:1.0591 Val Loss:1.4954 Val Acc:0.5838\n",
      "Epoch:482/500 Train Loss:1.0583 Val Loss:1.508 Val Acc:0.581\n",
      "Epoch:483/500 Train Loss:1.0559 Val Loss:1.5016 Val Acc:0.582\n",
      "Epoch:484/500 Train Loss:1.0511 Val Loss:1.5135 Val Acc:0.579\n",
      "Epoch:485/500 Train Loss:1.0608 Val Loss:1.5033 Val Acc:0.5837\n",
      "Epoch:486/500 Train Loss:1.0566 Val Loss:1.5117 Val Acc:0.5804\n",
      "Epoch:487/500 Train Loss:1.0526 Val Loss:1.5062 Val Acc:0.5776\n",
      "Epoch:488/500 Train Loss:1.0518 Val Loss:1.5033 Val Acc:0.5806\n",
      "Epoch:489/500 Train Loss:1.0569 Val Loss:1.5125 Val Acc:0.5796\n",
      "Epoch:490/500 Train Loss:1.0473 Val Loss:1.5005 Val Acc:0.5786\n",
      "Epoch:491/500 Train Loss:1.0512 Val Loss:1.5033 Val Acc:0.5866\n",
      "Epoch:492/500 Train Loss:1.051 Val Loss:1.5037 Val Acc:0.5857\n",
      "Epoch:493/500 Train Loss:1.0535 Val Loss:1.4962 Val Acc:0.5818\n",
      "Epoch:494/500 Train Loss:1.0494 Val Loss:1.5112 Val Acc:0.5826\n",
      "Epoch:495/500 Train Loss:1.0535 Val Loss:1.5013 Val Acc:0.5817\n",
      "Epoch:496/500 Train Loss:1.0492 Val Loss:1.4917 Val Acc:0.5866\n",
      "Epoch:497/500 Train Loss:1.0457 Val Loss:1.5087 Val Acc:0.5798\n",
      "Epoch:498/500 Train Loss:1.0454 Val Loss:1.5074 Val Acc:0.5779\n",
      "Epoch:499/500 Train Loss:1.045 Val Loss:1.498 Val Acc:0.5838\n",
      "Epoch:500/500 Train Loss:1.0463 Val Loss:1.499 Val Acc:0.584\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_encoder = None\n",
    "best_mlp = None\n",
    "evaluator = Evaluator(name='ogbn-arxiv')\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = train(encoder, mlp, optimizer, data_2012_2013)\n",
    "    val_loss, val_acc = test(encoder, mlp, data_2012_2013, evaluator)\n",
    "    print(f\"Epoch:{e}/{epochs} Train Loss:{round(train_loss,4)} Val Loss:{round(val_loss,4)} Val Acc:{round(val_acc, 4)}\")\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_encoder = deepcopy(encoder)\n",
    "        best_mlp = deepcopy(mlp)\n",
    "\n",
    "encoder = deepcopy(best_encoder)\n",
    "mlp = deepcopy(best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e6cea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5866355866355867"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888217b",
   "metadata": {},
   "source": [
    "# Prequential Evaluation at Subsequent Time Steps #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c97a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continual_adapt(src_data, tgt_split, encoder, mlp, device, lr=1e-3):\n",
    "    print(\"Start partitioning data...\")\n",
    "    tgt_data = temp_partition_arxiv(data, year_bound=tgt_split, proportion=1.0)\n",
    "    tgt_data.to(device)\n",
    "    print(\"Finish partitioning data...\")\n",
    "    tgt_encoder, tgt_mlp = deepcopy(encoder), deepcopy(mlp)\n",
    "    tgt_optimizer = torch.optim.Adam(list(tgt_encoder.parameters()) + list(tgt_mlp.parameters()), lr=lr)\n",
    "    \n",
    "    epochs = 500\n",
    "    best_val_loss = np.inf\n",
    "    best_tgt_encoder, best_tgt_mlp = None, None\n",
    "    patience = 10\n",
    "    staleness = 0\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        train_loss, train_src_loss, train_tgt_loss = adapt_train(tgt_encoder, tgt_mlp, src_data, tgt_data, tgt_optimizer)\n",
    "        val_loss, val_src_loss, val_tgt_loss = adapt_val(tgt_encoder, tgt_mlp, src_data, tgt_data)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_tgt_encoder = deepcopy(tgt_encoder)\n",
    "            best_tgt_mlp = deepcopy(tgt_mlp)\n",
    "            staleness = 0\n",
    "        else:\n",
    "            staleness += 1\n",
    "        print(f'Epoch {e}/{epochs} Train Loss: {round(train_loss,5)} Train Src Loss: {round(train_src_loss,5)} Train Tgt Loss: {round(train_tgt_loss,5)} \\n \\\n",
    "                Val Loss: {round(val_loss,5)} Val Src Loss: {round(val_src_loss,5)} Val Tgt Loss: {round(val_tgt_loss,5)}')\n",
    "        if staleness > patience:\n",
    "            break\n",
    "\n",
    "    tgt_encoder = deepcopy(best_tgt_encoder)\n",
    "    tgt_mlp = deepcopy(best_tgt_mlp)\n",
    "    \n",
    "    return tgt_encoder, tgt_mlp, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cc1a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298845d3",
   "metadata": {},
   "source": [
    "## 2013-2014 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c33a9c",
   "metadata": {},
   "source": [
    "* Test:\n",
    "** 2013-2014 (then adapt 2012-2013 adapt-val 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe91203",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2013_2015 = temp_partition_arxiv(data, year_bound=[-1,2013,2015], proportion=1.0)\n",
    "data_2013_2015 = data_2013_2015.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd7cba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder, mlp, data_2013_2015, evaluator)\n",
    "test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aea6798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.476 Test Acc: 0.591\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {round(test_loss,3)} Test Acc: {round(test_acc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "055bb2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start partitioning data...\n",
      "Finish partitioning data...\n",
      "Epoch 1/500 Train Loss: 1.72277 Train Src Loss: 1.04937 Train Tgt Loss: 0.6734 \n",
      "                 Val Loss: 2.24007 Val Src Loss: 1.59449 Val Tgt Loss: 0.64557\n",
      "Epoch 2/500 Train Loss: 1.71361 Train Src Loss: 1.14284 Train Tgt Loss: 0.57077 \n",
      "                 Val Loss: 2.21706 Val Src Loss: 1.58068 Val Tgt Loss: 0.63638\n",
      "Epoch 3/500 Train Loss: 1.68259 Train Src Loss: 1.11978 Train Tgt Loss: 0.56281 \n",
      "                 Val Loss: 2.23034 Val Src Loss: 1.59596 Val Tgt Loss: 0.63438\n",
      "Epoch 4/500 Train Loss: 1.66686 Train Src Loss: 1.09917 Train Tgt Loss: 0.56769 \n",
      "                 Val Loss: 2.21911 Val Src Loss: 1.58515 Val Tgt Loss: 0.63396\n",
      "Epoch 5/500 Train Loss: 1.67454 Train Src Loss: 1.10594 Train Tgt Loss: 0.5686 \n",
      "                 Val Loss: 2.21107 Val Src Loss: 1.58444 Val Tgt Loss: 0.62663\n",
      "Epoch 6/500 Train Loss: 1.65858 Train Src Loss: 1.09794 Train Tgt Loss: 0.56064 \n",
      "                 Val Loss: 2.20128 Val Src Loss: 1.59144 Val Tgt Loss: 0.60984\n",
      "Epoch 7/500 Train Loss: 1.64547 Train Src Loss: 1.10434 Train Tgt Loss: 0.54113 \n",
      "                 Val Loss: 2.17288 Val Src Loss: 1.5725 Val Tgt Loss: 0.60038\n",
      "Epoch 8/500 Train Loss: 1.6374 Train Src Loss: 1.10805 Train Tgt Loss: 0.52934 \n",
      "                 Val Loss: 2.16515 Val Src Loss: 1.56866 Val Tgt Loss: 0.59649\n",
      "Epoch 9/500 Train Loss: 1.63037 Train Src Loss: 1.11179 Train Tgt Loss: 0.51858 \n",
      "                 Val Loss: 2.17091 Val Src Loss: 1.57741 Val Tgt Loss: 0.5935\n",
      "Epoch 10/500 Train Loss: 1.62247 Train Src Loss: 1.10341 Train Tgt Loss: 0.51906 \n",
      "                 Val Loss: 2.16561 Val Src Loss: 1.56825 Val Tgt Loss: 0.59736\n",
      "Epoch 11/500 Train Loss: 1.63271 Train Src Loss: 1.1064 Train Tgt Loss: 0.5263 \n",
      "                 Val Loss: 2.1497 Val Src Loss: 1.55508 Val Tgt Loss: 0.59462\n",
      "Epoch 12/500 Train Loss: 1.62137 Train Src Loss: 1.09971 Train Tgt Loss: 0.52166 \n",
      "                 Val Loss: 2.15743 Val Src Loss: 1.55939 Val Tgt Loss: 0.59803\n",
      "Epoch 13/500 Train Loss: 1.61935 Train Src Loss: 1.09676 Train Tgt Loss: 0.52259 \n",
      "                 Val Loss: 2.1546 Val Src Loss: 1.56989 Val Tgt Loss: 0.58471\n",
      "Epoch 14/500 Train Loss: 1.6154 Train Src Loss: 1.09594 Train Tgt Loss: 0.51946 \n",
      "                 Val Loss: 2.15741 Val Src Loss: 1.56937 Val Tgt Loss: 0.58804\n",
      "Epoch 15/500 Train Loss: 1.61599 Train Src Loss: 1.10646 Train Tgt Loss: 0.50953 \n",
      "                 Val Loss: 2.15934 Val Src Loss: 1.57205 Val Tgt Loss: 0.58729\n",
      "Epoch 16/500 Train Loss: 1.61085 Train Src Loss: 1.09847 Train Tgt Loss: 0.51238 \n",
      "                 Val Loss: 2.15403 Val Src Loss: 1.5658 Val Tgt Loss: 0.58823\n",
      "Epoch 17/500 Train Loss: 1.60352 Train Src Loss: 1.09482 Train Tgt Loss: 0.5087 \n",
      "                 Val Loss: 2.16613 Val Src Loss: 1.58268 Val Tgt Loss: 0.58345\n",
      "Epoch 18/500 Train Loss: 1.60828 Train Src Loss: 1.10074 Train Tgt Loss: 0.50754 \n",
      "                 Val Loss: 2.15367 Val Src Loss: 1.57186 Val Tgt Loss: 0.58181\n",
      "Epoch 19/500 Train Loss: 1.60051 Train Src Loss: 1.09535 Train Tgt Loss: 0.50516 \n",
      "                 Val Loss: 2.15684 Val Src Loss: 1.58644 Val Tgt Loss: 0.5704\n",
      "Epoch 20/500 Train Loss: 1.60495 Train Src Loss: 1.1028 Train Tgt Loss: 0.50215 \n",
      "                 Val Loss: 2.15295 Val Src Loss: 1.57955 Val Tgt Loss: 0.5734\n",
      "Epoch 21/500 Train Loss: 1.60182 Train Src Loss: 1.10904 Train Tgt Loss: 0.49278 \n",
      "                 Val Loss: 2.16385 Val Src Loss: 1.59293 Val Tgt Loss: 0.57092\n",
      "Epoch 22/500 Train Loss: 1.59499 Train Src Loss: 1.10536 Train Tgt Loss: 0.48962 \n",
      "                 Val Loss: 2.14185 Val Src Loss: 1.57246 Val Tgt Loss: 0.56939\n",
      "Epoch 23/500 Train Loss: 1.5955 Train Src Loss: 1.10056 Train Tgt Loss: 0.49494 \n",
      "                 Val Loss: 2.16201 Val Src Loss: 1.58238 Val Tgt Loss: 0.57963\n",
      "Epoch 24/500 Train Loss: 1.59681 Train Src Loss: 1.09756 Train Tgt Loss: 0.49925 \n",
      "                 Val Loss: 2.14277 Val Src Loss: 1.56934 Val Tgt Loss: 0.57343\n",
      "Epoch 25/500 Train Loss: 1.59237 Train Src Loss: 1.0935 Train Tgt Loss: 0.49888 \n",
      "                 Val Loss: 2.14878 Val Src Loss: 1.57575 Val Tgt Loss: 0.57303\n",
      "Epoch 26/500 Train Loss: 1.59685 Train Src Loss: 1.09332 Train Tgt Loss: 0.50354 \n",
      "                 Val Loss: 2.14789 Val Src Loss: 1.57158 Val Tgt Loss: 0.57631\n",
      "Epoch 27/500 Train Loss: 1.59506 Train Src Loss: 1.09545 Train Tgt Loss: 0.49961 \n",
      "                 Val Loss: 2.13768 Val Src Loss: 1.56444 Val Tgt Loss: 0.57324\n",
      "Epoch 28/500 Train Loss: 1.58642 Train Src Loss: 1.08896 Train Tgt Loss: 0.49747 \n",
      "                 Val Loss: 2.13656 Val Src Loss: 1.5739 Val Tgt Loss: 0.56266\n",
      "Epoch 29/500 Train Loss: 1.58066 Train Src Loss: 1.09394 Train Tgt Loss: 0.48672 \n",
      "                 Val Loss: 2.16705 Val Src Loss: 1.60137 Val Tgt Loss: 0.56568\n",
      "Epoch 30/500 Train Loss: 1.58134 Train Src Loss: 1.09428 Train Tgt Loss: 0.48706 \n",
      "                 Val Loss: 2.15534 Val Src Loss: 1.59913 Val Tgt Loss: 0.55621\n",
      "Epoch 31/500 Train Loss: 1.58845 Train Src Loss: 1.10101 Train Tgt Loss: 0.48745 \n",
      "                 Val Loss: 2.15183 Val Src Loss: 1.58797 Val Tgt Loss: 0.56386\n",
      "Epoch 32/500 Train Loss: 1.58005 Train Src Loss: 1.09717 Train Tgt Loss: 0.48289 \n",
      "                 Val Loss: 2.13724 Val Src Loss: 1.57818 Val Tgt Loss: 0.55906\n",
      "Epoch 33/500 Train Loss: 1.58628 Train Src Loss: 1.10096 Train Tgt Loss: 0.48533 \n",
      "                 Val Loss: 2.13365 Val Src Loss: 1.57455 Val Tgt Loss: 0.5591\n",
      "Epoch 34/500 Train Loss: 1.57529 Train Src Loss: 1.09015 Train Tgt Loss: 0.48514 \n",
      "                 Val Loss: 2.15289 Val Src Loss: 1.58594 Val Tgt Loss: 0.56695\n",
      "Epoch 35/500 Train Loss: 1.58134 Train Src Loss: 1.09101 Train Tgt Loss: 0.49033 \n",
      "                 Val Loss: 2.14915 Val Src Loss: 1.58958 Val Tgt Loss: 0.55957\n",
      "Epoch 36/500 Train Loss: 1.5709 Train Src Loss: 1.08839 Train Tgt Loss: 0.48251 \n",
      "                 Val Loss: 2.14483 Val Src Loss: 1.58306 Val Tgt Loss: 0.56178\n",
      "Epoch 37/500 Train Loss: 1.57669 Train Src Loss: 1.09277 Train Tgt Loss: 0.48392 \n",
      "                 Val Loss: 2.13091 Val Src Loss: 1.57171 Val Tgt Loss: 0.55919\n",
      "Epoch 38/500 Train Loss: 1.5714 Train Src Loss: 1.09394 Train Tgt Loss: 0.47746 \n",
      "                 Val Loss: 2.12446 Val Src Loss: 1.56678 Val Tgt Loss: 0.55768\n",
      "Epoch 39/500 Train Loss: 1.56886 Train Src Loss: 1.09373 Train Tgt Loss: 0.47513 \n",
      "                 Val Loss: 2.13858 Val Src Loss: 1.58344 Val Tgt Loss: 0.55514\n",
      "Epoch 40/500 Train Loss: 1.57631 Train Src Loss: 1.09476 Train Tgt Loss: 0.48155 \n",
      "                 Val Loss: 2.1305 Val Src Loss: 1.56379 Val Tgt Loss: 0.56672\n",
      "Epoch 41/500 Train Loss: 1.57773 Train Src Loss: 1.09032 Train Tgt Loss: 0.48741 \n",
      "                 Val Loss: 2.14737 Val Src Loss: 1.58623 Val Tgt Loss: 0.56114\n",
      "Epoch 42/500 Train Loss: 1.55979 Train Src Loss: 1.08332 Train Tgt Loss: 0.47647 \n",
      "                 Val Loss: 2.12526 Val Src Loss: 1.57718 Val Tgt Loss: 0.54808\n",
      "Epoch 43/500 Train Loss: 1.55957 Train Src Loss: 1.08528 Train Tgt Loss: 0.47429 \n",
      "                 Val Loss: 2.1231 Val Src Loss: 1.57329 Val Tgt Loss: 0.54981\n",
      "Epoch 44/500 Train Loss: 1.55534 Train Src Loss: 1.08338 Train Tgt Loss: 0.47196 \n",
      "                 Val Loss: 2.13446 Val Src Loss: 1.57676 Val Tgt Loss: 0.55771\n",
      "Epoch 45/500 Train Loss: 1.56073 Train Src Loss: 1.08763 Train Tgt Loss: 0.4731 \n",
      "                 Val Loss: 2.13001 Val Src Loss: 1.57631 Val Tgt Loss: 0.5537\n",
      "Epoch 46/500 Train Loss: 1.55917 Train Src Loss: 1.07937 Train Tgt Loss: 0.4798 \n",
      "                 Val Loss: 2.1429 Val Src Loss: 1.58377 Val Tgt Loss: 0.55912\n",
      "Epoch 47/500 Train Loss: 1.55735 Train Src Loss: 1.08173 Train Tgt Loss: 0.47562 \n",
      "                 Val Loss: 2.11967 Val Src Loss: 1.57171 Val Tgt Loss: 0.54795\n",
      "Epoch 48/500 Train Loss: 1.5588 Train Src Loss: 1.08424 Train Tgt Loss: 0.47456 \n",
      "                 Val Loss: 2.12933 Val Src Loss: 1.58531 Val Tgt Loss: 0.54402\n",
      "Epoch 49/500 Train Loss: 1.5621 Train Src Loss: 1.08705 Train Tgt Loss: 0.47505 \n",
      "                 Val Loss: 2.13342 Val Src Loss: 1.58831 Val Tgt Loss: 0.54511\n",
      "Epoch 50/500 Train Loss: 1.55687 Train Src Loss: 1.08468 Train Tgt Loss: 0.4722 \n",
      "                 Val Loss: 2.14234 Val Src Loss: 1.58695 Val Tgt Loss: 0.55539\n",
      "Epoch 51/500 Train Loss: 1.56367 Train Src Loss: 1.08506 Train Tgt Loss: 0.47861 \n",
      "                 Val Loss: 2.14355 Val Src Loss: 1.58562 Val Tgt Loss: 0.55793\n",
      "Epoch 52/500 Train Loss: 1.56027 Train Src Loss: 1.0856 Train Tgt Loss: 0.47467 \n",
      "                 Val Loss: 2.12647 Val Src Loss: 1.5766 Val Tgt Loss: 0.54987\n",
      "Epoch 53/500 Train Loss: 1.54734 Train Src Loss: 1.07614 Train Tgt Loss: 0.4712 \n",
      "                 Val Loss: 2.14073 Val Src Loss: 1.58235 Val Tgt Loss: 0.55838\n",
      "Epoch 54/500 Train Loss: 1.55016 Train Src Loss: 1.07805 Train Tgt Loss: 0.47211 \n",
      "                 Val Loss: 2.12605 Val Src Loss: 1.57146 Val Tgt Loss: 0.55458\n",
      "Epoch 55/500 Train Loss: 1.54777 Train Src Loss: 1.07577 Train Tgt Loss: 0.472 \n",
      "                 Val Loss: 2.12884 Val Src Loss: 1.57935 Val Tgt Loss: 0.54949\n",
      "Epoch 56/500 Train Loss: 1.54361 Train Src Loss: 1.07981 Train Tgt Loss: 0.4638 \n",
      "                 Val Loss: 2.12256 Val Src Loss: 1.57869 Val Tgt Loss: 0.54387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500 Train Loss: 1.54431 Train Src Loss: 1.08128 Train Tgt Loss: 0.46303 \n",
      "                 Val Loss: 2.12513 Val Src Loss: 1.5828 Val Tgt Loss: 0.54233\n",
      "Epoch 58/500 Train Loss: 1.53494 Train Src Loss: 1.07516 Train Tgt Loss: 0.45978 \n",
      "                 Val Loss: 2.1492 Val Src Loss: 1.60152 Val Tgt Loss: 0.54768\n"
     ]
    }
   ],
   "source": [
    "encoder_2012_2014_2015, mlp_2012_2014_2015, best_val_loss = continual_adapt(data_2012_2013, [2012,2014,2015], encoder, mlp, device, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d69e18b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Val Loss: 2.11966609954834\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Val Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f73924",
   "metadata": {},
   "source": [
    "## 2015-2016 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa950b02",
   "metadata": {},
   "source": [
    "* Test:\n",
    "** 2015-2016 (then adapt 2014-2015 adapt-val 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0ee8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2015_2017 = temp_partition_arxiv(data, year_bound=[-1,2015,2017], proportion=1.0)\n",
    "data_2015_2017 = data_2015_2017.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ad9c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder_2012_2014_2015, mlp_2012_2014_2015, data_2015_2017, evaluator)\n",
    "test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40d59936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.655 Test Acc: 0.569\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {round(test_loss,3)} Test Acc: {round(test_acc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "695410a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start partitioning data...\n",
      "Finish partitioning data...\n",
      "Epoch 1/500 Train Loss: 1.60251 Train Src Loss: 1.08336 Train Tgt Loss: 0.51916 \n",
      "                 Val Loss: 2.18959 Val Src Loss: 1.6651 Val Tgt Loss: 0.52448\n",
      "Epoch 2/500 Train Loss: 1.6787 Train Src Loss: 1.16422 Train Tgt Loss: 0.51448 \n",
      "                 Val Loss: 2.11771 Val Src Loss: 1.57099 Val Tgt Loss: 0.54672\n",
      "Epoch 3/500 Train Loss: 1.60331 Train Src Loss: 1.0896 Train Tgt Loss: 0.51371 \n",
      "                 Val Loss: 2.12416 Val Src Loss: 1.58443 Val Tgt Loss: 0.53973\n",
      "Epoch 4/500 Train Loss: 1.62159 Train Src Loss: 1.10729 Train Tgt Loss: 0.5143 \n",
      "                 Val Loss: 2.14295 Val Src Loss: 1.60942 Val Tgt Loss: 0.53353\n",
      "Epoch 5/500 Train Loss: 1.62386 Train Src Loss: 1.11425 Train Tgt Loss: 0.50962 \n",
      "                 Val Loss: 2.09815 Val Src Loss: 1.58373 Val Tgt Loss: 0.51442\n",
      "Epoch 6/500 Train Loss: 1.61067 Train Src Loss: 1.10898 Train Tgt Loss: 0.50169 \n",
      "                 Val Loss: 2.09456 Val Src Loss: 1.58115 Val Tgt Loss: 0.51341\n",
      "Epoch 7/500 Train Loss: 1.60128 Train Src Loss: 1.10262 Train Tgt Loss: 0.49866 \n",
      "                 Val Loss: 2.09826 Val Src Loss: 1.58394 Val Tgt Loss: 0.51433\n",
      "Epoch 8/500 Train Loss: 1.58979 Train Src Loss: 1.09414 Train Tgt Loss: 0.49564 \n",
      "                 Val Loss: 2.09219 Val Src Loss: 1.57985 Val Tgt Loss: 0.51234\n",
      "Epoch 9/500 Train Loss: 1.59447 Train Src Loss: 1.09427 Train Tgt Loss: 0.5002 \n",
      "                 Val Loss: 2.10946 Val Src Loss: 1.58698 Val Tgt Loss: 0.52249\n",
      "Epoch 10/500 Train Loss: 1.591 Train Src Loss: 1.08367 Train Tgt Loss: 0.50733 \n",
      "                 Val Loss: 2.12646 Val Src Loss: 1.60952 Val Tgt Loss: 0.51695\n",
      "Epoch 11/500 Train Loss: 1.59635 Train Src Loss: 1.09244 Train Tgt Loss: 0.50391 \n",
      "                 Val Loss: 2.10148 Val Src Loss: 1.5882 Val Tgt Loss: 0.51328\n",
      "Epoch 12/500 Train Loss: 1.58043 Train Src Loss: 1.08249 Train Tgt Loss: 0.49794 \n",
      "                 Val Loss: 2.08765 Val Src Loss: 1.58532 Val Tgt Loss: 0.50232\n",
      "Epoch 13/500 Train Loss: 1.58816 Train Src Loss: 1.09163 Train Tgt Loss: 0.49653 \n",
      "                 Val Loss: 2.08461 Val Src Loss: 1.58615 Val Tgt Loss: 0.49846\n",
      "Epoch 14/500 Train Loss: 1.58599 Train Src Loss: 1.09663 Train Tgt Loss: 0.48936 \n",
      "                 Val Loss: 2.06923 Val Src Loss: 1.56563 Val Tgt Loss: 0.5036\n",
      "Epoch 15/500 Train Loss: 1.57229 Train Src Loss: 1.08305 Train Tgt Loss: 0.48924 \n",
      "                 Val Loss: 2.08641 Val Src Loss: 1.58281 Val Tgt Loss: 0.50359\n",
      "Epoch 16/500 Train Loss: 1.57264 Train Src Loss: 1.0814 Train Tgt Loss: 0.49124 \n",
      "                 Val Loss: 2.07422 Val Src Loss: 1.57397 Val Tgt Loss: 0.50024\n",
      "Epoch 17/500 Train Loss: 1.57017 Train Src Loss: 1.08187 Train Tgt Loss: 0.4883 \n",
      "                 Val Loss: 2.0705 Val Src Loss: 1.57032 Val Tgt Loss: 0.50018\n",
      "Epoch 18/500 Train Loss: 1.57045 Train Src Loss: 1.08326 Train Tgt Loss: 0.48719 \n",
      "                 Val Loss: 2.08583 Val Src Loss: 1.58512 Val Tgt Loss: 0.50072\n",
      "Epoch 19/500 Train Loss: 1.563 Train Src Loss: 1.07562 Train Tgt Loss: 0.48738 \n",
      "                 Val Loss: 2.06561 Val Src Loss: 1.57188 Val Tgt Loss: 0.49373\n",
      "Epoch 20/500 Train Loss: 1.56761 Train Src Loss: 1.08311 Train Tgt Loss: 0.4845 \n",
      "                 Val Loss: 2.06823 Val Src Loss: 1.58288 Val Tgt Loss: 0.48535\n",
      "Epoch 21/500 Train Loss: 1.56543 Train Src Loss: 1.08549 Train Tgt Loss: 0.47994 \n",
      "                 Val Loss: 2.08098 Val Src Loss: 1.59847 Val Tgt Loss: 0.48251\n",
      "Epoch 22/500 Train Loss: 1.56394 Train Src Loss: 1.08405 Train Tgt Loss: 0.47989 \n",
      "                 Val Loss: 2.08409 Val Src Loss: 1.59674 Val Tgt Loss: 0.48735\n",
      "Epoch 23/500 Train Loss: 1.56767 Train Src Loss: 1.08881 Train Tgt Loss: 0.47886 \n",
      "                 Val Loss: 2.05808 Val Src Loss: 1.57722 Val Tgt Loss: 0.48086\n",
      "Epoch 24/500 Train Loss: 1.56041 Train Src Loss: 1.08592 Train Tgt Loss: 0.47449 \n",
      "                 Val Loss: 2.07054 Val Src Loss: 1.58568 Val Tgt Loss: 0.48487\n",
      "Epoch 25/500 Train Loss: 1.55238 Train Src Loss: 1.07927 Train Tgt Loss: 0.47311 \n",
      "                 Val Loss: 2.06035 Val Src Loss: 1.57156 Val Tgt Loss: 0.48879\n",
      "Epoch 26/500 Train Loss: 1.54456 Train Src Loss: 1.07145 Train Tgt Loss: 0.47311 \n",
      "                 Val Loss: 2.05583 Val Src Loss: 1.57441 Val Tgt Loss: 0.48142\n",
      "Epoch 27/500 Train Loss: 1.55507 Train Src Loss: 1.07602 Train Tgt Loss: 0.47906 \n",
      "                 Val Loss: 2.0735 Val Src Loss: 1.58456 Val Tgt Loss: 0.48894\n",
      "Epoch 28/500 Train Loss: 1.55304 Train Src Loss: 1.07478 Train Tgt Loss: 0.47826 \n",
      "                 Val Loss: 2.0712 Val Src Loss: 1.58438 Val Tgt Loss: 0.48683\n",
      "Epoch 29/500 Train Loss: 1.5502 Train Src Loss: 1.06946 Train Tgt Loss: 0.48074 \n",
      "                 Val Loss: 2.05136 Val Src Loss: 1.56288 Val Tgt Loss: 0.48848\n",
      "Epoch 30/500 Train Loss: 1.54996 Train Src Loss: 1.07347 Train Tgt Loss: 0.47649 \n",
      "                 Val Loss: 2.04409 Val Src Loss: 1.56088 Val Tgt Loss: 0.48321\n",
      "Epoch 31/500 Train Loss: 1.54673 Train Src Loss: 1.06919 Train Tgt Loss: 0.47754 \n",
      "                 Val Loss: 2.04144 Val Src Loss: 1.56286 Val Tgt Loss: 0.47858\n",
      "Epoch 32/500 Train Loss: 1.54409 Train Src Loss: 1.06987 Train Tgt Loss: 0.47423 \n",
      "                 Val Loss: 2.05915 Val Src Loss: 1.5807 Val Tgt Loss: 0.47845\n",
      "Epoch 33/500 Train Loss: 1.54198 Train Src Loss: 1.06974 Train Tgt Loss: 0.47224 \n",
      "                 Val Loss: 2.06132 Val Src Loss: 1.57532 Val Tgt Loss: 0.486\n",
      "Epoch 34/500 Train Loss: 1.54484 Train Src Loss: 1.07386 Train Tgt Loss: 0.47098 \n",
      "                 Val Loss: 2.07416 Val Src Loss: 1.59485 Val Tgt Loss: 0.47932\n",
      "Epoch 35/500 Train Loss: 1.54534 Train Src Loss: 1.07569 Train Tgt Loss: 0.46966 \n",
      "                 Val Loss: 2.04987 Val Src Loss: 1.57498 Val Tgt Loss: 0.47489\n",
      "Epoch 36/500 Train Loss: 1.54099 Train Src Loss: 1.0778 Train Tgt Loss: 0.46319 \n",
      "                 Val Loss: 2.0456 Val Src Loss: 1.5715 Val Tgt Loss: 0.4741\n",
      "Epoch 37/500 Train Loss: 1.53549 Train Src Loss: 1.07014 Train Tgt Loss: 0.46535 \n",
      "                 Val Loss: 2.05329 Val Src Loss: 1.58162 Val Tgt Loss: 0.47168\n",
      "Epoch 38/500 Train Loss: 1.53339 Train Src Loss: 1.06941 Train Tgt Loss: 0.46398 \n",
      "                 Val Loss: 2.04767 Val Src Loss: 1.57614 Val Tgt Loss: 0.47152\n",
      "Epoch 39/500 Train Loss: 1.53404 Train Src Loss: 1.07371 Train Tgt Loss: 0.46033 \n",
      "                 Val Loss: 2.04842 Val Src Loss: 1.57146 Val Tgt Loss: 0.47696\n",
      "Epoch 40/500 Train Loss: 1.53927 Train Src Loss: 1.07635 Train Tgt Loss: 0.46292 \n",
      "                 Val Loss: 2.06459 Val Src Loss: 1.5917 Val Tgt Loss: 0.47289\n",
      "Epoch 41/500 Train Loss: 1.53374 Train Src Loss: 1.06954 Train Tgt Loss: 0.4642 \n",
      "                 Val Loss: 2.05524 Val Src Loss: 1.58065 Val Tgt Loss: 0.47459\n",
      "Epoch 42/500 Train Loss: 1.53081 Train Src Loss: 1.07112 Train Tgt Loss: 0.45968 \n",
      "                 Val Loss: 2.0544 Val Src Loss: 1.5712 Val Tgt Loss: 0.4832\n"
     ]
    }
   ],
   "source": [
    "encoder_2014_2016_2017, mlp_2014_2016_2017, best_val_loss = continual_adapt(data_2012_2013, [2014,2016,2017], encoder_2012_2014_2015, mlp_2012_2014_2015, device, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5562118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Val Loss: 2.041443347930908\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Val Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a70a4e",
   "metadata": {},
   "source": [
    "## 2017-2018 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac9074",
   "metadata": {},
   "source": [
    "* Test:\n",
    "** 2017-2018 (then adapt 2016-2017 adapt-val 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "823d9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017_2019 = temp_partition_arxiv(data, year_bound=[-1,2017,2019], proportion=1.0)\n",
    "data_2017_2019 = data_2017_2019.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10c0bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder_2014_2016_2017, mlp_2014_2016_2017, data_2017_2019, evaluator)\n",
    "test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6eae374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.742 Test Acc: 0.584\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {round(test_loss,3)} Test Acc: {round(test_acc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28c5c188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start partitioning data...\n",
      "Finish partitioning data...\n",
      "Epoch 1/500 Train Loss: 1.52092 Train Src Loss: 1.07163 Train Tgt Loss: 0.44929 \n",
      "                 Val Loss: 2.0423 Val Src Loss: 1.61552 Val Tgt Loss: 0.42678\n",
      "Epoch 2/500 Train Loss: 1.54883 Train Src Loss: 1.11537 Train Tgt Loss: 0.43345 \n",
      "                 Val Loss: 2.02244 Val Src Loss: 1.58887 Val Tgt Loss: 0.43358\n",
      "Epoch 3/500 Train Loss: 1.52743 Train Src Loss: 1.08633 Train Tgt Loss: 0.4411 \n",
      "                 Val Loss: 2.02202 Val Src Loss: 1.59185 Val Tgt Loss: 0.43018\n",
      "Epoch 4/500 Train Loss: 1.52403 Train Src Loss: 1.08783 Train Tgt Loss: 0.4362 \n",
      "                 Val Loss: 2.03936 Val Src Loss: 1.59928 Val Tgt Loss: 0.44007\n",
      "Epoch 5/500 Train Loss: 1.51558 Train Src Loss: 1.07656 Train Tgt Loss: 0.43902 \n",
      "                 Val Loss: 2.01399 Val Src Loss: 1.58132 Val Tgt Loss: 0.43267\n",
      "Epoch 6/500 Train Loss: 1.51021 Train Src Loss: 1.07439 Train Tgt Loss: 0.43582 \n",
      "                 Val Loss: 2.00509 Val Src Loss: 1.58161 Val Tgt Loss: 0.42349\n",
      "Epoch 7/500 Train Loss: 1.49948 Train Src Loss: 1.06774 Train Tgt Loss: 0.43174 \n",
      "                 Val Loss: 1.9964 Val Src Loss: 1.57673 Val Tgt Loss: 0.41967\n",
      "Epoch 8/500 Train Loss: 1.50587 Train Src Loss: 1.07584 Train Tgt Loss: 0.43003 \n",
      "                 Val Loss: 1.99318 Val Src Loss: 1.57691 Val Tgt Loss: 0.41628\n",
      "Epoch 9/500 Train Loss: 1.49767 Train Src Loss: 1.07169 Train Tgt Loss: 0.42598 \n",
      "                 Val Loss: 2.0038 Val Src Loss: 1.59495 Val Tgt Loss: 0.40885\n",
      "Epoch 10/500 Train Loss: 1.49722 Train Src Loss: 1.07283 Train Tgt Loss: 0.42439 \n",
      "                 Val Loss: 1.98112 Val Src Loss: 1.57235 Val Tgt Loss: 0.40877\n",
      "Epoch 11/500 Train Loss: 1.5 Train Src Loss: 1.07702 Train Tgt Loss: 0.42298 \n",
      "                 Val Loss: 1.99757 Val Src Loss: 1.59165 Val Tgt Loss: 0.40592\n",
      "Epoch 12/500 Train Loss: 1.49024 Train Src Loss: 1.07468 Train Tgt Loss: 0.41557 \n",
      "                 Val Loss: 2.00044 Val Src Loss: 1.59784 Val Tgt Loss: 0.4026\n",
      "Epoch 13/500 Train Loss: 1.4881 Train Src Loss: 1.07416 Train Tgt Loss: 0.41393 \n",
      "                 Val Loss: 2.01121 Val Src Loss: 1.60305 Val Tgt Loss: 0.40816\n",
      "Epoch 14/500 Train Loss: 1.50123 Train Src Loss: 1.08299 Train Tgt Loss: 0.41824 \n",
      "                 Val Loss: 2.00536 Val Src Loss: 1.59435 Val Tgt Loss: 0.41101\n",
      "Epoch 15/500 Train Loss: 1.48627 Train Src Loss: 1.06638 Train Tgt Loss: 0.41989 \n",
      "                 Val Loss: 1.99345 Val Src Loss: 1.58621 Val Tgt Loss: 0.40724\n",
      "Epoch 16/500 Train Loss: 1.48901 Train Src Loss: 1.06831 Train Tgt Loss: 0.4207 \n",
      "                 Val Loss: 1.97948 Val Src Loss: 1.56713 Val Tgt Loss: 0.41235\n",
      "Epoch 17/500 Train Loss: 1.48026 Train Src Loss: 1.05754 Train Tgt Loss: 0.42272 \n",
      "                 Val Loss: 1.98069 Val Src Loss: 1.57057 Val Tgt Loss: 0.41012\n",
      "Epoch 18/500 Train Loss: 1.47428 Train Src Loss: 1.0568 Train Tgt Loss: 0.41748 \n",
      "                 Val Loss: 1.97326 Val Src Loss: 1.56612 Val Tgt Loss: 0.40713\n",
      "Epoch 19/500 Train Loss: 1.47745 Train Src Loss: 1.06425 Train Tgt Loss: 0.4132 \n",
      "                 Val Loss: 1.97745 Val Src Loss: 1.57435 Val Tgt Loss: 0.40309\n",
      "Epoch 20/500 Train Loss: 1.47517 Train Src Loss: 1.05782 Train Tgt Loss: 0.41736 \n",
      "                 Val Loss: 1.98929 Val Src Loss: 1.59014 Val Tgt Loss: 0.39915\n",
      "Epoch 21/500 Train Loss: 1.46985 Train Src Loss: 1.05829 Train Tgt Loss: 0.41156 \n",
      "                 Val Loss: 1.9616 Val Src Loss: 1.56675 Val Tgt Loss: 0.39485\n",
      "Epoch 22/500 Train Loss: 1.47311 Train Src Loss: 1.06498 Train Tgt Loss: 0.40813 \n",
      "                 Val Loss: 1.98128 Val Src Loss: 1.58234 Val Tgt Loss: 0.39894\n",
      "Epoch 23/500 Train Loss: 1.47233 Train Src Loss: 1.06326 Train Tgt Loss: 0.40907 \n",
      "                 Val Loss: 1.97607 Val Src Loss: 1.57808 Val Tgt Loss: 0.39799\n",
      "Epoch 24/500 Train Loss: 1.47016 Train Src Loss: 1.05527 Train Tgt Loss: 0.4149 \n",
      "                 Val Loss: 1.98764 Val Src Loss: 1.58363 Val Tgt Loss: 0.40401\n",
      "Epoch 25/500 Train Loss: 1.47804 Train Src Loss: 1.06358 Train Tgt Loss: 0.41445 \n",
      "                 Val Loss: 1.99541 Val Src Loss: 1.59189 Val Tgt Loss: 0.40351\n",
      "Epoch 26/500 Train Loss: 1.47342 Train Src Loss: 1.06123 Train Tgt Loss: 0.41219 \n",
      "                 Val Loss: 1.98786 Val Src Loss: 1.59326 Val Tgt Loss: 0.3946\n",
      "Epoch 27/500 Train Loss: 1.46529 Train Src Loss: 1.05766 Train Tgt Loss: 0.40763 \n",
      "                 Val Loss: 1.98464 Val Src Loss: 1.59078 Val Tgt Loss: 0.39386\n",
      "Epoch 28/500 Train Loss: 1.46384 Train Src Loss: 1.05773 Train Tgt Loss: 0.4061 \n",
      "                 Val Loss: 1.97374 Val Src Loss: 1.5797 Val Tgt Loss: 0.39404\n",
      "Epoch 29/500 Train Loss: 1.46551 Train Src Loss: 1.05915 Train Tgt Loss: 0.40635 \n",
      "                 Val Loss: 1.96369 Val Src Loss: 1.57169 Val Tgt Loss: 0.392\n",
      "Epoch 30/500 Train Loss: 1.46051 Train Src Loss: 1.0578 Train Tgt Loss: 0.40271 \n",
      "                 Val Loss: 1.98604 Val Src Loss: 1.59066 Val Tgt Loss: 0.39538\n",
      "Epoch 31/500 Train Loss: 1.45096 Train Src Loss: 1.0471 Train Tgt Loss: 0.40386 \n",
      "                 Val Loss: 1.97781 Val Src Loss: 1.58091 Val Tgt Loss: 0.3969\n",
      "Epoch 32/500 Train Loss: 1.4574 Train Src Loss: 1.0497 Train Tgt Loss: 0.4077 \n",
      "                 Val Loss: 1.96717 Val Src Loss: 1.57479 Val Tgt Loss: 0.39238\n"
     ]
    }
   ],
   "source": [
    "encoder_2016_2018_2019, mlp_2016_2018_2019, best_val_loss = continual_adapt(data_2012_2013, [2016,2018,2019], encoder_2014_2016_2017, mlp_2014_2016_2017, device, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd0d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Val Loss: 1.9615998268127441\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Val Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929e206",
   "metadata": {},
   "source": [
    "## 2019-2020 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e931c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2019_2021 = temp_partition_arxiv(data, year_bound=[-1,2019,2021], proportion=1.0)\n",
    "data_2019_2021 = data_2019_2021.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d913502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder_2016_2018_2019, mlp_2016_2018_2019, data_2019_2021, evaluator)\n",
    "test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab582549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.792 Test Acc: 0.609\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {round(test_loss,3)} Test Acc: {round(test_acc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e6c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8c81e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5907802649083232, 0.5685134277860012, 0.5839074178880194, 0.6085426825504598]\n"
     ]
    }
   ],
   "source": [
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d782c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.587935948283201\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_acc_list) / len(test_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1145da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
