{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2a7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from typing import Any, Tuple, Optional, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f993cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhchung/dyngraph-uda/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbc0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da93a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from ogb.nodeproppred.evaluate import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa60451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from dataset import temp_partition_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cbb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ogbn-arxiv'\n",
    "dataset = PygNodePropPredDataset(name = dataset_name, root='/home/hhchung/data/ogb-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe7aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b2bf2",
   "metadata": {},
   "source": [
    "## Model Structure ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0423e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ThreeLayerGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.linear1(x)\n",
    "        x1 = F.elu(x1)\n",
    "        x1 = F.dropout(x1, p=self.dropout)\n",
    "        output = self.linear2(x1)\n",
    "        # x = F.softmax(x, dim=1)\n",
    "        features = x1\n",
    "        return output, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c49f4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel(nn.Module):\n",
    "    r\"\"\"Gaussian Kernel Matrix\n",
    "    Gaussian Kernel k is defined by\n",
    "    .. math::\n",
    "        k(x_1, x_2) = \\exp \\left( - \\dfrac{\\| x_1 - x_2 \\|^2}{2\\sigma^2} \\right)\n",
    "    where :math:`x_1, x_2 \\in R^d` are 1-d tensors.\n",
    "    Gaussian Kernel Matrix K is defined on input group :math:`X=(x_1, x_2, ..., x_m),`\n",
    "    .. math::\n",
    "        K(X)_{i,j} = k(x_i, x_j)\n",
    "    Also by default, during training this layer keeps running estimates of the\n",
    "    mean of L2 distances, which are then used to set hyperparameter  :math:`\\sigma`.\n",
    "    Mathematically, the estimation is :math:`\\sigma^2 = \\dfrac{\\alpha}{n^2}\\sum_{i,j} \\| x_i - x_j \\|^2`.\n",
    "    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n",
    "    keep running estimates, and use a fixed :math:`\\sigma` instead.\n",
    "    Args:\n",
    "        sigma (float, optional): bandwidth :math:`\\sigma`. Default: None\n",
    "        track_running_stats (bool, optional): If ``True``, this module tracks the running mean of :math:`\\sigma^2`.\n",
    "          Otherwise, it won't track such statistics and always uses fix :math:`\\sigma^2`. Default: ``True``\n",
    "        alpha (float, optional): :math:`\\alpha` which decides the magnitude of :math:`\\sigma^2` when track_running_stats is set to ``True``\n",
    "    Inputs:\n",
    "        - X (tensor): input group :math:`X`\n",
    "    Shape:\n",
    "        - Inputs: :math:`(minibatch, F)` where F means the dimension of input features.\n",
    "        - Outputs: :math:`(minibatch, minibatch)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Optional[float] = None, track_running_stats: Optional[bool] = True,\n",
    "                 alpha: Optional[float] = 1.):\n",
    "        super(GaussianKernel, self).__init__()\n",
    "        assert track_running_stats or sigma is not None\n",
    "        self.sigma_square = torch.tensor(sigma * sigma) if sigma is not None else None\n",
    "        self.track_running_stats = track_running_stats\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        l2_distance_square = ((X.unsqueeze(0) - X.unsqueeze(1)) ** 2).sum(2)\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.sigma_square = self.alpha * torch.mean(l2_distance_square.detach())\n",
    "\n",
    "        return torch.exp(-l2_distance_square / (2 * self.sigma_square))\n",
    "    \n",
    "class JointMultipleKernelMaximumMeanDiscrepancy(nn.Module):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        kernels (tuple(tuple(torch.nn.Module))): kernel functions, where `kernels[r]` corresponds to kernel :math:`k^{\\mathcal{L}[r]}`.\n",
    "        linear (bool): whether use the linear version of JAN. Default: False\n",
    "        thetas (list(Theta): use adversarial version JAN if not None. Default: None\n",
    "    Inputs:\n",
    "        - z_s (tuple(tensor)): multiple layers' activations from the source domain, :math:`z^s`\n",
    "        - z_t (tuple(tensor)): multiple layers' activations from the target domain, :math:`z^t`\n",
    "    Shape:\n",
    "        - :math:`z^{sl}` and :math:`z^{tl}`: :math:`(minibatch, *)`  where * means any dimension\n",
    "        - Outputs: scalar\n",
    "    .. note::\n",
    "        Activations :math:`z^{sl}` and :math:`z^{tl}` must have the same shape.\n",
    "    .. note::\n",
    "        The kernel values will add up when there are multiple kernels for a certain layer.\n",
    "    Examples::\n",
    "        >>> feature_dim = 1024\n",
    "        >>> batch_size = 10\n",
    "        >>> layer1_kernels = (GaussianKernel(alpha=0.5), GaussianKernel(1.), GaussianKernel(2.))\n",
    "        >>> layer2_kernels = (GaussianKernel(1.), )\n",
    "        >>> loss = JointMultipleKernelMaximumMeanDiscrepancy((layer1_kernels, layer2_kernels))\n",
    "        >>> # layer1 features from source domain and target domain\n",
    "        >>> z1_s, z1_t = torch.randn(batch_size, feature_dim), torch.randn(batch_size, feature_dim)\n",
    "        >>> # layer2 features from source domain and target domain\n",
    "        >>> z2_s, z2_t = torch.randn(batch_size, feature_dim), torch.randn(batch_size, feature_dim)\n",
    "        >>> output = loss((z1_s, z2_s), (z1_t, z2_t))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernels: Sequence[Sequence[nn.Module]], linear: Optional[bool] = True, thetas: Sequence[nn.Module] = None):\n",
    "        super(JointMultipleKernelMaximumMeanDiscrepancy, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        self.index_matrix = None\n",
    "        self.linear = linear\n",
    "        if thetas:\n",
    "            self.thetas = thetas\n",
    "        else:\n",
    "            self.thetas = [nn.Identity() for _ in kernels]\n",
    "\n",
    "    def forward(self, z_s: torch.Tensor, z_t: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = int(z_s[0].size(0))\n",
    "        self.index_matrix = _update_index_matrix(batch_size, self.index_matrix, self.linear).to(z_s[0].device)\n",
    "\n",
    "        kernel_matrix = torch.ones_like(self.index_matrix)\n",
    "        for layer_z_s, layer_z_t, layer_kernels, theta in zip(z_s, z_t, self.kernels, self.thetas):\n",
    "            layer_features = torch.cat([layer_z_s, layer_z_t], dim=0)\n",
    "            layer_features = theta(layer_features)\n",
    "            kernel_matrix *= sum(\n",
    "                [kernel(layer_features) for kernel in layer_kernels])  # Add up the matrix of each kernel\n",
    "\n",
    "        # Add 2 / (n-1) to make up for the value on the diagonal\n",
    "        # to ensure loss is positive in the non-linear version\n",
    "        loss = (kernel_matrix * self.index_matrix).sum() + 2. / float(batch_size - 1)\n",
    "        return loss\n",
    "\n",
    "def _update_index_matrix(batch_size: int, index_matrix: Optional[torch.Tensor] = None,\n",
    "                         linear: Optional[bool] = True) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Update the `index_matrix` which convert `kernel_matrix` to loss.\n",
    "    If `index_matrix` is a tensor with shape (2 x batch_size, 2 x batch_size), then return `index_matrix`.\n",
    "    Else return a new tensor with shape (2 x batch_size, 2 x batch_size).\n",
    "    \"\"\"\n",
    "    if index_matrix is None or index_matrix.size(0) != batch_size * 2:\n",
    "        index_matrix = torch.zeros(2 * batch_size, 2 * batch_size)\n",
    "        if linear:\n",
    "            for i in range(batch_size):\n",
    "                s1, s2 = i, (i + 1) % batch_size\n",
    "                t1, t2 = s1 + batch_size, s2 + batch_size\n",
    "                index_matrix[s1, s2] = 1. / float(batch_size)\n",
    "                index_matrix[t1, t2] = 1. / float(batch_size)\n",
    "                index_matrix[s1, t2] = -1. / float(batch_size)\n",
    "                index_matrix[s2, t1] = -1. / float(batch_size)\n",
    "        else:\n",
    "            for i in range(batch_size):\n",
    "                for j in range(batch_size):\n",
    "                    if i != j:\n",
    "                        index_matrix[i][j] = 1. / float(batch_size * (batch_size - 1))\n",
    "                        index_matrix[i + batch_size][j + batch_size] = 1. / float(batch_size * (batch_size - 1))\n",
    "            for i in range(batch_size):\n",
    "                for j in range(batch_size):\n",
    "                    index_matrix[i][j + batch_size] = -1. / float(batch_size * batch_size)\n",
    "                    index_matrix[i + batch_size][j] = -1. / float(batch_size * batch_size)\n",
    "    return index_matrix\n",
    "    \n",
    "class Theta(nn.Module):\n",
    "    \"\"\"\n",
    "    maximize loss respect to :math:`\\theta`\n",
    "    minimize loss respect to features\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super(Theta, self).__init__()\n",
    "        self.grl1 = GradientReverseLayer()\n",
    "        self.grl2 = GradientReverseLayer()\n",
    "        self.layer1 = nn.Linear(dim, dim)\n",
    "        nn.init.eye_(self.layer1.weight)\n",
    "        nn.init.zeros_(self.layer1.bias)\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.grl1(features)\n",
    "        return self.grl2(self.layer1(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f76cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, mlp, optimizer, data):\n",
    "    encoder.train()\n",
    "    mlp.train()\n",
    "    \n",
    "    out, _ = mlp(encoder(data.x, data.edge_index))\n",
    "    out = F.log_softmax(out, dim=1)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test(encoder, mlp, data, evaluator):\n",
    "    encoder.eval()\n",
    "    mlp.eval()\n",
    "    \n",
    "    out, _ = mlp(encoder(data.x, data.edge_index))\n",
    "    out = F.log_softmax(out, dim=1)\n",
    "    val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask]).item()\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    val_acc = evaluator.eval({\n",
    "        'y_true': data.y[data.val_mask].unsqueeze(1),\n",
    "        'y_pred': y_pred[data.val_mask],\n",
    "    })['acc']\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a14358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_jmmd(jmmd_loss, src_f, src_y, tgt_f, tgt_y, batch_size=32):\n",
    "    src_loader = torch.utils.data.DataLoader(tuple(zip(list(src_f), list(src_y))), batch_size=batch_size, shuffle=True)\n",
    "    tgt_loader = torch.utils.data.DataLoader(tuple(zip(list(tgt_f), list(tgt_y))), batch_size=batch_size, shuffle=True)\n",
    "    src_iter = iter(src_loader)\n",
    "    tgt_iter = iter(tgt_loader)\n",
    "    len_dataloader = min(len(src_loader), len(tgt_loader))\n",
    "    \n",
    "    total_transfer_loss = 0\n",
    "    for i in range(len_dataloader):\n",
    "        src_f, src_y = src_iter.next()\n",
    "        tgt_f, tgt_y = tgt_iter.next()\n",
    "        if src_f.shape[0] != tgt_f.shape[0]:\n",
    "            break\n",
    "        # if src_f.shape[0] < tgt_f.shape[0]:\n",
    "        #     src_iter = iter(src_loader)\n",
    "        #     src_f, src_y = src_iter.next()\n",
    "        # else:\n",
    "        #     tgt_iter = iter(tgt_loader)\n",
    "        #     tgt_f, tgt_y = tgt_iter.next()\n",
    "            \n",
    "        total_transfer_loss += jmmd_loss((src_f, F.softmax(src_y, dim=1)), (tgt_f, F.softmax(tgt_y, dim=1)))\n",
    "    \n",
    "    return total_transfer_loss / len_dataloader\n",
    "\n",
    "def adapt(encoder, classifier, jmmd_loss, src_data, tgt_data, optimizer, e, epochs, lambda_coeff):\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    jmmd_loss.train()\n",
    "    \n",
    "    # len_dataloader = min(len(src_loader), len(tgt_loader))\n",
    "    # src_iter = iter(src_loader)\n",
    "    # tgt_iter = iter(tgt_loader)\n",
    "    \n",
    "    # total_loss = 0\n",
    "    # total_cls_loss = 0\n",
    "    # total_transfer_loss = 0\n",
    "    # total_src_data_size = 0\n",
    "    \n",
    "    # for i in tqdm(range(len_dataloader)):\n",
    "    # src_data = src_iter.next().to(device)\n",
    "    # tgt_data = tgt_iter.next().to(device)\n",
    "\n",
    "    src_y, src_f = classifier(encoder(src_data.x, src_data.edge_index))\n",
    "    tgt_y, tgt_f = classifier(encoder(tgt_data.x, tgt_data.edge_index))\n",
    "    cls_loss = F.nll_loss(F.log_softmax(src_y[src_data.train_mask], dim=1), src_data.y[src_data.train_mask])\n",
    "    transfer_loss = minibatch_jmmd(jmmd_loss, src_f[src_data.train_mask], src_y[src_data.train_mask], tgt_f[tgt_data.train_mask], tgt_y[tgt_data.train_mask])\n",
    "    loss = cls_loss + transfer_loss * lambda_coeff\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), cls_loss.item(), transfer_loss.item()\n",
    "    \n",
    "@torch.no_grad()\n",
    "def adapt_test(encoder, classifier, jmmd_loss, src_data, tgt_data, e, epochs, lambda_coeff):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    jmmd_loss.eval()\n",
    "    # len_dataloader = min(len(src_loader), len(tgt_loader))\n",
    "    # src_iter = iter(src_loader)\n",
    "    # tgt_iter = iter(tgt_loader)\n",
    "    \n",
    "    # total_loss = 0\n",
    "    # total_cls_loss = 0\n",
    "    # total_transfer_loss = 0\n",
    "    # total_src_data_size = 0\n",
    "    \n",
    "    # for i in tqdm(range(len_dataloader)):\n",
    "    # src_data = src_iter.next().to(device)\n",
    "    # tgt_data = tgt_iter.next().to(device)\n",
    "\n",
    "    src_y, src_f = classifier(encoder(src_data.x, src_data.edge_index))\n",
    "    tgt_y, tgt_f = classifier(encoder(tgt_data.x, tgt_data.edge_index))\n",
    "    cls_loss = F.nll_loss(F.log_softmax(src_y[src_data.val_mask], dim=1), src_data.y[src_data.val_mask])\n",
    "    # transfer_loss = jmmd_loss((src_f, F.softmax(src_y, dim=1)), (tgt_f, F.softmax(tgt_y, dim=1)))\n",
    "    transfer_loss = minibatch_jmmd(jmmd_loss, src_f[src_data.val_mask], src_y[src_data.val_mask], tgt_f[tgt_data.val_mask], tgt_y[tgt_data.val_mask])\n",
    "    loss = cls_loss + transfer_loss * lambda_coeff\n",
    "\n",
    "\n",
    "    # total_loss += loss.item() * src_data.x.size(0)\n",
    "    # total_cls_loss += cls_loss.item() * src_data.x.size(0)\n",
    "    # total_transfer_loss += transfer_loss.item() * src_data.x.size(0)\n",
    "    # total_src_data_size += src_data.x.size(0)\n",
    "    \n",
    "    # total_loss /= total_src_data_size\n",
    "    # total_cls_loss /= total_src_data_size\n",
    "    # total_transfer_loss /= total_src_data_size\n",
    "    return loss.item(), cls_loss.item(), transfer_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c47f3e",
   "metadata": {},
   "source": [
    "## Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af34247",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygNodePropPredDataset(name = dataset_name, root='/home/hhchung/data/ogb-data')\n",
    "data = dataset[0]\n",
    "data.edge_index = to_undirected(data.edge_index, data.num_nodes) # mimicking barlow twins repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6e29e",
   "metadata": {},
   "source": [
    "## Data Partition ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525c0ad",
   "metadata": {},
   "source": [
    "* Train: 0-2011\n",
    "* Val: 2012\n",
    "* Test:\n",
    "** 2013-2014 (then adapt 2012-2013 adapt-val 2014)\n",
    "** 2015-2016 (then adapt 2014-2015 adapt-val 2016)\n",
    "** 2017-2018 (then adapt 2016-2017 adapt-val 2018)\n",
    "** 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0230e3f",
   "metadata": {},
   "source": [
    "## Source Training Stage ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0515f9",
   "metadata": {},
   "source": [
    "* Train: 0-2011\n",
    "* Val: 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a633f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = data.x.shape[1]\n",
    "class_dim = data.y\n",
    "hidden_dim = 128\n",
    "emb_dim = 256\n",
    "encoder = TwoLayerGraphSAGE(feat_dim, hidden_dim, emb_dim)\n",
    "mlp = MLPHead(emb_dim, emb_dim // 4, 40)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(mlp.parameters()), lr=1e-3)\n",
    "epochs = 500\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = encoder.to(device)\n",
    "mlp = mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddd5e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_2013 = temp_partition_arxiv(data, year_bound=[-1,2012,2013], proportion=1.0)\n",
    "data_2012_2013 = data_2012_2013.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "373ed8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/500 Train Loss:3.7146 Val Loss:3.6349 Val Acc:0.1565\n",
      "Epoch:2/500 Train Loss:3.6308 Val Loss:3.5643 Val Acc:0.1977\n",
      "Epoch:3/500 Train Loss:3.5491 Val Loss:3.4867 Val Acc:0.2036\n",
      "Epoch:4/500 Train Loss:3.4588 Val Loss:3.4016 Val Acc:0.2045\n",
      "Epoch:5/500 Train Loss:3.3554 Val Loss:3.3091 Val Acc:0.2045\n",
      "Epoch:6/500 Train Loss:3.2474 Val Loss:3.2373 Val Acc:0.2045\n",
      "Epoch:7/500 Train Loss:3.159 Val Loss:3.2137 Val Acc:0.2045\n",
      "Epoch:8/500 Train Loss:3.1197 Val Loss:3.2375 Val Acc:0.2045\n",
      "Epoch:9/500 Train Loss:3.1278 Val Loss:3.2435 Val Acc:0.2045\n",
      "Epoch:10/500 Train Loss:3.1227 Val Loss:3.1977 Val Acc:0.2045\n",
      "Epoch:11/500 Train Loss:3.0835 Val Loss:3.1572 Val Acc:0.2034\n",
      "Epoch:12/500 Train Loss:3.0444 Val Loss:3.1318 Val Acc:0.2026\n",
      "Epoch:13/500 Train Loss:3.014 Val Loss:3.1054 Val Acc:0.1984\n",
      "Epoch:14/500 Train Loss:3.0002 Val Loss:3.1003 Val Acc:0.1997\n",
      "Epoch:15/500 Train Loss:2.985 Val Loss:3.0831 Val Acc:0.2009\n",
      "Epoch:16/500 Train Loss:2.9754 Val Loss:3.0729 Val Acc:0.2068\n",
      "Epoch:17/500 Train Loss:2.9613 Val Loss:3.0626 Val Acc:0.207\n",
      "Epoch:18/500 Train Loss:2.9419 Val Loss:3.0551 Val Acc:0.205\n",
      "Epoch:19/500 Train Loss:2.9214 Val Loss:3.0459 Val Acc:0.2048\n",
      "Epoch:20/500 Train Loss:2.906 Val Loss:3.049 Val Acc:0.2053\n",
      "Epoch:21/500 Train Loss:2.8953 Val Loss:3.04 Val Acc:0.2047\n",
      "Epoch:22/500 Train Loss:2.8782 Val Loss:3.0268 Val Acc:0.2047\n",
      "Epoch:23/500 Train Loss:2.8634 Val Loss:3.0 Val Acc:0.205\n",
      "Epoch:24/500 Train Loss:2.8341 Val Loss:2.9766 Val Acc:0.2056\n",
      "Epoch:25/500 Train Loss:2.8117 Val Loss:2.9472 Val Acc:0.2089\n",
      "Epoch:26/500 Train Loss:2.7811 Val Loss:2.9274 Val Acc:0.2154\n",
      "Epoch:27/500 Train Loss:2.7661 Val Loss:2.9087 Val Acc:0.223\n",
      "Epoch:28/500 Train Loss:2.7452 Val Loss:2.8845 Val Acc:0.2323\n",
      "Epoch:29/500 Train Loss:2.7206 Val Loss:2.8627 Val Acc:0.2336\n",
      "Epoch:30/500 Train Loss:2.6849 Val Loss:2.8377 Val Acc:0.234\n",
      "Epoch:31/500 Train Loss:2.6561 Val Loss:2.8239 Val Acc:0.2364\n",
      "Epoch:32/500 Train Loss:2.6327 Val Loss:2.8059 Val Acc:0.2427\n",
      "Epoch:33/500 Train Loss:2.6152 Val Loss:2.7712 Val Acc:0.2514\n",
      "Epoch:34/500 Train Loss:2.5896 Val Loss:2.7462 Val Acc:0.2617\n",
      "Epoch:35/500 Train Loss:2.5623 Val Loss:2.7241 Val Acc:0.2707\n",
      "Epoch:36/500 Train Loss:2.5438 Val Loss:2.7076 Val Acc:0.2779\n",
      "Epoch:37/500 Train Loss:2.5284 Val Loss:2.6854 Val Acc:0.2811\n",
      "Epoch:38/500 Train Loss:2.4995 Val Loss:2.663 Val Acc:0.28\n",
      "Epoch:39/500 Train Loss:2.4703 Val Loss:2.654 Val Acc:0.2816\n",
      "Epoch:40/500 Train Loss:2.4514 Val Loss:2.6352 Val Acc:0.2841\n",
      "Epoch:41/500 Train Loss:2.4253 Val Loss:2.6044 Val Acc:0.2936\n",
      "Epoch:42/500 Train Loss:2.4065 Val Loss:2.5893 Val Acc:0.3005\n",
      "Epoch:43/500 Train Loss:2.3828 Val Loss:2.5678 Val Acc:0.3066\n",
      "Epoch:44/500 Train Loss:2.3625 Val Loss:2.5535 Val Acc:0.3054\n",
      "Epoch:45/500 Train Loss:2.3444 Val Loss:2.5384 Val Acc:0.3086\n",
      "Epoch:46/500 Train Loss:2.3257 Val Loss:2.5292 Val Acc:0.3075\n",
      "Epoch:47/500 Train Loss:2.3075 Val Loss:2.5186 Val Acc:0.3144\n",
      "Epoch:48/500 Train Loss:2.2894 Val Loss:2.5064 Val Acc:0.306\n",
      "Epoch:49/500 Train Loss:2.2709 Val Loss:2.4829 Val Acc:0.3144\n",
      "Epoch:50/500 Train Loss:2.2592 Val Loss:2.4763 Val Acc:0.3117\n",
      "Epoch:51/500 Train Loss:2.2438 Val Loss:2.467 Val Acc:0.3139\n",
      "Epoch:52/500 Train Loss:2.2297 Val Loss:2.4648 Val Acc:0.3139\n",
      "Epoch:53/500 Train Loss:2.2215 Val Loss:2.4456 Val Acc:0.3226\n",
      "Epoch:54/500 Train Loss:2.2054 Val Loss:2.4308 Val Acc:0.3228\n",
      "Epoch:55/500 Train Loss:2.1957 Val Loss:2.4168 Val Acc:0.3232\n",
      "Epoch:56/500 Train Loss:2.185 Val Loss:2.4127 Val Acc:0.3282\n",
      "Epoch:57/500 Train Loss:2.174 Val Loss:2.404 Val Acc:0.3293\n",
      "Epoch:58/500 Train Loss:2.1672 Val Loss:2.3976 Val Acc:0.3284\n",
      "Epoch:59/500 Train Loss:2.1559 Val Loss:2.3872 Val Acc:0.3312\n",
      "Epoch:60/500 Train Loss:2.143 Val Loss:2.3662 Val Acc:0.3358\n",
      "Epoch:61/500 Train Loss:2.1323 Val Loss:2.3571 Val Acc:0.3372\n",
      "Epoch:62/500 Train Loss:2.1204 Val Loss:2.3413 Val Acc:0.3473\n",
      "Epoch:63/500 Train Loss:2.1097 Val Loss:2.3396 Val Acc:0.3467\n",
      "Epoch:64/500 Train Loss:2.0979 Val Loss:2.3202 Val Acc:0.3475\n",
      "Epoch:65/500 Train Loss:2.0923 Val Loss:2.3143 Val Acc:0.3447\n",
      "Epoch:66/500 Train Loss:2.0795 Val Loss:2.3036 Val Acc:0.3534\n",
      "Epoch:67/500 Train Loss:2.0741 Val Loss:2.2908 Val Acc:0.356\n",
      "Epoch:68/500 Train Loss:2.0557 Val Loss:2.2748 Val Acc:0.363\n",
      "Epoch:69/500 Train Loss:2.0429 Val Loss:2.2594 Val Acc:0.3674\n",
      "Epoch:70/500 Train Loss:2.0397 Val Loss:2.2583 Val Acc:0.3643\n",
      "Epoch:71/500 Train Loss:2.0282 Val Loss:2.2361 Val Acc:0.3709\n",
      "Epoch:72/500 Train Loss:2.0135 Val Loss:2.2245 Val Acc:0.3747\n",
      "Epoch:73/500 Train Loss:2.005 Val Loss:2.2009 Val Acc:0.3796\n",
      "Epoch:74/500 Train Loss:1.9926 Val Loss:2.1983 Val Acc:0.3849\n",
      "Epoch:75/500 Train Loss:1.9791 Val Loss:2.1898 Val Acc:0.3857\n",
      "Epoch:76/500 Train Loss:1.9682 Val Loss:2.1742 Val Acc:0.389\n",
      "Epoch:77/500 Train Loss:1.9599 Val Loss:2.1574 Val Acc:0.3941\n",
      "Epoch:78/500 Train Loss:1.9486 Val Loss:2.1483 Val Acc:0.3983\n",
      "Epoch:79/500 Train Loss:1.9327 Val Loss:2.1357 Val Acc:0.4034\n",
      "Epoch:80/500 Train Loss:1.9259 Val Loss:2.1224 Val Acc:0.4068\n",
      "Epoch:81/500 Train Loss:1.9176 Val Loss:2.1058 Val Acc:0.4101\n",
      "Epoch:82/500 Train Loss:1.9003 Val Loss:2.0915 Val Acc:0.4162\n",
      "Epoch:83/500 Train Loss:1.8947 Val Loss:2.0852 Val Acc:0.4183\n",
      "Epoch:84/500 Train Loss:1.8846 Val Loss:2.0606 Val Acc:0.4252\n",
      "Epoch:85/500 Train Loss:1.8697 Val Loss:2.0548 Val Acc:0.4266\n",
      "Epoch:86/500 Train Loss:1.8647 Val Loss:2.0377 Val Acc:0.4319\n",
      "Epoch:87/500 Train Loss:1.8522 Val Loss:2.0334 Val Acc:0.4357\n",
      "Epoch:88/500 Train Loss:1.846 Val Loss:2.0199 Val Acc:0.4385\n",
      "Epoch:89/500 Train Loss:1.8312 Val Loss:2.0066 Val Acc:0.4404\n",
      "Epoch:90/500 Train Loss:1.8254 Val Loss:1.9964 Val Acc:0.4448\n",
      "Epoch:91/500 Train Loss:1.8148 Val Loss:1.9956 Val Acc:0.4449\n",
      "Epoch:92/500 Train Loss:1.8065 Val Loss:1.9786 Val Acc:0.4538\n",
      "Epoch:93/500 Train Loss:1.7974 Val Loss:1.9786 Val Acc:0.4527\n",
      "Epoch:94/500 Train Loss:1.7923 Val Loss:1.9674 Val Acc:0.4589\n",
      "Epoch:95/500 Train Loss:1.7871 Val Loss:1.9624 Val Acc:0.4545\n",
      "Epoch:96/500 Train Loss:1.7709 Val Loss:1.9582 Val Acc:0.4581\n",
      "Epoch:97/500 Train Loss:1.7706 Val Loss:1.951 Val Acc:0.4597\n",
      "Epoch:98/500 Train Loss:1.7596 Val Loss:1.9456 Val Acc:0.465\n",
      "Epoch:99/500 Train Loss:1.7528 Val Loss:1.9449 Val Acc:0.4583\n",
      "Epoch:100/500 Train Loss:1.7426 Val Loss:1.9311 Val Acc:0.4597\n",
      "Epoch:101/500 Train Loss:1.7379 Val Loss:1.9264 Val Acc:0.4682\n",
      "Epoch:102/500 Train Loss:1.732 Val Loss:1.918 Val Acc:0.4695\n",
      "Epoch:103/500 Train Loss:1.7216 Val Loss:1.9151 Val Acc:0.4659\n",
      "Epoch:104/500 Train Loss:1.7176 Val Loss:1.9008 Val Acc:0.474\n",
      "Epoch:105/500 Train Loss:1.712 Val Loss:1.8961 Val Acc:0.4726\n",
      "Epoch:106/500 Train Loss:1.7006 Val Loss:1.8869 Val Acc:0.48\n",
      "Epoch:107/500 Train Loss:1.6926 Val Loss:1.8825 Val Acc:0.4839\n",
      "Epoch:108/500 Train Loss:1.6865 Val Loss:1.876 Val Acc:0.4811\n",
      "Epoch:109/500 Train Loss:1.6796 Val Loss:1.8736 Val Acc:0.4827\n",
      "Epoch:110/500 Train Loss:1.675 Val Loss:1.8642 Val Acc:0.487\n",
      "Epoch:111/500 Train Loss:1.6701 Val Loss:1.8546 Val Acc:0.4923\n",
      "Epoch:112/500 Train Loss:1.6576 Val Loss:1.8546 Val Acc:0.4848\n",
      "Epoch:113/500 Train Loss:1.651 Val Loss:1.8455 Val Acc:0.488\n",
      "Epoch:114/500 Train Loss:1.6467 Val Loss:1.8481 Val Acc:0.487\n",
      "Epoch:115/500 Train Loss:1.6415 Val Loss:1.8376 Val Acc:0.4866\n",
      "Epoch:116/500 Train Loss:1.6319 Val Loss:1.8364 Val Acc:0.4887\n",
      "Epoch:117/500 Train Loss:1.6275 Val Loss:1.8156 Val Acc:0.4923\n",
      "Epoch:118/500 Train Loss:1.6186 Val Loss:1.8152 Val Acc:0.4951\n",
      "Epoch:119/500 Train Loss:1.6128 Val Loss:1.8204 Val Acc:0.4953\n",
      "Epoch:120/500 Train Loss:1.6062 Val Loss:1.8128 Val Acc:0.4956\n",
      "Epoch:121/500 Train Loss:1.6013 Val Loss:1.81 Val Acc:0.4939\n",
      "Epoch:122/500 Train Loss:1.5966 Val Loss:1.8062 Val Acc:0.497\n",
      "Epoch:123/500 Train Loss:1.5952 Val Loss:1.7983 Val Acc:0.5004\n",
      "Epoch:124/500 Train Loss:1.5872 Val Loss:1.7961 Val Acc:0.5032\n",
      "Epoch:125/500 Train Loss:1.5829 Val Loss:1.7975 Val Acc:0.4956\n",
      "Epoch:126/500 Train Loss:1.5797 Val Loss:1.7878 Val Acc:0.5005\n",
      "Epoch:127/500 Train Loss:1.5692 Val Loss:1.7908 Val Acc:0.5018\n",
      "Epoch:128/500 Train Loss:1.568 Val Loss:1.7781 Val Acc:0.5047\n",
      "Epoch:129/500 Train Loss:1.5659 Val Loss:1.7777 Val Acc:0.5082\n",
      "Epoch:130/500 Train Loss:1.5625 Val Loss:1.7708 Val Acc:0.5023\n",
      "Epoch:131/500 Train Loss:1.5599 Val Loss:1.7651 Val Acc:0.5019\n",
      "Epoch:132/500 Train Loss:1.5549 Val Loss:1.765 Val Acc:0.5049\n",
      "Epoch:133/500 Train Loss:1.5466 Val Loss:1.7661 Val Acc:0.5054\n",
      "Epoch:134/500 Train Loss:1.5442 Val Loss:1.755 Val Acc:0.5077\n",
      "Epoch:135/500 Train Loss:1.5409 Val Loss:1.7545 Val Acc:0.5117\n",
      "Epoch:136/500 Train Loss:1.5378 Val Loss:1.7477 Val Acc:0.4998\n",
      "Epoch:137/500 Train Loss:1.5349 Val Loss:1.7459 Val Acc:0.5122\n",
      "Epoch:138/500 Train Loss:1.5291 Val Loss:1.7466 Val Acc:0.5043\n",
      "Epoch:139/500 Train Loss:1.5236 Val Loss:1.7382 Val Acc:0.5161\n",
      "Epoch:140/500 Train Loss:1.5244 Val Loss:1.7433 Val Acc:0.5106\n",
      "Epoch:141/500 Train Loss:1.5167 Val Loss:1.7368 Val Acc:0.5099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:142/500 Train Loss:1.5141 Val Loss:1.739 Val Acc:0.5114\n",
      "Epoch:143/500 Train Loss:1.5089 Val Loss:1.7335 Val Acc:0.511\n",
      "Epoch:144/500 Train Loss:1.5028 Val Loss:1.7282 Val Acc:0.5117\n",
      "Epoch:145/500 Train Loss:1.5014 Val Loss:1.7215 Val Acc:0.5094\n",
      "Epoch:146/500 Train Loss:1.4982 Val Loss:1.7283 Val Acc:0.5172\n",
      "Epoch:147/500 Train Loss:1.5045 Val Loss:1.7207 Val Acc:0.5159\n",
      "Epoch:148/500 Train Loss:1.4934 Val Loss:1.7122 Val Acc:0.5207\n",
      "Epoch:149/500 Train Loss:1.4897 Val Loss:1.7171 Val Acc:0.5167\n",
      "Epoch:150/500 Train Loss:1.4878 Val Loss:1.7082 Val Acc:0.5201\n",
      "Epoch:151/500 Train Loss:1.4852 Val Loss:1.7078 Val Acc:0.5161\n",
      "Epoch:152/500 Train Loss:1.4839 Val Loss:1.7085 Val Acc:0.5173\n",
      "Epoch:153/500 Train Loss:1.4802 Val Loss:1.7029 Val Acc:0.5253\n",
      "Epoch:154/500 Train Loss:1.4737 Val Loss:1.705 Val Acc:0.5186\n",
      "Epoch:155/500 Train Loss:1.474 Val Loss:1.7022 Val Acc:0.5197\n",
      "Epoch:156/500 Train Loss:1.4681 Val Loss:1.6938 Val Acc:0.5277\n",
      "Epoch:157/500 Train Loss:1.4676 Val Loss:1.6962 Val Acc:0.5221\n",
      "Epoch:158/500 Train Loss:1.4619 Val Loss:1.6918 Val Acc:0.5231\n",
      "Epoch:159/500 Train Loss:1.4582 Val Loss:1.6931 Val Acc:0.5218\n",
      "Epoch:160/500 Train Loss:1.4549 Val Loss:1.6857 Val Acc:0.5242\n",
      "Epoch:161/500 Train Loss:1.4557 Val Loss:1.6804 Val Acc:0.5285\n",
      "Epoch:162/500 Train Loss:1.4558 Val Loss:1.6859 Val Acc:0.5254\n",
      "Epoch:163/500 Train Loss:1.4474 Val Loss:1.6823 Val Acc:0.5228\n",
      "Epoch:164/500 Train Loss:1.4473 Val Loss:1.6767 Val Acc:0.5304\n",
      "Epoch:165/500 Train Loss:1.4429 Val Loss:1.6726 Val Acc:0.5246\n",
      "Epoch:166/500 Train Loss:1.4498 Val Loss:1.6752 Val Acc:0.5239\n",
      "Epoch:167/500 Train Loss:1.4367 Val Loss:1.6776 Val Acc:0.5206\n",
      "Epoch:168/500 Train Loss:1.438 Val Loss:1.6723 Val Acc:0.5274\n",
      "Epoch:169/500 Train Loss:1.4341 Val Loss:1.6684 Val Acc:0.5321\n",
      "Epoch:170/500 Train Loss:1.4334 Val Loss:1.6668 Val Acc:0.5333\n",
      "Epoch:171/500 Train Loss:1.43 Val Loss:1.6723 Val Acc:0.5291\n",
      "Epoch:172/500 Train Loss:1.4291 Val Loss:1.6727 Val Acc:0.5315\n",
      "Epoch:173/500 Train Loss:1.4248 Val Loss:1.6522 Val Acc:0.5313\n",
      "Epoch:174/500 Train Loss:1.4151 Val Loss:1.6631 Val Acc:0.5304\n",
      "Epoch:175/500 Train Loss:1.4153 Val Loss:1.6567 Val Acc:0.5308\n",
      "Epoch:176/500 Train Loss:1.4159 Val Loss:1.6581 Val Acc:0.5336\n",
      "Epoch:177/500 Train Loss:1.4161 Val Loss:1.6649 Val Acc:0.5326\n",
      "Epoch:178/500 Train Loss:1.409 Val Loss:1.6533 Val Acc:0.5291\n",
      "Epoch:179/500 Train Loss:1.4075 Val Loss:1.6487 Val Acc:0.5363\n",
      "Epoch:180/500 Train Loss:1.4027 Val Loss:1.6487 Val Acc:0.535\n",
      "Epoch:181/500 Train Loss:1.4073 Val Loss:1.6467 Val Acc:0.5358\n",
      "Epoch:182/500 Train Loss:1.4052 Val Loss:1.6521 Val Acc:0.5321\n",
      "Epoch:183/500 Train Loss:1.403 Val Loss:1.6357 Val Acc:0.5395\n",
      "Epoch:184/500 Train Loss:1.3971 Val Loss:1.6399 Val Acc:0.5386\n",
      "Epoch:185/500 Train Loss:1.3964 Val Loss:1.6465 Val Acc:0.5333\n",
      "Epoch:186/500 Train Loss:1.3946 Val Loss:1.6394 Val Acc:0.5392\n",
      "Epoch:187/500 Train Loss:1.3934 Val Loss:1.6352 Val Acc:0.536\n",
      "Epoch:188/500 Train Loss:1.3878 Val Loss:1.6394 Val Acc:0.5406\n",
      "Epoch:189/500 Train Loss:1.3868 Val Loss:1.6343 Val Acc:0.5388\n",
      "Epoch:190/500 Train Loss:1.3851 Val Loss:1.6314 Val Acc:0.5385\n",
      "Epoch:191/500 Train Loss:1.3852 Val Loss:1.6317 Val Acc:0.542\n",
      "Epoch:192/500 Train Loss:1.3794 Val Loss:1.6227 Val Acc:0.5419\n",
      "Epoch:193/500 Train Loss:1.3802 Val Loss:1.6288 Val Acc:0.5414\n",
      "Epoch:194/500 Train Loss:1.3782 Val Loss:1.6291 Val Acc:0.5403\n",
      "Epoch:195/500 Train Loss:1.3765 Val Loss:1.621 Val Acc:0.5406\n",
      "Epoch:196/500 Train Loss:1.3677 Val Loss:1.6121 Val Acc:0.5422\n",
      "Epoch:197/500 Train Loss:1.3682 Val Loss:1.6134 Val Acc:0.5444\n",
      "Epoch:198/500 Train Loss:1.3668 Val Loss:1.6147 Val Acc:0.5437\n",
      "Epoch:199/500 Train Loss:1.3707 Val Loss:1.6151 Val Acc:0.542\n",
      "Epoch:200/500 Train Loss:1.3654 Val Loss:1.6215 Val Acc:0.5433\n",
      "Epoch:201/500 Train Loss:1.3614 Val Loss:1.6128 Val Acc:0.5448\n",
      "Epoch:202/500 Train Loss:1.3589 Val Loss:1.6099 Val Acc:0.5453\n",
      "Epoch:203/500 Train Loss:1.3555 Val Loss:1.6184 Val Acc:0.5422\n",
      "Epoch:204/500 Train Loss:1.3563 Val Loss:1.601 Val Acc:0.549\n",
      "Epoch:205/500 Train Loss:1.3557 Val Loss:1.5976 Val Acc:0.5453\n",
      "Epoch:206/500 Train Loss:1.3549 Val Loss:1.6055 Val Acc:0.5441\n",
      "Epoch:207/500 Train Loss:1.3499 Val Loss:1.5973 Val Acc:0.5528\n",
      "Epoch:208/500 Train Loss:1.3489 Val Loss:1.6038 Val Acc:0.5436\n",
      "Epoch:209/500 Train Loss:1.3497 Val Loss:1.6018 Val Acc:0.5467\n",
      "Epoch:210/500 Train Loss:1.3454 Val Loss:1.6005 Val Acc:0.5479\n",
      "Epoch:211/500 Train Loss:1.3438 Val Loss:1.6003 Val Acc:0.5531\n",
      "Epoch:212/500 Train Loss:1.3351 Val Loss:1.6026 Val Acc:0.5484\n",
      "Epoch:213/500 Train Loss:1.3425 Val Loss:1.598 Val Acc:0.5481\n",
      "Epoch:214/500 Train Loss:1.3353 Val Loss:1.5964 Val Acc:0.5493\n",
      "Epoch:215/500 Train Loss:1.3395 Val Loss:1.6035 Val Acc:0.5462\n",
      "Epoch:216/500 Train Loss:1.3316 Val Loss:1.591 Val Acc:0.55\n",
      "Epoch:217/500 Train Loss:1.3288 Val Loss:1.594 Val Acc:0.5554\n",
      "Epoch:218/500 Train Loss:1.3313 Val Loss:1.5876 Val Acc:0.5467\n",
      "Epoch:219/500 Train Loss:1.3295 Val Loss:1.5966 Val Acc:0.5531\n",
      "Epoch:220/500 Train Loss:1.3297 Val Loss:1.5899 Val Acc:0.5526\n",
      "Epoch:221/500 Train Loss:1.3232 Val Loss:1.5806 Val Acc:0.5543\n",
      "Epoch:222/500 Train Loss:1.3199 Val Loss:1.5853 Val Acc:0.5501\n",
      "Epoch:223/500 Train Loss:1.3208 Val Loss:1.5899 Val Acc:0.5507\n",
      "Epoch:224/500 Train Loss:1.3201 Val Loss:1.577 Val Acc:0.5534\n",
      "Epoch:225/500 Train Loss:1.3168 Val Loss:1.5784 Val Acc:0.5556\n",
      "Epoch:226/500 Train Loss:1.3205 Val Loss:1.5742 Val Acc:0.5528\n",
      "Epoch:227/500 Train Loss:1.315 Val Loss:1.5792 Val Acc:0.5535\n",
      "Epoch:228/500 Train Loss:1.3154 Val Loss:1.5693 Val Acc:0.5596\n",
      "Epoch:229/500 Train Loss:1.3091 Val Loss:1.5748 Val Acc:0.5563\n",
      "Epoch:230/500 Train Loss:1.3081 Val Loss:1.579 Val Acc:0.5556\n",
      "Epoch:231/500 Train Loss:1.3044 Val Loss:1.5806 Val Acc:0.5574\n",
      "Epoch:232/500 Train Loss:1.3023 Val Loss:1.5776 Val Acc:0.5559\n",
      "Epoch:233/500 Train Loss:1.3093 Val Loss:1.5789 Val Acc:0.5532\n",
      "Epoch:234/500 Train Loss:1.3002 Val Loss:1.5683 Val Acc:0.5521\n",
      "Epoch:235/500 Train Loss:1.3032 Val Loss:1.5692 Val Acc:0.5585\n",
      "Epoch:236/500 Train Loss:1.2941 Val Loss:1.5687 Val Acc:0.556\n",
      "Epoch:237/500 Train Loss:1.2964 Val Loss:1.5659 Val Acc:0.5602\n",
      "Epoch:238/500 Train Loss:1.2949 Val Loss:1.5711 Val Acc:0.5579\n",
      "Epoch:239/500 Train Loss:1.2913 Val Loss:1.5633 Val Acc:0.5594\n",
      "Epoch:240/500 Train Loss:1.295 Val Loss:1.5628 Val Acc:0.5618\n",
      "Epoch:241/500 Train Loss:1.2865 Val Loss:1.5567 Val Acc:0.5579\n",
      "Epoch:242/500 Train Loss:1.2864 Val Loss:1.5672 Val Acc:0.5574\n",
      "Epoch:243/500 Train Loss:1.2864 Val Loss:1.5673 Val Acc:0.5607\n",
      "Epoch:244/500 Train Loss:1.2827 Val Loss:1.5664 Val Acc:0.5616\n",
      "Epoch:245/500 Train Loss:1.2792 Val Loss:1.5703 Val Acc:0.5582\n",
      "Epoch:246/500 Train Loss:1.2785 Val Loss:1.5621 Val Acc:0.5591\n",
      "Epoch:247/500 Train Loss:1.284 Val Loss:1.5688 Val Acc:0.5535\n",
      "Epoch:248/500 Train Loss:1.2747 Val Loss:1.5625 Val Acc:0.5632\n",
      "Epoch:249/500 Train Loss:1.2774 Val Loss:1.5643 Val Acc:0.5574\n",
      "Epoch:250/500 Train Loss:1.2738 Val Loss:1.5564 Val Acc:0.5577\n",
      "Epoch:251/500 Train Loss:1.2701 Val Loss:1.5614 Val Acc:0.5608\n",
      "Epoch:252/500 Train Loss:1.2706 Val Loss:1.5562 Val Acc:0.5625\n",
      "Epoch:253/500 Train Loss:1.2663 Val Loss:1.5547 Val Acc:0.5607\n",
      "Epoch:254/500 Train Loss:1.2703 Val Loss:1.5591 Val Acc:0.5661\n",
      "Epoch:255/500 Train Loss:1.2734 Val Loss:1.5607 Val Acc:0.5632\n",
      "Epoch:256/500 Train Loss:1.268 Val Loss:1.5459 Val Acc:0.5685\n",
      "Epoch:257/500 Train Loss:1.2589 Val Loss:1.5365 Val Acc:0.5643\n",
      "Epoch:258/500 Train Loss:1.2643 Val Loss:1.5528 Val Acc:0.5655\n",
      "Epoch:259/500 Train Loss:1.2643 Val Loss:1.5486 Val Acc:0.5598\n",
      "Epoch:260/500 Train Loss:1.2639 Val Loss:1.5548 Val Acc:0.5638\n",
      "Epoch:261/500 Train Loss:1.2628 Val Loss:1.5524 Val Acc:0.5618\n",
      "Epoch:262/500 Train Loss:1.259 Val Loss:1.5556 Val Acc:0.5677\n",
      "Epoch:263/500 Train Loss:1.2611 Val Loss:1.5425 Val Acc:0.5652\n",
      "Epoch:264/500 Train Loss:1.2511 Val Loss:1.5444 Val Acc:0.5638\n",
      "Epoch:265/500 Train Loss:1.2539 Val Loss:1.5428 Val Acc:0.5664\n",
      "Epoch:266/500 Train Loss:1.2502 Val Loss:1.5521 Val Acc:0.5666\n",
      "Epoch:267/500 Train Loss:1.2535 Val Loss:1.5428 Val Acc:0.566\n",
      "Epoch:268/500 Train Loss:1.2464 Val Loss:1.5475 Val Acc:0.5649\n",
      "Epoch:269/500 Train Loss:1.2426 Val Loss:1.5351 Val Acc:0.5624\n",
      "Epoch:270/500 Train Loss:1.2431 Val Loss:1.541 Val Acc:0.5649\n",
      "Epoch:271/500 Train Loss:1.2408 Val Loss:1.5367 Val Acc:0.5692\n",
      "Epoch:272/500 Train Loss:1.2416 Val Loss:1.5341 Val Acc:0.5669\n",
      "Epoch:273/500 Train Loss:1.2412 Val Loss:1.5417 Val Acc:0.568\n",
      "Epoch:274/500 Train Loss:1.2379 Val Loss:1.5384 Val Acc:0.5683\n",
      "Epoch:275/500 Train Loss:1.2412 Val Loss:1.5456 Val Acc:0.5666\n",
      "Epoch:276/500 Train Loss:1.2392 Val Loss:1.5434 Val Acc:0.5624\n",
      "Epoch:277/500 Train Loss:1.2404 Val Loss:1.5355 Val Acc:0.5667\n",
      "Epoch:278/500 Train Loss:1.2371 Val Loss:1.5374 Val Acc:0.5652\n",
      "Epoch:279/500 Train Loss:1.2351 Val Loss:1.5383 Val Acc:0.5649\n",
      "Epoch:280/500 Train Loss:1.236 Val Loss:1.5319 Val Acc:0.5652\n",
      "Epoch:281/500 Train Loss:1.2323 Val Loss:1.5374 Val Acc:0.5649\n",
      "Epoch:282/500 Train Loss:1.2312 Val Loss:1.5346 Val Acc:0.5672\n",
      "Epoch:283/500 Train Loss:1.2324 Val Loss:1.5378 Val Acc:0.5697\n",
      "Epoch:284/500 Train Loss:1.2283 Val Loss:1.5345 Val Acc:0.5644\n",
      "Epoch:285/500 Train Loss:1.2241 Val Loss:1.5422 Val Acc:0.5692\n",
      "Epoch:286/500 Train Loss:1.2288 Val Loss:1.5324 Val Acc:0.5719\n",
      "Epoch:287/500 Train Loss:1.2237 Val Loss:1.5325 Val Acc:0.5716\n",
      "Epoch:288/500 Train Loss:1.2263 Val Loss:1.5329 Val Acc:0.5709\n",
      "Epoch:289/500 Train Loss:1.2195 Val Loss:1.537 Val Acc:0.572\n",
      "Epoch:290/500 Train Loss:1.2249 Val Loss:1.5207 Val Acc:0.574\n",
      "Epoch:291/500 Train Loss:1.2193 Val Loss:1.5284 Val Acc:0.5717\n",
      "Epoch:292/500 Train Loss:1.219 Val Loss:1.547 Val Acc:0.5695\n",
      "Epoch:293/500 Train Loss:1.2172 Val Loss:1.5275 Val Acc:0.5697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:294/500 Train Loss:1.2225 Val Loss:1.5292 Val Acc:0.5685\n",
      "Epoch:295/500 Train Loss:1.2144 Val Loss:1.5323 Val Acc:0.5669\n",
      "Epoch:296/500 Train Loss:1.2115 Val Loss:1.5335 Val Acc:0.5691\n",
      "Epoch:297/500 Train Loss:1.2103 Val Loss:1.537 Val Acc:0.5703\n",
      "Epoch:298/500 Train Loss:1.2123 Val Loss:1.5268 Val Acc:0.5692\n",
      "Epoch:299/500 Train Loss:1.2106 Val Loss:1.5247 Val Acc:0.5709\n",
      "Epoch:300/500 Train Loss:1.2097 Val Loss:1.5251 Val Acc:0.5706\n",
      "Epoch:301/500 Train Loss:1.2112 Val Loss:1.5229 Val Acc:0.5669\n",
      "Epoch:302/500 Train Loss:1.2096 Val Loss:1.5351 Val Acc:0.572\n",
      "Epoch:303/500 Train Loss:1.2014 Val Loss:1.5283 Val Acc:0.5685\n",
      "Epoch:304/500 Train Loss:1.2054 Val Loss:1.5254 Val Acc:0.5685\n",
      "Epoch:305/500 Train Loss:1.2005 Val Loss:1.5201 Val Acc:0.5709\n",
      "Epoch:306/500 Train Loss:1.204 Val Loss:1.5154 Val Acc:0.5748\n",
      "Epoch:307/500 Train Loss:1.2003 Val Loss:1.5169 Val Acc:0.5706\n",
      "Epoch:308/500 Train Loss:1.1989 Val Loss:1.5318 Val Acc:0.5702\n",
      "Epoch:309/500 Train Loss:1.1942 Val Loss:1.5142 Val Acc:0.5789\n",
      "Epoch:310/500 Train Loss:1.2001 Val Loss:1.5287 Val Acc:0.5674\n",
      "Epoch:311/500 Train Loss:1.1966 Val Loss:1.5212 Val Acc:0.575\n",
      "Epoch:312/500 Train Loss:1.198 Val Loss:1.5189 Val Acc:0.5728\n",
      "Epoch:313/500 Train Loss:1.1914 Val Loss:1.5185 Val Acc:0.5695\n",
      "Epoch:314/500 Train Loss:1.1961 Val Loss:1.5257 Val Acc:0.5768\n",
      "Epoch:315/500 Train Loss:1.1871 Val Loss:1.5191 Val Acc:0.5773\n",
      "Epoch:316/500 Train Loss:1.1937 Val Loss:1.5173 Val Acc:0.5779\n",
      "Epoch:317/500 Train Loss:1.1873 Val Loss:1.519 Val Acc:0.5672\n",
      "Epoch:318/500 Train Loss:1.1881 Val Loss:1.5059 Val Acc:0.5759\n",
      "Epoch:319/500 Train Loss:1.1908 Val Loss:1.5194 Val Acc:0.575\n",
      "Epoch:320/500 Train Loss:1.1902 Val Loss:1.5342 Val Acc:0.5708\n",
      "Epoch:321/500 Train Loss:1.1835 Val Loss:1.5272 Val Acc:0.572\n",
      "Epoch:322/500 Train Loss:1.1817 Val Loss:1.5226 Val Acc:0.5716\n",
      "Epoch:323/500 Train Loss:1.1876 Val Loss:1.5091 Val Acc:0.5663\n",
      "Epoch:324/500 Train Loss:1.1825 Val Loss:1.5207 Val Acc:0.5716\n",
      "Epoch:325/500 Train Loss:1.1837 Val Loss:1.5184 Val Acc:0.5723\n",
      "Epoch:326/500 Train Loss:1.1824 Val Loss:1.5211 Val Acc:0.5761\n",
      "Epoch:327/500 Train Loss:1.1788 Val Loss:1.5091 Val Acc:0.5784\n",
      "Epoch:328/500 Train Loss:1.1786 Val Loss:1.5177 Val Acc:0.5784\n",
      "Epoch:329/500 Train Loss:1.1798 Val Loss:1.5207 Val Acc:0.5772\n",
      "Epoch:330/500 Train Loss:1.1772 Val Loss:1.5272 Val Acc:0.5699\n",
      "Epoch:331/500 Train Loss:1.1787 Val Loss:1.521 Val Acc:0.5725\n",
      "Epoch:332/500 Train Loss:1.1778 Val Loss:1.5139 Val Acc:0.5733\n",
      "Epoch:333/500 Train Loss:1.1699 Val Loss:1.5049 Val Acc:0.5779\n",
      "Epoch:334/500 Train Loss:1.1755 Val Loss:1.5187 Val Acc:0.5736\n",
      "Epoch:335/500 Train Loss:1.1759 Val Loss:1.5107 Val Acc:0.5775\n",
      "Epoch:336/500 Train Loss:1.1713 Val Loss:1.5094 Val Acc:0.5775\n",
      "Epoch:337/500 Train Loss:1.1685 Val Loss:1.5153 Val Acc:0.5726\n",
      "Epoch:338/500 Train Loss:1.1724 Val Loss:1.5111 Val Acc:0.5747\n",
      "Epoch:339/500 Train Loss:1.1664 Val Loss:1.5159 Val Acc:0.5716\n",
      "Epoch:340/500 Train Loss:1.1602 Val Loss:1.5136 Val Acc:0.5685\n",
      "Epoch:341/500 Train Loss:1.1661 Val Loss:1.501 Val Acc:0.5787\n",
      "Epoch:342/500 Train Loss:1.1634 Val Loss:1.5137 Val Acc:0.5759\n",
      "Epoch:343/500 Train Loss:1.1684 Val Loss:1.5259 Val Acc:0.5744\n",
      "Epoch:344/500 Train Loss:1.1656 Val Loss:1.5053 Val Acc:0.5744\n",
      "Epoch:345/500 Train Loss:1.1643 Val Loss:1.5103 Val Acc:0.5768\n",
      "Epoch:346/500 Train Loss:1.1592 Val Loss:1.5078 Val Acc:0.5778\n",
      "Epoch:347/500 Train Loss:1.1616 Val Loss:1.5097 Val Acc:0.5739\n",
      "Epoch:348/500 Train Loss:1.1611 Val Loss:1.5109 Val Acc:0.5772\n",
      "Epoch:349/500 Train Loss:1.1599 Val Loss:1.5103 Val Acc:0.5772\n",
      "Epoch:350/500 Train Loss:1.1562 Val Loss:1.5192 Val Acc:0.5751\n",
      "Epoch:351/500 Train Loss:1.1545 Val Loss:1.5183 Val Acc:0.5747\n",
      "Epoch:352/500 Train Loss:1.1592 Val Loss:1.5244 Val Acc:0.5775\n",
      "Epoch:353/500 Train Loss:1.1573 Val Loss:1.495 Val Acc:0.582\n",
      "Epoch:354/500 Train Loss:1.1521 Val Loss:1.5024 Val Acc:0.5782\n",
      "Epoch:355/500 Train Loss:1.1571 Val Loss:1.5177 Val Acc:0.5744\n",
      "Epoch:356/500 Train Loss:1.155 Val Loss:1.5116 Val Acc:0.5756\n",
      "Epoch:357/500 Train Loss:1.1524 Val Loss:1.5167 Val Acc:0.5726\n",
      "Epoch:358/500 Train Loss:1.1493 Val Loss:1.5104 Val Acc:0.5736\n",
      "Epoch:359/500 Train Loss:1.1467 Val Loss:1.5074 Val Acc:0.5789\n",
      "Epoch:360/500 Train Loss:1.1515 Val Loss:1.5103 Val Acc:0.5754\n",
      "Epoch:361/500 Train Loss:1.1489 Val Loss:1.5116 Val Acc:0.5801\n",
      "Epoch:362/500 Train Loss:1.1444 Val Loss:1.5187 Val Acc:0.5739\n",
      "Epoch:363/500 Train Loss:1.1458 Val Loss:1.5178 Val Acc:0.5747\n",
      "Epoch:364/500 Train Loss:1.141 Val Loss:1.511 Val Acc:0.5817\n",
      "Epoch:365/500 Train Loss:1.148 Val Loss:1.5023 Val Acc:0.5784\n",
      "Epoch:366/500 Train Loss:1.1467 Val Loss:1.516 Val Acc:0.5786\n",
      "Epoch:367/500 Train Loss:1.1432 Val Loss:1.504 Val Acc:0.5765\n",
      "Epoch:368/500 Train Loss:1.143 Val Loss:1.5122 Val Acc:0.5768\n",
      "Epoch:369/500 Train Loss:1.1396 Val Loss:1.519 Val Acc:0.5733\n",
      "Epoch:370/500 Train Loss:1.1422 Val Loss:1.5115 Val Acc:0.5765\n",
      "Epoch:371/500 Train Loss:1.1452 Val Loss:1.5147 Val Acc:0.5725\n",
      "Epoch:372/500 Train Loss:1.1423 Val Loss:1.5151 Val Acc:0.5745\n",
      "Epoch:373/500 Train Loss:1.1412 Val Loss:1.5163 Val Acc:0.5787\n",
      "Epoch:374/500 Train Loss:1.1331 Val Loss:1.4988 Val Acc:0.5796\n",
      "Epoch:375/500 Train Loss:1.1382 Val Loss:1.4951 Val Acc:0.5837\n",
      "Epoch:376/500 Train Loss:1.1342 Val Loss:1.5062 Val Acc:0.5789\n",
      "Epoch:377/500 Train Loss:1.1323 Val Loss:1.5169 Val Acc:0.5737\n",
      "Epoch:378/500 Train Loss:1.1351 Val Loss:1.5102 Val Acc:0.5767\n",
      "Epoch:379/500 Train Loss:1.1314 Val Loss:1.4956 Val Acc:0.5849\n",
      "Epoch:380/500 Train Loss:1.1295 Val Loss:1.504 Val Acc:0.5792\n",
      "Epoch:381/500 Train Loss:1.134 Val Loss:1.5114 Val Acc:0.5748\n",
      "Epoch:382/500 Train Loss:1.1252 Val Loss:1.4969 Val Acc:0.5828\n",
      "Epoch:383/500 Train Loss:1.1333 Val Loss:1.5068 Val Acc:0.5782\n",
      "Epoch:384/500 Train Loss:1.1301 Val Loss:1.5112 Val Acc:0.5779\n",
      "Epoch:385/500 Train Loss:1.1334 Val Loss:1.5081 Val Acc:0.5767\n",
      "Epoch:386/500 Train Loss:1.1285 Val Loss:1.5053 Val Acc:0.5772\n",
      "Epoch:387/500 Train Loss:1.1287 Val Loss:1.502 Val Acc:0.5789\n",
      "Epoch:388/500 Train Loss:1.1322 Val Loss:1.5038 Val Acc:0.5737\n",
      "Epoch:389/500 Train Loss:1.1283 Val Loss:1.4956 Val Acc:0.5784\n",
      "Epoch:390/500 Train Loss:1.1224 Val Loss:1.5001 Val Acc:0.58\n",
      "Epoch:391/500 Train Loss:1.1216 Val Loss:1.5133 Val Acc:0.5792\n",
      "Epoch:392/500 Train Loss:1.1232 Val Loss:1.5116 Val Acc:0.577\n",
      "Epoch:393/500 Train Loss:1.1191 Val Loss:1.5028 Val Acc:0.5787\n",
      "Epoch:394/500 Train Loss:1.1238 Val Loss:1.5084 Val Acc:0.5758\n",
      "Epoch:395/500 Train Loss:1.1216 Val Loss:1.5089 Val Acc:0.5801\n",
      "Epoch:396/500 Train Loss:1.1243 Val Loss:1.5073 Val Acc:0.5756\n",
      "Epoch:397/500 Train Loss:1.1208 Val Loss:1.5058 Val Acc:0.5789\n",
      "Epoch:398/500 Train Loss:1.1155 Val Loss:1.5074 Val Acc:0.5787\n",
      "Epoch:399/500 Train Loss:1.1215 Val Loss:1.5127 Val Acc:0.5815\n",
      "Epoch:400/500 Train Loss:1.1186 Val Loss:1.5043 Val Acc:0.5753\n",
      "Epoch:401/500 Train Loss:1.1211 Val Loss:1.5079 Val Acc:0.5781\n",
      "Epoch:402/500 Train Loss:1.1181 Val Loss:1.5134 Val Acc:0.5768\n",
      "Epoch:403/500 Train Loss:1.1173 Val Loss:1.5005 Val Acc:0.5779\n",
      "Epoch:404/500 Train Loss:1.1092 Val Loss:1.5061 Val Acc:0.5761\n",
      "Epoch:405/500 Train Loss:1.111 Val Loss:1.5119 Val Acc:0.5753\n",
      "Epoch:406/500 Train Loss:1.1139 Val Loss:1.5077 Val Acc:0.5767\n",
      "Epoch:407/500 Train Loss:1.1158 Val Loss:1.5071 Val Acc:0.5795\n",
      "Epoch:408/500 Train Loss:1.1172 Val Loss:1.5031 Val Acc:0.5779\n",
      "Epoch:409/500 Train Loss:1.1061 Val Loss:1.5181 Val Acc:0.5784\n",
      "Epoch:410/500 Train Loss:1.1102 Val Loss:1.4989 Val Acc:0.5761\n",
      "Epoch:411/500 Train Loss:1.1136 Val Loss:1.5007 Val Acc:0.5792\n",
      "Epoch:412/500 Train Loss:1.1035 Val Loss:1.5003 Val Acc:0.5814\n",
      "Epoch:413/500 Train Loss:1.1034 Val Loss:1.5179 Val Acc:0.5818\n",
      "Epoch:414/500 Train Loss:1.1062 Val Loss:1.5035 Val Acc:0.584\n",
      "Epoch:415/500 Train Loss:1.1108 Val Loss:1.4955 Val Acc:0.5768\n",
      "Epoch:416/500 Train Loss:1.102 Val Loss:1.5096 Val Acc:0.5801\n",
      "Epoch:417/500 Train Loss:1.1003 Val Loss:1.5047 Val Acc:0.582\n",
      "Epoch:418/500 Train Loss:1.1062 Val Loss:1.5067 Val Acc:0.5754\n",
      "Epoch:419/500 Train Loss:1.0994 Val Loss:1.5102 Val Acc:0.582\n",
      "Epoch:420/500 Train Loss:1.0991 Val Loss:1.5105 Val Acc:0.5809\n",
      "Epoch:421/500 Train Loss:1.1007 Val Loss:1.502 Val Acc:0.5818\n",
      "Epoch:422/500 Train Loss:1.1004 Val Loss:1.5084 Val Acc:0.5773\n",
      "Epoch:423/500 Train Loss:1.1003 Val Loss:1.4965 Val Acc:0.5812\n",
      "Epoch:424/500 Train Loss:1.0951 Val Loss:1.4919 Val Acc:0.5851\n",
      "Epoch:425/500 Train Loss:1.0918 Val Loss:1.5123 Val Acc:0.5754\n",
      "Epoch:426/500 Train Loss:1.098 Val Loss:1.4917 Val Acc:0.584\n",
      "Epoch:427/500 Train Loss:1.0969 Val Loss:1.495 Val Acc:0.5824\n",
      "Epoch:428/500 Train Loss:1.0977 Val Loss:1.5085 Val Acc:0.582\n",
      "Epoch:429/500 Train Loss:1.0968 Val Loss:1.5057 Val Acc:0.581\n",
      "Epoch:430/500 Train Loss:1.0992 Val Loss:1.5102 Val Acc:0.5823\n",
      "Epoch:431/500 Train Loss:1.0925 Val Loss:1.5002 Val Acc:0.5848\n",
      "Epoch:432/500 Train Loss:1.0905 Val Loss:1.5101 Val Acc:0.5765\n",
      "Epoch:433/500 Train Loss:1.0896 Val Loss:1.5148 Val Acc:0.5773\n",
      "Epoch:434/500 Train Loss:1.0907 Val Loss:1.5079 Val Acc:0.5812\n",
      "Epoch:435/500 Train Loss:1.0899 Val Loss:1.504 Val Acc:0.5812\n",
      "Epoch:436/500 Train Loss:1.0882 Val Loss:1.5054 Val Acc:0.5837\n",
      "Epoch:437/500 Train Loss:1.0912 Val Loss:1.5079 Val Acc:0.581\n",
      "Epoch:438/500 Train Loss:1.0877 Val Loss:1.519 Val Acc:0.5737\n",
      "Epoch:439/500 Train Loss:1.0879 Val Loss:1.5084 Val Acc:0.5843\n",
      "Epoch:440/500 Train Loss:1.0883 Val Loss:1.5021 Val Acc:0.5838\n",
      "Epoch:441/500 Train Loss:1.0863 Val Loss:1.5165 Val Acc:0.574\n",
      "Epoch:442/500 Train Loss:1.0929 Val Loss:1.5062 Val Acc:0.5817\n",
      "Epoch:443/500 Train Loss:1.084 Val Loss:1.5097 Val Acc:0.5807\n",
      "Epoch:444/500 Train Loss:1.0844 Val Loss:1.4936 Val Acc:0.582\n",
      "Epoch:445/500 Train Loss:1.0843 Val Loss:1.5014 Val Acc:0.5787\n",
      "Epoch:446/500 Train Loss:1.0837 Val Loss:1.5051 Val Acc:0.5786\n",
      "Epoch:447/500 Train Loss:1.0786 Val Loss:1.5107 Val Acc:0.5807\n",
      "Epoch:448/500 Train Loss:1.0792 Val Loss:1.5047 Val Acc:0.5767\n",
      "Epoch:449/500 Train Loss:1.08 Val Loss:1.5109 Val Acc:0.58\n",
      "Epoch:450/500 Train Loss:1.0821 Val Loss:1.5141 Val Acc:0.5798\n",
      "Epoch:451/500 Train Loss:1.0839 Val Loss:1.5116 Val Acc:0.5846\n",
      "Epoch:452/500 Train Loss:1.073 Val Loss:1.4963 Val Acc:0.5835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:453/500 Train Loss:1.0772 Val Loss:1.4941 Val Acc:0.5841\n",
      "Epoch:454/500 Train Loss:1.0836 Val Loss:1.5134 Val Acc:0.5772\n",
      "Epoch:455/500 Train Loss:1.0745 Val Loss:1.4939 Val Acc:0.5835\n",
      "Epoch:456/500 Train Loss:1.0771 Val Loss:1.504 Val Acc:0.5762\n",
      "Epoch:457/500 Train Loss:1.0735 Val Loss:1.5112 Val Acc:0.5739\n",
      "Epoch:458/500 Train Loss:1.0782 Val Loss:1.5069 Val Acc:0.5841\n",
      "Epoch:459/500 Train Loss:1.0769 Val Loss:1.5114 Val Acc:0.5829\n",
      "Epoch:460/500 Train Loss:1.0744 Val Loss:1.5017 Val Acc:0.5865\n",
      "Epoch:461/500 Train Loss:1.0718 Val Loss:1.5025 Val Acc:0.5809\n",
      "Epoch:462/500 Train Loss:1.0682 Val Loss:1.506 Val Acc:0.5798\n",
      "Epoch:463/500 Train Loss:1.0739 Val Loss:1.5114 Val Acc:0.5789\n",
      "Epoch:464/500 Train Loss:1.0753 Val Loss:1.4942 Val Acc:0.5796\n",
      "Epoch:465/500 Train Loss:1.0695 Val Loss:1.4959 Val Acc:0.5838\n",
      "Epoch:466/500 Train Loss:1.0629 Val Loss:1.4971 Val Acc:0.5838\n",
      "Epoch:467/500 Train Loss:1.0673 Val Loss:1.5051 Val Acc:0.5826\n",
      "Epoch:468/500 Train Loss:1.0684 Val Loss:1.4953 Val Acc:0.5814\n",
      "Epoch:469/500 Train Loss:1.0718 Val Loss:1.5109 Val Acc:0.5812\n",
      "Epoch:470/500 Train Loss:1.0655 Val Loss:1.4947 Val Acc:0.5815\n",
      "Epoch:471/500 Train Loss:1.0682 Val Loss:1.5031 Val Acc:0.5765\n",
      "Epoch:472/500 Train Loss:1.0635 Val Loss:1.5119 Val Acc:0.5803\n",
      "Epoch:473/500 Train Loss:1.0612 Val Loss:1.5086 Val Acc:0.5835\n",
      "Epoch:474/500 Train Loss:1.0645 Val Loss:1.498 Val Acc:0.5817\n",
      "Epoch:475/500 Train Loss:1.0668 Val Loss:1.4919 Val Acc:0.5841\n",
      "Epoch:476/500 Train Loss:1.0638 Val Loss:1.515 Val Acc:0.5789\n",
      "Epoch:477/500 Train Loss:1.0613 Val Loss:1.5088 Val Acc:0.5804\n",
      "Epoch:478/500 Train Loss:1.0556 Val Loss:1.4878 Val Acc:0.584\n",
      "Epoch:479/500 Train Loss:1.0615 Val Loss:1.4956 Val Acc:0.5829\n",
      "Epoch:480/500 Train Loss:1.0614 Val Loss:1.5081 Val Acc:0.5852\n",
      "Epoch:481/500 Train Loss:1.0591 Val Loss:1.4954 Val Acc:0.5838\n",
      "Epoch:482/500 Train Loss:1.0583 Val Loss:1.508 Val Acc:0.581\n",
      "Epoch:483/500 Train Loss:1.0559 Val Loss:1.5016 Val Acc:0.582\n",
      "Epoch:484/500 Train Loss:1.0511 Val Loss:1.5135 Val Acc:0.579\n",
      "Epoch:485/500 Train Loss:1.0608 Val Loss:1.5033 Val Acc:0.5837\n",
      "Epoch:486/500 Train Loss:1.0566 Val Loss:1.5117 Val Acc:0.5804\n",
      "Epoch:487/500 Train Loss:1.0526 Val Loss:1.5062 Val Acc:0.5776\n",
      "Epoch:488/500 Train Loss:1.0518 Val Loss:1.5033 Val Acc:0.5806\n",
      "Epoch:489/500 Train Loss:1.0569 Val Loss:1.5125 Val Acc:0.5796\n",
      "Epoch:490/500 Train Loss:1.0473 Val Loss:1.5005 Val Acc:0.5786\n",
      "Epoch:491/500 Train Loss:1.0512 Val Loss:1.5033 Val Acc:0.5866\n",
      "Epoch:492/500 Train Loss:1.051 Val Loss:1.5037 Val Acc:0.5857\n",
      "Epoch:493/500 Train Loss:1.0535 Val Loss:1.4962 Val Acc:0.5818\n",
      "Epoch:494/500 Train Loss:1.0494 Val Loss:1.5112 Val Acc:0.5826\n",
      "Epoch:495/500 Train Loss:1.0535 Val Loss:1.5013 Val Acc:0.5817\n",
      "Epoch:496/500 Train Loss:1.0492 Val Loss:1.4917 Val Acc:0.5866\n",
      "Epoch:497/500 Train Loss:1.0457 Val Loss:1.5087 Val Acc:0.5798\n",
      "Epoch:498/500 Train Loss:1.0454 Val Loss:1.5074 Val Acc:0.5779\n",
      "Epoch:499/500 Train Loss:1.045 Val Loss:1.498 Val Acc:0.5838\n",
      "Epoch:500/500 Train Loss:1.0463 Val Loss:1.499 Val Acc:0.584\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_encoder = None\n",
    "best_mlp = None\n",
    "evaluator = Evaluator(name='ogbn-arxiv')\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = train(encoder, mlp, optimizer, data_2012_2013)\n",
    "    val_loss, val_acc = test(encoder, mlp, data_2012_2013, evaluator)\n",
    "    print(f\"Epoch:{e}/{epochs} Train Loss:{round(train_loss,4)} Val Loss:{round(val_loss,4)} Val Acc:{round(val_acc, 4)}\")\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_encoder = deepcopy(encoder)\n",
    "        best_mlp = deepcopy(mlp)\n",
    "\n",
    "encoder = deepcopy(best_encoder)\n",
    "mlp = deepcopy(best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31e6cea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5866355866355867"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888217b",
   "metadata": {},
   "source": [
    "# Prequential Evaluation at Subsequent Time Steps #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c97a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continual_adapt(src_data, tgt_split, encoder, mlp, device, lambda_coeff=1, lr=1e-3):\n",
    "    print(\"Start partitioning data...\")\n",
    "    tgt_data = temp_partition_arxiv(data, year_bound=tgt_split, proportion=1.0)\n",
    "    tgt_data.to(device)\n",
    "    print(\"Finish partitioning data...\")\n",
    "    thetas = None # none adversarial\n",
    "    jmmd_loss = JointMultipleKernelMaximumMeanDiscrepancy(\n",
    "        kernels=(\n",
    "            [GaussianKernel(alpha=2 ** k) for k in range(-3, 2)],\n",
    "            (GaussianKernel(sigma=0.92, track_running_stats=False),)\n",
    "        ),\n",
    "        linear=False, thetas=thetas\n",
    "    ).to(device)\n",
    "    tgt_encoder, tgt_mlp = deepcopy(encoder), deepcopy(mlp)\n",
    "    tgt_optimizer = torch.optim.Adam(list(tgt_encoder.parameters()) + list(tgt_mlp.parameters()), lr=lr)\n",
    "    \n",
    "    epochs = 500\n",
    "    best_val_loss = np.inf\n",
    "    best_val_cls_loss, best_val_transfer_loss = None, None\n",
    "    best_tgt_encoder, best_tgt_mlp = None, None\n",
    "    patience = 10\n",
    "    staleness = 0\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        total_train_loss, total_train_cls_loss, total_train_transfer_loss = adapt(tgt_encoder, tgt_mlp, jmmd_loss, src_data, tgt_data, tgt_optimizer, e, epochs, lambda_coeff) \n",
    "        total_val_loss, total_val_cls_loss, total_val_transfer_loss = adapt_test(tgt_encoder, tgt_mlp, jmmd_loss, src_data, tgt_data, e, epochs, lambda_coeff)\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            best_val_cls_loss = total_val_cls_loss\n",
    "            best_val_transfer_loss = total_val_transfer_loss\n",
    "            best_tgt_encoder = deepcopy(tgt_encoder)\n",
    "            best_tgt_mlp = deepcopy(tgt_mlp)\n",
    "            staleness = 0\n",
    "        else:\n",
    "            staleness += 1\n",
    "        print(f'Epoch {e}/{epochs} Train Total Loss: {round(total_train_loss,3)} Train Src Cls Loss: {round(total_train_cls_loss,3)} Train Tgt Transfer Loss: {round(total_train_transfer_loss,3)} \\n Val Total Loss: {round(total_val_loss,3)} Val Src Cls Loss: {round(total_val_cls_loss,3)} Val Tgt Transfer Loss: {round(total_val_transfer_loss,3)}')\n",
    "\n",
    "        if staleness > patience:\n",
    "            break\n",
    "\n",
    "    tgt_encoder = deepcopy(best_tgt_encoder)\n",
    "    tgt_mlp = deepcopy(best_tgt_mlp)\n",
    "    \n",
    "    return tgt_encoder, tgt_mlp, best_val_loss, best_val_cls_loss, best_val_transfer_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cc1a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_coeff = 100\n",
    "lr = 1e-3\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298845d3",
   "metadata": {},
   "source": [
    "## 2013-2014 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c33a9c",
   "metadata": {},
   "source": [
    "* Test:\n",
    "** 2013-2014 (then adapt 2012-2013 adapt-val 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fe91203",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2013_2015 = temp_partition_arxiv(data, year_bound=[-1,2013,2015], proportion=1.0)\n",
    "data_2013_2015 = data_2013_2015.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd7cba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder, mlp, data_2013_2015, evaluator)\n",
    "test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aea6798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.476 Test Acc: 0.591\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {round(test_loss,3)} Test Acc: {round(test_acc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "055bb2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start partitioning data...\n",
      "Finish partitioning data...\n",
      "Epoch 1/500 Train Total Loss: 9.452 Train Src Cls Loss: 1.055 Train Tgt Transfer Loss: 0.084 \n",
      " Val Total Loss: 8.937 Val Src Cls Loss: 1.627 Val Tgt Transfer Loss: 0.073\n",
      "Epoch 2/500 Train Total Loss: 8.869 Train Src Cls Loss: 1.11 Train Tgt Transfer Loss: 0.078 \n",
      " Val Total Loss: 8.84 Val Src Cls Loss: 1.837 Val Tgt Transfer Loss: 0.07\n",
      "Epoch 3/500 Train Total Loss: 8.84 Train Src Cls Loss: 1.237 Train Tgt Transfer Loss: 0.076 \n",
      " Val Total Loss: 9.27 Val Src Cls Loss: 2.02 Val Tgt Transfer Loss: 0.073\n",
      "Epoch 4/500 Train Total Loss: 8.871 Train Src Cls Loss: 1.356 Train Tgt Transfer Loss: 0.075 \n",
      " Val Total Loss: 9.311 Val Src Cls Loss: 1.998 Val Tgt Transfer Loss: 0.073\n",
      "Epoch 5/500 Train Total Loss: 8.711 Train Src Cls Loss: 1.318 Train Tgt Transfer Loss: 0.074 \n",
      " Val Total Loss: 9.106 Val Src Cls Loss: 1.998 Val Tgt Transfer Loss: 0.071\n",
      "Epoch 6/500 Train Total Loss: 8.623 Train Src Cls Loss: 1.337 Train Tgt Transfer Loss: 0.073 \n",
      " Val Total Loss: 8.926 Val Src Cls Loss: 2.021 Val Tgt Transfer Loss: 0.069\n",
      "Epoch 7/500 Train Total Loss: 8.788 Train Src Cls Loss: 1.355 Train Tgt Transfer Loss: 0.074 \n",
      " Val Total Loss: 8.944 Val Src Cls Loss: 2.032 Val Tgt Transfer Loss: 0.069\n",
      "Epoch 8/500 Train Total Loss: 8.176 Train Src Cls Loss: 1.349 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 9.397 Val Src Cls Loss: 2.029 Val Tgt Transfer Loss: 0.074\n",
      "Epoch 9/500 Train Total Loss: 8.081 Train Src Cls Loss: 1.36 Train Tgt Transfer Loss: 0.067 \n",
      " Val Total Loss: 8.696 Val Src Cls Loss: 2.003 Val Tgt Transfer Loss: 0.067\n",
      "Epoch 10/500 Train Total Loss: 7.793 Train Src Cls Loss: 1.36 Train Tgt Transfer Loss: 0.064 \n",
      " Val Total Loss: 9.502 Val Src Cls Loss: 1.989 Val Tgt Transfer Loss: 0.075\n",
      "Epoch 11/500 Train Total Loss: 8.042 Train Src Cls Loss: 1.339 Train Tgt Transfer Loss: 0.067 \n",
      " Val Total Loss: 9.448 Val Src Cls Loss: 1.977 Val Tgt Transfer Loss: 0.075\n",
      "Epoch 12/500 Train Total Loss: 7.983 Train Src Cls Loss: 1.328 Train Tgt Transfer Loss: 0.067 \n",
      " Val Total Loss: 9.33 Val Src Cls Loss: 1.93 Val Tgt Transfer Loss: 0.074\n",
      "Epoch 13/500 Train Total Loss: 8.024 Train Src Cls Loss: 1.317 Train Tgt Transfer Loss: 0.067 \n",
      " Val Total Loss: 9.342 Val Src Cls Loss: 1.887 Val Tgt Transfer Loss: 0.075\n",
      "Epoch 14/500 Train Total Loss: 8.23 Train Src Cls Loss: 1.301 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 8.987 Val Src Cls Loss: 1.868 Val Tgt Transfer Loss: 0.071\n",
      "Epoch 15/500 Train Total Loss: 7.794 Train Src Cls Loss: 1.291 Train Tgt Transfer Loss: 0.065 \n",
      " Val Total Loss: 8.932 Val Src Cls Loss: 1.841 Val Tgt Transfer Loss: 0.071\n",
      "Epoch 16/500 Train Total Loss: 7.989 Train Src Cls Loss: 1.305 Train Tgt Transfer Loss: 0.067 \n",
      " Val Total Loss: 8.733 Val Src Cls Loss: 1.839 Val Tgt Transfer Loss: 0.069\n",
      "Epoch 17/500 Train Total Loss: 8.44 Train Src Cls Loss: 1.3 Train Tgt Transfer Loss: 0.071 \n",
      " Val Total Loss: 8.738 Val Src Cls Loss: 1.821 Val Tgt Transfer Loss: 0.069\n",
      "Epoch 18/500 Train Total Loss: 7.706 Train Src Cls Loss: 1.303 Train Tgt Transfer Loss: 0.064 \n",
      " Val Total Loss: 9.003 Val Src Cls Loss: 1.822 Val Tgt Transfer Loss: 0.072\n",
      "Epoch 19/500 Train Total Loss: 7.831 Train Src Cls Loss: 1.296 Train Tgt Transfer Loss: 0.065 \n",
      " Val Total Loss: 9.577 Val Src Cls Loss: 1.811 Val Tgt Transfer Loss: 0.078\n",
      "Epoch 20/500 Train Total Loss: 8.3 Train Src Cls Loss: 1.295 Train Tgt Transfer Loss: 0.07 \n",
      " Val Total Loss: 9.582 Val Src Cls Loss: 1.792 Val Tgt Transfer Loss: 0.078\n"
     ]
    }
   ],
   "source": [
    "encoder_2012_2014_2015, mlp_2012_2014_2015, best_val_loss, best_val_cls_loss, best_val_transfer_loss = continual_adapt(data_2012_2013, [2012,2014,2015], encoder, mlp, device, lambda_coeff=lambda_coeff, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d69e18b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Val Loss: 8.695588111877441 Val Cls Loss: 2.0033702850341797 Val Transfer Loss: 0.06692218035459518\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Val Loss: {best_val_loss} Val Cls Loss: {best_val_cls_loss} Val Transfer Loss: {best_val_transfer_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f73924",
   "metadata": {},
   "source": [
    "## 2015-2016 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa950b02",
   "metadata": {},
   "source": [
    "* Test:\n",
    "** 2015-2016 (then adapt 2014-2015 adapt-val 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0ee8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2015_2017 = temp_partition_arxiv(data, year_bound=[-1,2015,2017], proportion=1.0)\n",
    "data_2015_2017 = data_2015_2017.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ad9c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder_2012_2014_2015, mlp_2012_2014_2015, data_2015_2017, evaluator)\n",
    "test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40d59936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.128 Test Acc: 0.481\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {round(test_loss,3)} Test Acc: {round(test_acc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "695410a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start partitioning data...\n",
      "Finish partitioning data...\n",
      "Epoch 1/500 Train Total Loss: 9.468 Train Src Cls Loss: 1.352 Train Tgt Transfer Loss: 0.081 \n",
      " Val Total Loss: 12.756 Val Src Cls Loss: 2.239 Val Tgt Transfer Loss: 0.105\n",
      "Epoch 2/500 Train Total Loss: 9.503 Train Src Cls Loss: 1.504 Train Tgt Transfer Loss: 0.08 \n",
      " Val Total Loss: 11.841 Val Src Cls Loss: 2.136 Val Tgt Transfer Loss: 0.097\n",
      "Epoch 3/500 Train Total Loss: 9.102 Train Src Cls Loss: 1.444 Train Tgt Transfer Loss: 0.077 \n",
      " Val Total Loss: 11.862 Val Src Cls Loss: 2.052 Val Tgt Transfer Loss: 0.098\n",
      "Epoch 4/500 Train Total Loss: 8.752 Train Src Cls Loss: 1.392 Train Tgt Transfer Loss: 0.074 \n",
      " Val Total Loss: 12.227 Val Src Cls Loss: 2.031 Val Tgt Transfer Loss: 0.102\n",
      "Epoch 5/500 Train Total Loss: 8.836 Train Src Cls Loss: 1.402 Train Tgt Transfer Loss: 0.074 \n",
      " Val Total Loss: 11.492 Val Src Cls Loss: 2.031 Val Tgt Transfer Loss: 0.095\n",
      "Epoch 6/500 Train Total Loss: 8.464 Train Src Cls Loss: 1.404 Train Tgt Transfer Loss: 0.071 \n",
      " Val Total Loss: 10.876 Val Src Cls Loss: 2.012 Val Tgt Transfer Loss: 0.089\n",
      "Epoch 7/500 Train Total Loss: 8.416 Train Src Cls Loss: 1.412 Train Tgt Transfer Loss: 0.07 \n",
      " Val Total Loss: 9.98 Val Src Cls Loss: 2.032 Val Tgt Transfer Loss: 0.079\n",
      "Epoch 8/500 Train Total Loss: 8.593 Train Src Cls Loss: 1.447 Train Tgt Transfer Loss: 0.071 \n",
      " Val Total Loss: 10.627 Val Src Cls Loss: 2.05 Val Tgt Transfer Loss: 0.086\n",
      "Epoch 9/500 Train Total Loss: 8.243 Train Src Cls Loss: 1.477 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 9.885 Val Src Cls Loss: 2.031 Val Tgt Transfer Loss: 0.079\n",
      "Epoch 10/500 Train Total Loss: 8.5 Train Src Cls Loss: 1.459 Train Tgt Transfer Loss: 0.07 \n",
      " Val Total Loss: 9.862 Val Src Cls Loss: 1.937 Val Tgt Transfer Loss: 0.079\n",
      "Epoch 11/500 Train Total Loss: 8.454 Train Src Cls Loss: 1.425 Train Tgt Transfer Loss: 0.07 \n",
      " Val Total Loss: 10.091 Val Src Cls Loss: 1.872 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 12/500 Train Total Loss: 8.156 Train Src Cls Loss: 1.39 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 10.321 Val Src Cls Loss: 1.822 Val Tgt Transfer Loss: 0.085\n",
      "Epoch 13/500 Train Total Loss: 7.921 Train Src Cls Loss: 1.365 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 10.007 Val Src Cls Loss: 1.818 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 14/500 Train Total Loss: 8.249 Train Src Cls Loss: 1.361 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 10.673 Val Src Cls Loss: 1.789 Val Tgt Transfer Loss: 0.089\n",
      "Epoch 15/500 Train Total Loss: 7.787 Train Src Cls Loss: 1.351 Train Tgt Transfer Loss: 0.064 \n",
      " Val Total Loss: 9.88 Val Src Cls Loss: 1.81 Val Tgt Transfer Loss: 0.081\n",
      "Epoch 16/500 Train Total Loss: 8.306 Train Src Cls Loss: 1.352 Train Tgt Transfer Loss: 0.07 \n",
      " Val Total Loss: 10.494 Val Src Cls Loss: 1.809 Val Tgt Transfer Loss: 0.087\n",
      "Epoch 17/500 Train Total Loss: 7.911 Train Src Cls Loss: 1.351 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 10.178 Val Src Cls Loss: 1.818 Val Tgt Transfer Loss: 0.084\n",
      "Epoch 18/500 Train Total Loss: 7.708 Train Src Cls Loss: 1.357 Train Tgt Transfer Loss: 0.064 \n",
      " Val Total Loss: 9.948 Val Src Cls Loss: 1.855 Val Tgt Transfer Loss: 0.081\n",
      "Epoch 19/500 Train Total Loss: 8.31 Train Src Cls Loss: 1.361 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 10.026 Val Src Cls Loss: 1.852 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 20/500 Train Total Loss: 7.931 Train Src Cls Loss: 1.365 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 9.825 Val Src Cls Loss: 1.859 Val Tgt Transfer Loss: 0.08\n",
      "Epoch 21/500 Train Total Loss: 7.909 Train Src Cls Loss: 1.36 Train Tgt Transfer Loss: 0.065 \n",
      " Val Total Loss: 10.177 Val Src Cls Loss: 1.862 Val Tgt Transfer Loss: 0.083\n",
      "Epoch 22/500 Train Total Loss: 8.129 Train Src Cls Loss: 1.351 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 9.938 Val Src Cls Loss: 1.839 Val Tgt Transfer Loss: 0.081\n",
      "Epoch 23/500 Train Total Loss: 8.279 Train Src Cls Loss: 1.343 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 10.053 Val Src Cls Loss: 1.844 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 24/500 Train Total Loss: 7.935 Train Src Cls Loss: 1.34 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 9.755 Val Src Cls Loss: 1.839 Val Tgt Transfer Loss: 0.079\n",
      "Epoch 25/500 Train Total Loss: 8.11 Train Src Cls Loss: 1.339 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 9.299 Val Src Cls Loss: 1.846 Val Tgt Transfer Loss: 0.075\n",
      "Epoch 26/500 Train Total Loss: 7.773 Train Src Cls Loss: 1.339 Train Tgt Transfer Loss: 0.064 \n",
      " Val Total Loss: 9.392 Val Src Cls Loss: 1.842 Val Tgt Transfer Loss: 0.075\n",
      "Epoch 27/500 Train Total Loss: 8.098 Train Src Cls Loss: 1.34 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 10.043 Val Src Cls Loss: 1.817 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 28/500 Train Total Loss: 8.151 Train Src Cls Loss: 1.321 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 10.368 Val Src Cls Loss: 1.805 Val Tgt Transfer Loss: 0.086\n",
      "Epoch 29/500 Train Total Loss: 7.871 Train Src Cls Loss: 1.316 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 9.38 Val Src Cls Loss: 1.813 Val Tgt Transfer Loss: 0.076\n",
      "Epoch 30/500 Train Total Loss: 7.645 Train Src Cls Loss: 1.312 Train Tgt Transfer Loss: 0.063 \n",
      " Val Total Loss: 10.005 Val Src Cls Loss: 1.8 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 31/500 Train Total Loss: 7.892 Train Src Cls Loss: 1.316 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 9.506 Val Src Cls Loss: 1.808 Val Tgt Transfer Loss: 0.077\n",
      "Epoch 32/500 Train Total Loss: 8.073 Train Src Cls Loss: 1.318 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 9.853 Val Src Cls Loss: 1.789 Val Tgt Transfer Loss: 0.081\n",
      "Epoch 33/500 Train Total Loss: 7.949 Train Src Cls Loss: 1.317 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 9.446 Val Src Cls Loss: 1.776 Val Tgt Transfer Loss: 0.077\n",
      "Epoch 34/500 Train Total Loss: 8.071 Train Src Cls Loss: 1.313 Train Tgt Transfer Loss: 0.068 \n",
      " Val Total Loss: 9.612 Val Src Cls Loss: 1.778 Val Tgt Transfer Loss: 0.078\n",
      "Epoch 35/500 Train Total Loss: 7.885 Train Src Cls Loss: 1.3 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 9.371 Val Src Cls Loss: 1.762 Val Tgt Transfer Loss: 0.076\n",
      "Epoch 36/500 Train Total Loss: 7.832 Train Src Cls Loss: 1.3 Train Tgt Transfer Loss: 0.065 \n",
      " Val Total Loss: 9.94 Val Src Cls Loss: 1.775 Val Tgt Transfer Loss: 0.082\n"
     ]
    }
   ],
   "source": [
    "encoder_2014_2016_2017, mlp_2014_2016_2017, best_val_loss, best_val_cls_loss, best_val_transfer_loss = continual_adapt(data_2012_2013, [2014,2016,2017], encoder_2012_2014_2015, mlp_2012_2014_2015, device, lambda_coeff=lambda_coeff, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5562118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Val Loss: 9.29905891418457 Val Cls Loss: 1.8460171222686768 Val Transfer Loss: 0.07453041523694992\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Val Loss: {best_val_loss} Val Cls Loss: {best_val_cls_loss} Val Transfer Loss: {best_val_transfer_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a70a4e",
   "metadata": {},
   "source": [
    "## 2017-2018 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac9074",
   "metadata": {},
   "source": [
    "* Test:\n",
    "** 2017-2018 (then adapt 2016-2017 adapt-val 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "823d9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017_2019 = temp_partition_arxiv(data, year_bound=[-1,2017,2019], proportion=1.0)\n",
    "data_2017_2019 = data_2017_2019.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10c0bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder_2014_2016_2017, mlp_2014_2016_2017, data_2017_2019, evaluator)\n",
    "test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6eae374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.779 Test Acc: 0.351\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {round(test_loss,3)} Test Acc: {round(test_acc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28c5c188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start partitioning data...\n",
      "Finish partitioning data...\n",
      "Epoch 1/500 Train Total Loss: 10.616 Train Src Cls Loss: 1.338 Train Tgt Transfer Loss: 0.093 \n",
      " Val Total Loss: 12.597 Val Src Cls Loss: 1.948 Val Tgt Transfer Loss: 0.106\n",
      "Epoch 2/500 Train Total Loss: 9.34 Train Src Cls Loss: 1.434 Train Tgt Transfer Loss: 0.079 \n",
      " Val Total Loss: 11.48 Val Src Cls Loss: 2.099 Val Tgt Transfer Loss: 0.094\n",
      "Epoch 3/500 Train Total Loss: 9.25 Train Src Cls Loss: 1.536 Train Tgt Transfer Loss: 0.077 \n",
      " Val Total Loss: 10.994 Val Src Cls Loss: 2.129 Val Tgt Transfer Loss: 0.089\n",
      "Epoch 4/500 Train Total Loss: 8.81 Train Src Cls Loss: 1.597 Train Tgt Transfer Loss: 0.072 \n",
      " Val Total Loss: 10.348 Val Src Cls Loss: 2.15 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 5/500 Train Total Loss: 9.028 Train Src Cls Loss: 1.623 Train Tgt Transfer Loss: 0.074 \n",
      " Val Total Loss: 10.92 Val Src Cls Loss: 2.135 Val Tgt Transfer Loss: 0.088\n",
      "Epoch 6/500 Train Total Loss: 9.228 Train Src Cls Loss: 1.629 Train Tgt Transfer Loss: 0.076 \n",
      " Val Total Loss: 10.374 Val Src Cls Loss: 2.104 Val Tgt Transfer Loss: 0.083\n",
      "Epoch 7/500 Train Total Loss: 8.809 Train Src Cls Loss: 1.622 Train Tgt Transfer Loss: 0.072 \n",
      " Val Total Loss: 10.284 Val Src Cls Loss: 2.072 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 8/500 Train Total Loss: 8.773 Train Src Cls Loss: 1.624 Train Tgt Transfer Loss: 0.071 \n",
      " Val Total Loss: 9.913 Val Src Cls Loss: 2.058 Val Tgt Transfer Loss: 0.079\n",
      "Epoch 9/500 Train Total Loss: 8.742 Train Src Cls Loss: 1.617 Train Tgt Transfer Loss: 0.071 \n",
      " Val Total Loss: 9.413 Val Src Cls Loss: 2.032 Val Tgt Transfer Loss: 0.074\n",
      "Epoch 10/500 Train Total Loss: 8.714 Train Src Cls Loss: 1.609 Train Tgt Transfer Loss: 0.071 \n",
      " Val Total Loss: 9.114 Val Src Cls Loss: 2.002 Val Tgt Transfer Loss: 0.071\n",
      "Epoch 11/500 Train Total Loss: 8.539 Train Src Cls Loss: 1.618 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 10.155 Val Src Cls Loss: 1.983 Val Tgt Transfer Loss: 0.082\n",
      "Epoch 12/500 Train Total Loss: 8.325 Train Src Cls Loss: 1.609 Train Tgt Transfer Loss: 0.067 \n",
      " Val Total Loss: 9.807 Val Src Cls Loss: 1.976 Val Tgt Transfer Loss: 0.078\n",
      "Epoch 13/500 Train Total Loss: 8.348 Train Src Cls Loss: 1.606 Train Tgt Transfer Loss: 0.067 \n",
      " Val Total Loss: 10.708 Val Src Cls Loss: 1.959 Val Tgt Transfer Loss: 0.087\n",
      "Epoch 14/500 Train Total Loss: 8.444 Train Src Cls Loss: 1.591 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 9.708 Val Src Cls Loss: 1.947 Val Tgt Transfer Loss: 0.078\n",
      "Epoch 15/500 Train Total Loss: 8.152 Train Src Cls Loss: 1.582 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 9.814 Val Src Cls Loss: 1.958 Val Tgt Transfer Loss: 0.079\n",
      "Epoch 16/500 Train Total Loss: 8.167 Train Src Cls Loss: 1.576 Train Tgt Transfer Loss: 0.066 \n",
      " Val Total Loss: 10.048 Val Src Cls Loss: 1.942 Val Tgt Transfer Loss: 0.081\n",
      "Epoch 17/500 Train Total Loss: 8.444 Train Src Cls Loss: 1.575 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 9.688 Val Src Cls Loss: 1.963 Val Tgt Transfer Loss: 0.077\n",
      "Epoch 18/500 Train Total Loss: 8.453 Train Src Cls Loss: 1.569 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 10.027 Val Src Cls Loss: 1.965 Val Tgt Transfer Loss: 0.081\n",
      "Epoch 19/500 Train Total Loss: 8.454 Train Src Cls Loss: 1.566 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 9.133 Val Src Cls Loss: 1.988 Val Tgt Transfer Loss: 0.071\n",
      "Epoch 20/500 Train Total Loss: 8.536 Train Src Cls Loss: 1.558 Train Tgt Transfer Loss: 0.07 \n",
      " Val Total Loss: 9.259 Val Src Cls Loss: 1.99 Val Tgt Transfer Loss: 0.073\n",
      "Epoch 21/500 Train Total Loss: 8.482 Train Src Cls Loss: 1.571 Train Tgt Transfer Loss: 0.069 \n",
      " Val Total Loss: 9.234 Val Src Cls Loss: 1.999 Val Tgt Transfer Loss: 0.072\n"
     ]
    }
   ],
   "source": [
    "encoder_2016_2018_2019, mlp_2016_2018_2019, best_val_loss, best_val_cls_loss, best_val_transfer_loss = continual_adapt(data_2012_2013, [2016,2018,2019], encoder_2014_2016_2017, mlp_2014_2016_2017, device, lambda_coeff=lambda_coeff, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cd0d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Val Loss: 9.11361026763916 Val Cls Loss: 2.0024688243865967 Val Transfer Loss: 0.07111141085624695\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Val Loss: {best_val_loss} Val Cls Loss: {best_val_cls_loss} Val Transfer Loss: {best_val_transfer_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929e206",
   "metadata": {},
   "source": [
    "## 2019-2020 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e931c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2019_2021 = temp_partition_arxiv(data, year_bound=[-1,2019,2021], proportion=1.0)\n",
    "data_2019_2021 = data_2019_2021.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d913502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder_2016_2018_2019, mlp_2016_2018_2019, data_2019_2021, evaluator)\n",
    "test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab582549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.979 Test Acc: 0.291\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {round(test_loss,3)} Test Acc: {round(test_acc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e6c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8c81e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5907802649083232, 0.4812151970113484, 0.3507152475556683, 0.29148406476966443]\n"
     ]
    }
   ],
   "source": [
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d782c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4285486935612511\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_acc_list) / len(test_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1145da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
