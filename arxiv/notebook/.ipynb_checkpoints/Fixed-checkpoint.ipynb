{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2a7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f993cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hhchung/dyngraph-uda/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbc0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da93a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from ogb.nodeproppred.evaluate import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa60451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import temp_partition_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cbb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ogbn-arxiv'\n",
    "dataset = PygNodePropPredDataset(name = dataset_name, root='/home/hhchung/data/ogb-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe7aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b2bf2",
   "metadata": {},
   "source": [
    "## Model Structure ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0423e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        return x\n",
    "\n",
    "class ThreeLayerGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout)\n",
    "        x = self.linear2(x)\n",
    "        # x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f76cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, mlp, optimizer, data):\n",
    "    encoder.train()\n",
    "    mlp.train()\n",
    "    \n",
    "    out = F.log_softmax(mlp(encoder(data.x, data.edge_index)), dim=1)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test(encoder, mlp, data, evaluator):\n",
    "    encoder.eval()\n",
    "    mlp.eval()\n",
    "    \n",
    "    out = F.log_softmax(mlp(encoder(data.x, data.edge_index)), dim=1)\n",
    "    val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask]).item()\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    val_acc = evaluator.eval({\n",
    "        'y_true': data.y[data.val_mask].unsqueeze(1),\n",
    "        'y_pred': y_pred[data.val_mask],\n",
    "    })['acc']\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c47f3e",
   "metadata": {},
   "source": [
    "## Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9af34247",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygNodePropPredDataset(name = dataset_name, root='/home/hhchung/data/ogb-data')\n",
    "data = dataset[0]\n",
    "data.edge_index = to_undirected(data.edge_index, data.num_nodes) # mimicking barlow twins repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6e29e",
   "metadata": {},
   "source": [
    "## Data Partition ##\n",
    "\n",
    "* Train: 0-2011\n",
    "* Val: 2012\n",
    "* Test:\n",
    "** 2013-2014 (then adapt 2012-2013 adapt-val 2014)\n",
    "** 2015-2016 (then adapt 2014-2015 adapt-val 2016)\n",
    "** 2017-2018 (then adapt 2016-2017 adapt-val 2018)\n",
    "** 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0230e3f",
   "metadata": {},
   "source": [
    "## Source Training Stage ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0515f9",
   "metadata": {},
   "source": [
    "* Train: 0-2011\n",
    "* Val: 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a633f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = data.x.shape[1]\n",
    "class_dim = data.y\n",
    "hidden_dim = 128\n",
    "emb_dim = 256\n",
    "encoder = ThreeLayerGraphSAGE(feat_dim, hidden_dim, emb_dim)\n",
    "mlp = MLPHead(emb_dim, emb_dim // 4, 40)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(mlp.parameters()), lr=1e-3)\n",
    "epochs = 500\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = encoder.to(device)\n",
    "mlp = mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd5e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_2013 = temp_partition_arxiv(data, year_bound=[-1,2012,2013], proportion=1.0)\n",
    "data_2012_2013 = data_2012_2013.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373ed8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/500 Train Loss:3.6856 Val Loss:3.6195 Val Acc:0.1966\n",
      "Epoch:2/500 Train Loss:3.5901 Val Loss:3.5396 Val Acc:0.2044\n",
      "Epoch:3/500 Train Loss:3.4895 Val Loss:3.4454 Val Acc:0.2045\n",
      "Epoch:4/500 Train Loss:3.368 Val Loss:3.3583 Val Acc:0.2045\n",
      "Epoch:5/500 Train Loss:3.2441 Val Loss:3.3238 Val Acc:0.2045\n",
      "Epoch:6/500 Train Loss:3.1718 Val Loss:3.3318 Val Acc:0.2045\n",
      "Epoch:7/500 Train Loss:3.1591 Val Loss:3.2723 Val Acc:0.2045\n",
      "Epoch:8/500 Train Loss:3.0964 Val Loss:3.2117 Val Acc:0.2033\n",
      "Epoch:9/500 Train Loss:3.0419 Val Loss:3.2022 Val Acc:0.1841\n",
      "Epoch:10/500 Train Loss:3.0427 Val Loss:3.188 Val Acc:0.1699\n",
      "Epoch:11/500 Train Loss:3.0402 Val Loss:3.1491 Val Acc:0.1918\n",
      "Epoch:12/500 Train Loss:3.0042 Val Loss:3.1139 Val Acc:0.2061\n",
      "Epoch:13/500 Train Loss:2.9534 Val Loss:3.0913 Val Acc:0.2057\n",
      "Epoch:14/500 Train Loss:2.9167 Val Loss:3.0838 Val Acc:0.2045\n",
      "Epoch:15/500 Train Loss:2.9075 Val Loss:3.0642 Val Acc:0.2047\n",
      "Epoch:16/500 Train Loss:2.8882 Val Loss:3.0173 Val Acc:0.2067\n",
      "Epoch:17/500 Train Loss:2.8515 Val Loss:2.987 Val Acc:0.2166\n",
      "Epoch:18/500 Train Loss:2.8203 Val Loss:2.9635 Val Acc:0.2305\n",
      "Epoch:19/500 Train Loss:2.8066 Val Loss:2.9402 Val Acc:0.2359\n",
      "Epoch:20/500 Train Loss:2.7724 Val Loss:2.9062 Val Acc:0.2348\n",
      "Epoch:21/500 Train Loss:2.7371 Val Loss:2.8965 Val Acc:0.2295\n",
      "Epoch:22/500 Train Loss:2.7061 Val Loss:2.879 Val Acc:0.2348\n",
      "Epoch:23/500 Train Loss:2.6866 Val Loss:2.8432 Val Acc:0.2451\n",
      "Epoch:24/500 Train Loss:2.6511 Val Loss:2.8214 Val Acc:0.257\n",
      "Epoch:25/500 Train Loss:2.6375 Val Loss:2.8162 Val Acc:0.2573\n",
      "Epoch:26/500 Train Loss:2.6175 Val Loss:2.7957 Val Acc:0.2597\n",
      "Epoch:27/500 Train Loss:2.5917 Val Loss:2.7853 Val Acc:0.2645\n",
      "Epoch:28/500 Train Loss:2.5803 Val Loss:2.7812 Val Acc:0.2646\n",
      "Epoch:29/500 Train Loss:2.5666 Val Loss:2.7604 Val Acc:0.2702\n",
      "Epoch:30/500 Train Loss:2.539 Val Loss:2.7399 Val Acc:0.2783\n",
      "Epoch:31/500 Train Loss:2.5264 Val Loss:2.7269 Val Acc:0.2797\n",
      "Epoch:32/500 Train Loss:2.5034 Val Loss:2.7205 Val Acc:0.2794\n",
      "Epoch:33/500 Train Loss:2.4863 Val Loss:2.7007 Val Acc:0.2817\n",
      "Epoch:34/500 Train Loss:2.4669 Val Loss:2.6821 Val Acc:0.2836\n",
      "Epoch:35/500 Train Loss:2.4416 Val Loss:2.6743 Val Acc:0.2866\n",
      "Epoch:36/500 Train Loss:2.4211 Val Loss:2.652 Val Acc:0.287\n",
      "Epoch:37/500 Train Loss:2.4056 Val Loss:2.6361 Val Acc:0.2853\n",
      "Epoch:38/500 Train Loss:2.3862 Val Loss:2.6253 Val Acc:0.2856\n",
      "Epoch:39/500 Train Loss:2.3692 Val Loss:2.6071 Val Acc:0.2912\n",
      "Epoch:40/500 Train Loss:2.3466 Val Loss:2.5898 Val Acc:0.2977\n",
      "Epoch:41/500 Train Loss:2.3304 Val Loss:2.5815 Val Acc:0.3023\n",
      "Epoch:42/500 Train Loss:2.3179 Val Loss:2.5616 Val Acc:0.2979\n",
      "Epoch:43/500 Train Loss:2.2992 Val Loss:2.5567 Val Acc:0.3019\n",
      "Epoch:44/500 Train Loss:2.2822 Val Loss:2.5507 Val Acc:0.3038\n",
      "Epoch:45/500 Train Loss:2.2702 Val Loss:2.5355 Val Acc:0.3026\n",
      "Epoch:46/500 Train Loss:2.2563 Val Loss:2.523 Val Acc:0.3058\n",
      "Epoch:47/500 Train Loss:2.2461 Val Loss:2.5076 Val Acc:0.3156\n",
      "Epoch:48/500 Train Loss:2.2351 Val Loss:2.4975 Val Acc:0.3114\n",
      "Epoch:49/500 Train Loss:2.2221 Val Loss:2.4821 Val Acc:0.3195\n",
      "Epoch:50/500 Train Loss:2.2008 Val Loss:2.4747 Val Acc:0.3176\n",
      "Epoch:51/500 Train Loss:2.2008 Val Loss:2.448 Val Acc:0.3267\n",
      "Epoch:52/500 Train Loss:2.1893 Val Loss:2.4351 Val Acc:0.3251\n",
      "Epoch:53/500 Train Loss:2.1787 Val Loss:2.4297 Val Acc:0.328\n",
      "Epoch:54/500 Train Loss:2.1731 Val Loss:2.4093 Val Acc:0.3335\n",
      "Epoch:55/500 Train Loss:2.1598 Val Loss:2.3882 Val Acc:0.3375\n",
      "Epoch:56/500 Train Loss:2.1517 Val Loss:2.3745 Val Acc:0.3481\n",
      "Epoch:57/500 Train Loss:2.1377 Val Loss:2.3702 Val Acc:0.3486\n",
      "Epoch:58/500 Train Loss:2.1262 Val Loss:2.3571 Val Acc:0.3514\n",
      "Epoch:59/500 Train Loss:2.1183 Val Loss:2.3486 Val Acc:0.352\n",
      "Epoch:60/500 Train Loss:2.1124 Val Loss:2.3354 Val Acc:0.354\n",
      "Epoch:61/500 Train Loss:2.0959 Val Loss:2.3275 Val Acc:0.3624\n",
      "Epoch:62/500 Train Loss:2.0834 Val Loss:2.3167 Val Acc:0.3594\n",
      "Epoch:63/500 Train Loss:2.0773 Val Loss:2.3075 Val Acc:0.3604\n",
      "Epoch:64/500 Train Loss:2.0691 Val Loss:2.2934 Val Acc:0.3686\n",
      "Epoch:65/500 Train Loss:2.0545 Val Loss:2.2777 Val Acc:0.3677\n",
      "Epoch:66/500 Train Loss:2.0466 Val Loss:2.2733 Val Acc:0.3688\n",
      "Epoch:67/500 Train Loss:2.0345 Val Loss:2.2577 Val Acc:0.372\n",
      "Epoch:68/500 Train Loss:2.0204 Val Loss:2.2519 Val Acc:0.3734\n",
      "Epoch:69/500 Train Loss:2.0157 Val Loss:2.2393 Val Acc:0.372\n",
      "Epoch:70/500 Train Loss:1.9976 Val Loss:2.2215 Val Acc:0.3837\n",
      "Epoch:71/500 Train Loss:1.9902 Val Loss:2.2075 Val Acc:0.3855\n",
      "Epoch:72/500 Train Loss:1.9812 Val Loss:2.1984 Val Acc:0.3874\n",
      "Epoch:73/500 Train Loss:1.9692 Val Loss:2.1891 Val Acc:0.3981\n",
      "Epoch:74/500 Train Loss:1.959 Val Loss:2.1771 Val Acc:0.3891\n",
      "Epoch:75/500 Train Loss:1.9389 Val Loss:2.1647 Val Acc:0.3967\n",
      "Epoch:76/500 Train Loss:1.9316 Val Loss:2.1488 Val Acc:0.4082\n",
      "Epoch:77/500 Train Loss:1.9248 Val Loss:2.1241 Val Acc:0.4151\n",
      "Epoch:78/500 Train Loss:1.9079 Val Loss:2.1208 Val Acc:0.4141\n",
      "Epoch:79/500 Train Loss:1.8968 Val Loss:2.1107 Val Acc:0.4165\n",
      "Epoch:80/500 Train Loss:1.8879 Val Loss:2.102 Val Acc:0.4214\n",
      "Epoch:81/500 Train Loss:1.8699 Val Loss:2.0749 Val Acc:0.4242\n",
      "Epoch:82/500 Train Loss:1.8619 Val Loss:2.0693 Val Acc:0.4269\n",
      "Epoch:83/500 Train Loss:1.8455 Val Loss:2.0608 Val Acc:0.4289\n",
      "Epoch:84/500 Train Loss:1.83 Val Loss:2.0449 Val Acc:0.4357\n",
      "Epoch:85/500 Train Loss:1.8179 Val Loss:2.0436 Val Acc:0.4351\n",
      "Epoch:86/500 Train Loss:1.8031 Val Loss:2.0252 Val Acc:0.4435\n",
      "Epoch:87/500 Train Loss:1.7942 Val Loss:2.0214 Val Acc:0.4415\n",
      "Epoch:88/500 Train Loss:1.7843 Val Loss:2.0055 Val Acc:0.4494\n",
      "Epoch:89/500 Train Loss:1.772 Val Loss:1.9892 Val Acc:0.449\n",
      "Epoch:90/500 Train Loss:1.7531 Val Loss:1.9903 Val Acc:0.4503\n",
      "Epoch:91/500 Train Loss:1.748 Val Loss:1.9719 Val Acc:0.4542\n",
      "Epoch:92/500 Train Loss:1.7401 Val Loss:1.9581 Val Acc:0.4634\n",
      "Epoch:93/500 Train Loss:1.7253 Val Loss:1.948 Val Acc:0.4674\n",
      "Epoch:94/500 Train Loss:1.7173 Val Loss:1.9547 Val Acc:0.4601\n",
      "Epoch:95/500 Train Loss:1.7041 Val Loss:1.9514 Val Acc:0.4656\n",
      "Epoch:96/500 Train Loss:1.7025 Val Loss:1.9291 Val Acc:0.4668\n",
      "Epoch:97/500 Train Loss:1.6908 Val Loss:1.9093 Val Acc:0.4752\n",
      "Epoch:98/500 Train Loss:1.6749 Val Loss:1.9092 Val Acc:0.4749\n",
      "Epoch:99/500 Train Loss:1.6673 Val Loss:1.9151 Val Acc:0.4723\n",
      "Epoch:100/500 Train Loss:1.664 Val Loss:1.8971 Val Acc:0.4741\n",
      "Epoch:101/500 Train Loss:1.6626 Val Loss:1.9032 Val Acc:0.4709\n",
      "Epoch:102/500 Train Loss:1.6481 Val Loss:1.885 Val Acc:0.4743\n",
      "Epoch:103/500 Train Loss:1.6438 Val Loss:1.8734 Val Acc:0.4817\n",
      "Epoch:104/500 Train Loss:1.6316 Val Loss:1.8779 Val Acc:0.4813\n",
      "Epoch:105/500 Train Loss:1.629 Val Loss:1.8652 Val Acc:0.487\n",
      "Epoch:106/500 Train Loss:1.6233 Val Loss:1.8528 Val Acc:0.4895\n",
      "Epoch:107/500 Train Loss:1.62 Val Loss:1.8534 Val Acc:0.4827\n",
      "Epoch:108/500 Train Loss:1.6091 Val Loss:1.8401 Val Acc:0.4903\n",
      "Epoch:109/500 Train Loss:1.6077 Val Loss:1.8372 Val Acc:0.4943\n",
      "Epoch:110/500 Train Loss:1.6017 Val Loss:1.8398 Val Acc:0.4973\n",
      "Epoch:111/500 Train Loss:1.5952 Val Loss:1.8293 Val Acc:0.4963\n",
      "Epoch:112/500 Train Loss:1.5881 Val Loss:1.8302 Val Acc:0.497\n",
      "Epoch:113/500 Train Loss:1.5851 Val Loss:1.83 Val Acc:0.4988\n",
      "Epoch:114/500 Train Loss:1.5825 Val Loss:1.8212 Val Acc:0.501\n",
      "Epoch:115/500 Train Loss:1.575 Val Loss:1.8179 Val Acc:0.4984\n",
      "Epoch:116/500 Train Loss:1.5682 Val Loss:1.8201 Val Acc:0.4988\n",
      "Epoch:117/500 Train Loss:1.5578 Val Loss:1.809 Val Acc:0.5027\n",
      "Epoch:118/500 Train Loss:1.5599 Val Loss:1.7979 Val Acc:0.5044\n",
      "Epoch:119/500 Train Loss:1.5575 Val Loss:1.796 Val Acc:0.5018\n",
      "Epoch:120/500 Train Loss:1.5591 Val Loss:1.7976 Val Acc:0.5004\n",
      "Epoch:121/500 Train Loss:1.5465 Val Loss:1.7944 Val Acc:0.5052\n",
      "Epoch:122/500 Train Loss:1.545 Val Loss:1.7969 Val Acc:0.5106\n",
      "Epoch:123/500 Train Loss:1.5436 Val Loss:1.7876 Val Acc:0.5138\n",
      "Epoch:124/500 Train Loss:1.543 Val Loss:1.7882 Val Acc:0.5016\n",
      "Epoch:125/500 Train Loss:1.5352 Val Loss:1.7841 Val Acc:0.5096\n",
      "Epoch:126/500 Train Loss:1.5268 Val Loss:1.775 Val Acc:0.5114\n",
      "Epoch:127/500 Train Loss:1.5321 Val Loss:1.7707 Val Acc:0.5068\n",
      "Epoch:128/500 Train Loss:1.5271 Val Loss:1.7688 Val Acc:0.5144\n",
      "Epoch:129/500 Train Loss:1.5252 Val Loss:1.7732 Val Acc:0.5138\n",
      "Epoch:130/500 Train Loss:1.5162 Val Loss:1.7666 Val Acc:0.511\n",
      "Epoch:131/500 Train Loss:1.5173 Val Loss:1.7726 Val Acc:0.5088\n",
      "Epoch:132/500 Train Loss:1.5078 Val Loss:1.7677 Val Acc:0.5183\n",
      "Epoch:133/500 Train Loss:1.5095 Val Loss:1.7601 Val Acc:0.512\n",
      "Epoch:134/500 Train Loss:1.5062 Val Loss:1.7564 Val Acc:0.5164\n",
      "Epoch:135/500 Train Loss:1.501 Val Loss:1.7669 Val Acc:0.5155\n",
      "Epoch:136/500 Train Loss:1.5004 Val Loss:1.7548 Val Acc:0.5192\n",
      "Epoch:137/500 Train Loss:1.494 Val Loss:1.7508 Val Acc:0.5148\n",
      "Epoch:138/500 Train Loss:1.5019 Val Loss:1.7449 Val Acc:0.5212\n",
      "Epoch:139/500 Train Loss:1.4909 Val Loss:1.7574 Val Acc:0.5206\n",
      "Epoch:140/500 Train Loss:1.4919 Val Loss:1.7324 Val Acc:0.5179\n",
      "Epoch:141/500 Train Loss:1.4946 Val Loss:1.7487 Val Acc:0.5189\n",
      "Epoch:142/500 Train Loss:1.4893 Val Loss:1.7463 Val Acc:0.5145\n",
      "Epoch:143/500 Train Loss:1.4881 Val Loss:1.7441 Val Acc:0.5175\n",
      "Epoch:144/500 Train Loss:1.4758 Val Loss:1.7506 Val Acc:0.5178\n",
      "Epoch:145/500 Train Loss:1.4807 Val Loss:1.74 Val Acc:0.5229\n",
      "Epoch:146/500 Train Loss:1.4739 Val Loss:1.7463 Val Acc:0.5183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:147/500 Train Loss:1.4733 Val Loss:1.7418 Val Acc:0.5217\n",
      "Epoch:148/500 Train Loss:1.465 Val Loss:1.7313 Val Acc:0.5187\n",
      "Epoch:149/500 Train Loss:1.4676 Val Loss:1.7253 Val Acc:0.5235\n",
      "Epoch:150/500 Train Loss:1.4662 Val Loss:1.7294 Val Acc:0.5256\n",
      "Epoch:151/500 Train Loss:1.4597 Val Loss:1.7305 Val Acc:0.52\n",
      "Epoch:152/500 Train Loss:1.4582 Val Loss:1.7274 Val Acc:0.5285\n",
      "Epoch:153/500 Train Loss:1.461 Val Loss:1.719 Val Acc:0.5228\n",
      "Epoch:154/500 Train Loss:1.4537 Val Loss:1.7302 Val Acc:0.5228\n",
      "Epoch:155/500 Train Loss:1.4552 Val Loss:1.7212 Val Acc:0.5226\n",
      "Epoch:156/500 Train Loss:1.4535 Val Loss:1.7305 Val Acc:0.5254\n",
      "Epoch:157/500 Train Loss:1.4523 Val Loss:1.7202 Val Acc:0.5276\n",
      "Epoch:158/500 Train Loss:1.4455 Val Loss:1.7195 Val Acc:0.5271\n",
      "Epoch:159/500 Train Loss:1.4445 Val Loss:1.7109 Val Acc:0.5254\n",
      "Epoch:160/500 Train Loss:1.4483 Val Loss:1.7184 Val Acc:0.5271\n",
      "Epoch:161/500 Train Loss:1.439 Val Loss:1.7219 Val Acc:0.5229\n",
      "Epoch:162/500 Train Loss:1.4448 Val Loss:1.7167 Val Acc:0.5268\n",
      "Epoch:163/500 Train Loss:1.4373 Val Loss:1.7158 Val Acc:0.5217\n",
      "Epoch:164/500 Train Loss:1.4338 Val Loss:1.7107 Val Acc:0.5209\n",
      "Epoch:165/500 Train Loss:1.4315 Val Loss:1.7013 Val Acc:0.5274\n",
      "Epoch:166/500 Train Loss:1.4317 Val Loss:1.7115 Val Acc:0.5215\n",
      "Epoch:167/500 Train Loss:1.4288 Val Loss:1.7037 Val Acc:0.5256\n",
      "Epoch:168/500 Train Loss:1.4295 Val Loss:1.7086 Val Acc:0.522\n",
      "Epoch:169/500 Train Loss:1.4285 Val Loss:1.695 Val Acc:0.5242\n",
      "Epoch:170/500 Train Loss:1.4265 Val Loss:1.7037 Val Acc:0.5234\n",
      "Epoch:171/500 Train Loss:1.4231 Val Loss:1.702 Val Acc:0.5322\n",
      "Epoch:172/500 Train Loss:1.4217 Val Loss:1.7026 Val Acc:0.5232\n",
      "Epoch:173/500 Train Loss:1.4178 Val Loss:1.6918 Val Acc:0.5287\n",
      "Epoch:174/500 Train Loss:1.4157 Val Loss:1.7024 Val Acc:0.528\n",
      "Epoch:175/500 Train Loss:1.4173 Val Loss:1.6973 Val Acc:0.5288\n",
      "Epoch:176/500 Train Loss:1.4135 Val Loss:1.6817 Val Acc:0.5333\n",
      "Epoch:177/500 Train Loss:1.4074 Val Loss:1.7001 Val Acc:0.5253\n",
      "Epoch:178/500 Train Loss:1.412 Val Loss:1.6971 Val Acc:0.5316\n",
      "Epoch:179/500 Train Loss:1.4078 Val Loss:1.6783 Val Acc:0.5302\n",
      "Epoch:180/500 Train Loss:1.4019 Val Loss:1.6925 Val Acc:0.5304\n",
      "Epoch:181/500 Train Loss:1.4007 Val Loss:1.6779 Val Acc:0.5343\n",
      "Epoch:182/500 Train Loss:1.4032 Val Loss:1.682 Val Acc:0.5388\n",
      "Epoch:183/500 Train Loss:1.4005 Val Loss:1.6816 Val Acc:0.5291\n",
      "Epoch:184/500 Train Loss:1.3909 Val Loss:1.6837 Val Acc:0.5319\n",
      "Epoch:185/500 Train Loss:1.3952 Val Loss:1.6884 Val Acc:0.5285\n",
      "Epoch:186/500 Train Loss:1.3953 Val Loss:1.6757 Val Acc:0.535\n",
      "Epoch:187/500 Train Loss:1.3886 Val Loss:1.6869 Val Acc:0.5279\n",
      "Epoch:188/500 Train Loss:1.3877 Val Loss:1.6876 Val Acc:0.5347\n",
      "Epoch:189/500 Train Loss:1.387 Val Loss:1.6805 Val Acc:0.5321\n",
      "Epoch:190/500 Train Loss:1.3857 Val Loss:1.681 Val Acc:0.5326\n",
      "Epoch:191/500 Train Loss:1.384 Val Loss:1.6739 Val Acc:0.5389\n",
      "Epoch:192/500 Train Loss:1.3829 Val Loss:1.6727 Val Acc:0.5305\n",
      "Epoch:193/500 Train Loss:1.3774 Val Loss:1.6599 Val Acc:0.5329\n",
      "Epoch:194/500 Train Loss:1.3758 Val Loss:1.6768 Val Acc:0.5308\n",
      "Epoch:195/500 Train Loss:1.3765 Val Loss:1.6709 Val Acc:0.5298\n",
      "Epoch:196/500 Train Loss:1.3797 Val Loss:1.6572 Val Acc:0.5419\n",
      "Epoch:197/500 Train Loss:1.3718 Val Loss:1.6711 Val Acc:0.533\n",
      "Epoch:198/500 Train Loss:1.3722 Val Loss:1.6688 Val Acc:0.5298\n",
      "Epoch:199/500 Train Loss:1.3715 Val Loss:1.65 Val Acc:0.5391\n",
      "Epoch:200/500 Train Loss:1.3683 Val Loss:1.6561 Val Acc:0.5369\n",
      "Epoch:201/500 Train Loss:1.3677 Val Loss:1.6614 Val Acc:0.5355\n",
      "Epoch:202/500 Train Loss:1.3695 Val Loss:1.6504 Val Acc:0.5419\n",
      "Epoch:203/500 Train Loss:1.3659 Val Loss:1.6557 Val Acc:0.5355\n",
      "Epoch:204/500 Train Loss:1.3638 Val Loss:1.6601 Val Acc:0.536\n",
      "Epoch:205/500 Train Loss:1.364 Val Loss:1.651 Val Acc:0.5391\n",
      "Epoch:206/500 Train Loss:1.3587 Val Loss:1.6486 Val Acc:0.5433\n",
      "Epoch:207/500 Train Loss:1.3536 Val Loss:1.6482 Val Acc:0.5319\n",
      "Epoch:208/500 Train Loss:1.3507 Val Loss:1.6574 Val Acc:0.5344\n",
      "Epoch:209/500 Train Loss:1.3512 Val Loss:1.6584 Val Acc:0.5372\n",
      "Epoch:210/500 Train Loss:1.3518 Val Loss:1.6549 Val Acc:0.5391\n",
      "Epoch:211/500 Train Loss:1.3532 Val Loss:1.6377 Val Acc:0.5402\n",
      "Epoch:212/500 Train Loss:1.3489 Val Loss:1.6429 Val Acc:0.54\n",
      "Epoch:213/500 Train Loss:1.3448 Val Loss:1.6477 Val Acc:0.5374\n",
      "Epoch:214/500 Train Loss:1.3408 Val Loss:1.6402 Val Acc:0.5392\n",
      "Epoch:215/500 Train Loss:1.3415 Val Loss:1.638 Val Acc:0.5409\n",
      "Epoch:216/500 Train Loss:1.3389 Val Loss:1.6289 Val Acc:0.5475\n",
      "Epoch:217/500 Train Loss:1.3307 Val Loss:1.6386 Val Acc:0.5441\n",
      "Epoch:218/500 Train Loss:1.3406 Val Loss:1.6493 Val Acc:0.5386\n",
      "Epoch:219/500 Train Loss:1.3352 Val Loss:1.6385 Val Acc:0.5462\n",
      "Epoch:220/500 Train Loss:1.3328 Val Loss:1.6388 Val Acc:0.5388\n",
      "Epoch:221/500 Train Loss:1.3365 Val Loss:1.6322 Val Acc:0.5413\n",
      "Epoch:222/500 Train Loss:1.327 Val Loss:1.6269 Val Acc:0.5472\n",
      "Epoch:223/500 Train Loss:1.329 Val Loss:1.634 Val Acc:0.54\n",
      "Epoch:224/500 Train Loss:1.3267 Val Loss:1.6356 Val Acc:0.5411\n",
      "Epoch:225/500 Train Loss:1.3237 Val Loss:1.6326 Val Acc:0.5403\n",
      "Epoch:226/500 Train Loss:1.325 Val Loss:1.6396 Val Acc:0.5444\n",
      "Epoch:227/500 Train Loss:1.3209 Val Loss:1.6318 Val Acc:0.5395\n",
      "Epoch:228/500 Train Loss:1.3264 Val Loss:1.638 Val Acc:0.5453\n",
      "Epoch:229/500 Train Loss:1.3202 Val Loss:1.6238 Val Acc:0.5479\n",
      "Epoch:230/500 Train Loss:1.3153 Val Loss:1.6256 Val Acc:0.5405\n",
      "Epoch:231/500 Train Loss:1.316 Val Loss:1.6179 Val Acc:0.5472\n",
      "Epoch:232/500 Train Loss:1.3099 Val Loss:1.6213 Val Acc:0.5439\n",
      "Epoch:233/500 Train Loss:1.3144 Val Loss:1.6218 Val Acc:0.5439\n",
      "Epoch:234/500 Train Loss:1.31 Val Loss:1.6248 Val Acc:0.5523\n",
      "Epoch:235/500 Train Loss:1.3133 Val Loss:1.6175 Val Acc:0.5483\n",
      "Epoch:236/500 Train Loss:1.3063 Val Loss:1.6271 Val Acc:0.5445\n",
      "Epoch:237/500 Train Loss:1.3012 Val Loss:1.6136 Val Acc:0.5427\n",
      "Epoch:238/500 Train Loss:1.308 Val Loss:1.6165 Val Acc:0.5445\n",
      "Epoch:239/500 Train Loss:1.3065 Val Loss:1.6164 Val Acc:0.5507\n",
      "Epoch:240/500 Train Loss:1.2982 Val Loss:1.6154 Val Acc:0.5473\n",
      "Epoch:241/500 Train Loss:1.3031 Val Loss:1.6148 Val Acc:0.5423\n",
      "Epoch:242/500 Train Loss:1.2934 Val Loss:1.6118 Val Acc:0.5472\n",
      "Epoch:243/500 Train Loss:1.3004 Val Loss:1.6098 Val Acc:0.5456\n",
      "Epoch:244/500 Train Loss:1.2949 Val Loss:1.6038 Val Acc:0.5507\n",
      "Epoch:245/500 Train Loss:1.2984 Val Loss:1.6049 Val Acc:0.549\n",
      "Epoch:246/500 Train Loss:1.2851 Val Loss:1.6201 Val Acc:0.5507\n",
      "Epoch:247/500 Train Loss:1.289 Val Loss:1.6019 Val Acc:0.5476\n",
      "Epoch:248/500 Train Loss:1.2929 Val Loss:1.5989 Val Acc:0.5486\n",
      "Epoch:249/500 Train Loss:1.288 Val Loss:1.6075 Val Acc:0.5495\n",
      "Epoch:250/500 Train Loss:1.2893 Val Loss:1.604 Val Acc:0.5465\n",
      "Epoch:251/500 Train Loss:1.2857 Val Loss:1.6071 Val Acc:0.5431\n",
      "Epoch:252/500 Train Loss:1.2822 Val Loss:1.6048 Val Acc:0.5489\n",
      "Epoch:253/500 Train Loss:1.2817 Val Loss:1.6005 Val Acc:0.5481\n",
      "Epoch:254/500 Train Loss:1.2771 Val Loss:1.6062 Val Acc:0.5507\n",
      "Epoch:255/500 Train Loss:1.2783 Val Loss:1.5891 Val Acc:0.5528\n",
      "Epoch:256/500 Train Loss:1.277 Val Loss:1.6042 Val Acc:0.5524\n",
      "Epoch:257/500 Train Loss:1.2748 Val Loss:1.5971 Val Acc:0.5521\n",
      "Epoch:258/500 Train Loss:1.2793 Val Loss:1.5979 Val Acc:0.5492\n",
      "Epoch:259/500 Train Loss:1.2774 Val Loss:1.591 Val Acc:0.554\n",
      "Epoch:260/500 Train Loss:1.2745 Val Loss:1.5961 Val Acc:0.5459\n",
      "Epoch:261/500 Train Loss:1.2701 Val Loss:1.6025 Val Acc:0.5512\n",
      "Epoch:262/500 Train Loss:1.275 Val Loss:1.5938 Val Acc:0.5548\n",
      "Epoch:263/500 Train Loss:1.2669 Val Loss:1.5854 Val Acc:0.5538\n",
      "Epoch:264/500 Train Loss:1.2612 Val Loss:1.5936 Val Acc:0.5509\n",
      "Epoch:265/500 Train Loss:1.2686 Val Loss:1.6007 Val Acc:0.5535\n",
      "Epoch:266/500 Train Loss:1.2666 Val Loss:1.5889 Val Acc:0.5501\n",
      "Epoch:267/500 Train Loss:1.2618 Val Loss:1.5987 Val Acc:0.5534\n",
      "Epoch:268/500 Train Loss:1.2695 Val Loss:1.5788 Val Acc:0.5562\n",
      "Epoch:269/500 Train Loss:1.2589 Val Loss:1.5874 Val Acc:0.5535\n",
      "Epoch:270/500 Train Loss:1.2599 Val Loss:1.5911 Val Acc:0.557\n",
      "Epoch:271/500 Train Loss:1.2601 Val Loss:1.5901 Val Acc:0.5532\n",
      "Epoch:272/500 Train Loss:1.2616 Val Loss:1.5827 Val Acc:0.559\n",
      "Epoch:273/500 Train Loss:1.2552 Val Loss:1.5836 Val Acc:0.5588\n",
      "Epoch:274/500 Train Loss:1.2511 Val Loss:1.5991 Val Acc:0.5532\n",
      "Epoch:275/500 Train Loss:1.2547 Val Loss:1.5809 Val Acc:0.5579\n",
      "Epoch:276/500 Train Loss:1.2555 Val Loss:1.5831 Val Acc:0.5593\n",
      "Epoch:277/500 Train Loss:1.2463 Val Loss:1.578 Val Acc:0.5594\n",
      "Epoch:278/500 Train Loss:1.2538 Val Loss:1.5892 Val Acc:0.5587\n",
      "Epoch:279/500 Train Loss:1.2512 Val Loss:1.5774 Val Acc:0.559\n",
      "Epoch:280/500 Train Loss:1.247 Val Loss:1.579 Val Acc:0.5593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:281/500 Train Loss:1.2514 Val Loss:1.5722 Val Acc:0.558\n",
      "Epoch:282/500 Train Loss:1.2441 Val Loss:1.5783 Val Acc:0.5518\n",
      "Epoch:283/500 Train Loss:1.244 Val Loss:1.5827 Val Acc:0.5576\n",
      "Epoch:284/500 Train Loss:1.2438 Val Loss:1.5763 Val Acc:0.5639\n",
      "Epoch:285/500 Train Loss:1.2442 Val Loss:1.563 Val Acc:0.5596\n",
      "Epoch:286/500 Train Loss:1.2427 Val Loss:1.5702 Val Acc:0.5618\n",
      "Epoch:287/500 Train Loss:1.2425 Val Loss:1.571 Val Acc:0.5622\n",
      "Epoch:288/500 Train Loss:1.2354 Val Loss:1.5716 Val Acc:0.5566\n",
      "Epoch:289/500 Train Loss:1.2391 Val Loss:1.5705 Val Acc:0.5621\n",
      "Epoch:290/500 Train Loss:1.239 Val Loss:1.5751 Val Acc:0.5573\n",
      "Epoch:291/500 Train Loss:1.2379 Val Loss:1.5692 Val Acc:0.5611\n",
      "Epoch:292/500 Train Loss:1.2351 Val Loss:1.5626 Val Acc:0.5596\n",
      "Epoch:293/500 Train Loss:1.2349 Val Loss:1.5769 Val Acc:0.5602\n",
      "Epoch:294/500 Train Loss:1.2317 Val Loss:1.5747 Val Acc:0.5546\n",
      "Epoch:295/500 Train Loss:1.2344 Val Loss:1.5692 Val Acc:0.561\n",
      "Epoch:296/500 Train Loss:1.2312 Val Loss:1.5733 Val Acc:0.5618\n",
      "Epoch:297/500 Train Loss:1.227 Val Loss:1.5654 Val Acc:0.5632\n",
      "Epoch:298/500 Train Loss:1.2255 Val Loss:1.5674 Val Acc:0.559\n",
      "Epoch:299/500 Train Loss:1.225 Val Loss:1.5774 Val Acc:0.5632\n",
      "Epoch:300/500 Train Loss:1.2263 Val Loss:1.5695 Val Acc:0.5616\n",
      "Epoch:301/500 Train Loss:1.2259 Val Loss:1.5719 Val Acc:0.5605\n",
      "Epoch:302/500 Train Loss:1.2261 Val Loss:1.5663 Val Acc:0.5675\n",
      "Epoch:303/500 Train Loss:1.222 Val Loss:1.5493 Val Acc:0.5657\n",
      "Epoch:304/500 Train Loss:1.2229 Val Loss:1.5615 Val Acc:0.5627\n",
      "Epoch:305/500 Train Loss:1.2212 Val Loss:1.56 Val Acc:0.5582\n",
      "Epoch:306/500 Train Loss:1.2153 Val Loss:1.5746 Val Acc:0.5608\n",
      "Epoch:307/500 Train Loss:1.2177 Val Loss:1.5647 Val Acc:0.5638\n",
      "Epoch:308/500 Train Loss:1.2165 Val Loss:1.5791 Val Acc:0.5639\n",
      "Epoch:309/500 Train Loss:1.2155 Val Loss:1.5645 Val Acc:0.5644\n",
      "Epoch:310/500 Train Loss:1.2172 Val Loss:1.5716 Val Acc:0.5591\n",
      "Epoch:311/500 Train Loss:1.2148 Val Loss:1.5643 Val Acc:0.5625\n",
      "Epoch:312/500 Train Loss:1.2139 Val Loss:1.5681 Val Acc:0.5674\n",
      "Epoch:313/500 Train Loss:1.2187 Val Loss:1.5534 Val Acc:0.5722\n",
      "Epoch:314/500 Train Loss:1.2135 Val Loss:1.5565 Val Acc:0.5677\n",
      "Epoch:315/500 Train Loss:1.2083 Val Loss:1.5623 Val Acc:0.5615\n",
      "Epoch:316/500 Train Loss:1.2078 Val Loss:1.5606 Val Acc:0.5638\n",
      "Epoch:317/500 Train Loss:1.2121 Val Loss:1.5559 Val Acc:0.565\n",
      "Epoch:318/500 Train Loss:1.207 Val Loss:1.5683 Val Acc:0.5661\n",
      "Epoch:319/500 Train Loss:1.2028 Val Loss:1.5595 Val Acc:0.5632\n",
      "Epoch:320/500 Train Loss:1.208 Val Loss:1.5542 Val Acc:0.565\n",
      "Epoch:321/500 Train Loss:1.2025 Val Loss:1.5557 Val Acc:0.5691\n",
      "Epoch:322/500 Train Loss:1.2019 Val Loss:1.5551 Val Acc:0.5681\n",
      "Epoch:323/500 Train Loss:1.2033 Val Loss:1.5622 Val Acc:0.5657\n",
      "Epoch:324/500 Train Loss:1.2012 Val Loss:1.559 Val Acc:0.5653\n",
      "Epoch:325/500 Train Loss:1.1944 Val Loss:1.569 Val Acc:0.5619\n",
      "Epoch:326/500 Train Loss:1.1963 Val Loss:1.5601 Val Acc:0.5686\n",
      "Epoch:327/500 Train Loss:1.1955 Val Loss:1.5485 Val Acc:0.5754\n",
      "Epoch:328/500 Train Loss:1.1877 Val Loss:1.5603 Val Acc:0.5649\n",
      "Epoch:329/500 Train Loss:1.1967 Val Loss:1.5484 Val Acc:0.5663\n",
      "Epoch:330/500 Train Loss:1.1918 Val Loss:1.5612 Val Acc:0.5658\n",
      "Epoch:331/500 Train Loss:1.1864 Val Loss:1.554 Val Acc:0.5655\n",
      "Epoch:332/500 Train Loss:1.1946 Val Loss:1.549 Val Acc:0.5664\n",
      "Epoch:333/500 Train Loss:1.1942 Val Loss:1.5523 Val Acc:0.5706\n",
      "Epoch:334/500 Train Loss:1.1908 Val Loss:1.5546 Val Acc:0.5674\n",
      "Epoch:335/500 Train Loss:1.1891 Val Loss:1.5491 Val Acc:0.5669\n",
      "Epoch:336/500 Train Loss:1.191 Val Loss:1.5556 Val Acc:0.5658\n",
      "Epoch:337/500 Train Loss:1.1885 Val Loss:1.5505 Val Acc:0.5725\n",
      "Epoch:338/500 Train Loss:1.1852 Val Loss:1.54 Val Acc:0.5713\n",
      "Epoch:339/500 Train Loss:1.1891 Val Loss:1.5474 Val Acc:0.5633\n",
      "Epoch:340/500 Train Loss:1.1854 Val Loss:1.5645 Val Acc:0.5615\n",
      "Epoch:341/500 Train Loss:1.1875 Val Loss:1.5386 Val Acc:0.5683\n",
      "Epoch:342/500 Train Loss:1.1762 Val Loss:1.5503 Val Acc:0.5672\n",
      "Epoch:343/500 Train Loss:1.1842 Val Loss:1.5505 Val Acc:0.5672\n",
      "Epoch:344/500 Train Loss:1.1774 Val Loss:1.5354 Val Acc:0.5758\n",
      "Epoch:345/500 Train Loss:1.1818 Val Loss:1.5418 Val Acc:0.5695\n",
      "Epoch:346/500 Train Loss:1.1788 Val Loss:1.5393 Val Acc:0.5745\n",
      "Epoch:347/500 Train Loss:1.1739 Val Loss:1.5433 Val Acc:0.5708\n",
      "Epoch:348/500 Train Loss:1.1769 Val Loss:1.5455 Val Acc:0.5737\n",
      "Epoch:349/500 Train Loss:1.1766 Val Loss:1.5457 Val Acc:0.5711\n",
      "Epoch:350/500 Train Loss:1.1722 Val Loss:1.531 Val Acc:0.5742\n",
      "Epoch:351/500 Train Loss:1.1738 Val Loss:1.5519 Val Acc:0.5666\n",
      "Epoch:352/500 Train Loss:1.1684 Val Loss:1.5371 Val Acc:0.5744\n",
      "Epoch:353/500 Train Loss:1.1659 Val Loss:1.5502 Val Acc:0.5653\n",
      "Epoch:354/500 Train Loss:1.1682 Val Loss:1.5232 Val Acc:0.5702\n",
      "Epoch:355/500 Train Loss:1.1678 Val Loss:1.5621 Val Acc:0.5674\n",
      "Epoch:356/500 Train Loss:1.1687 Val Loss:1.5503 Val Acc:0.5663\n",
      "Epoch:357/500 Train Loss:1.1684 Val Loss:1.5336 Val Acc:0.5713\n",
      "Epoch:358/500 Train Loss:1.1668 Val Loss:1.5581 Val Acc:0.5713\n",
      "Epoch:359/500 Train Loss:1.161 Val Loss:1.5427 Val Acc:0.5664\n",
      "Epoch:360/500 Train Loss:1.1667 Val Loss:1.544 Val Acc:0.5649\n",
      "Epoch:361/500 Train Loss:1.1635 Val Loss:1.5332 Val Acc:0.5697\n",
      "Epoch:362/500 Train Loss:1.1564 Val Loss:1.5345 Val Acc:0.5776\n",
      "Epoch:363/500 Train Loss:1.1646 Val Loss:1.5426 Val Acc:0.5776\n",
      "Epoch:364/500 Train Loss:1.1628 Val Loss:1.5304 Val Acc:0.5773\n",
      "Epoch:365/500 Train Loss:1.1619 Val Loss:1.5441 Val Acc:0.5754\n",
      "Epoch:366/500 Train Loss:1.1583 Val Loss:1.5415 Val Acc:0.5737\n",
      "Epoch:367/500 Train Loss:1.155 Val Loss:1.5308 Val Acc:0.5714\n",
      "Epoch:368/500 Train Loss:1.1513 Val Loss:1.5359 Val Acc:0.5695\n",
      "Epoch:369/500 Train Loss:1.1522 Val Loss:1.5397 Val Acc:0.5686\n",
      "Epoch:370/500 Train Loss:1.1543 Val Loss:1.5503 Val Acc:0.5688\n",
      "Epoch:371/500 Train Loss:1.1542 Val Loss:1.5355 Val Acc:0.5765\n",
      "Epoch:372/500 Train Loss:1.1496 Val Loss:1.5225 Val Acc:0.572\n",
      "Epoch:373/500 Train Loss:1.156 Val Loss:1.5275 Val Acc:0.5733\n",
      "Epoch:374/500 Train Loss:1.1561 Val Loss:1.5302 Val Acc:0.5733\n",
      "Epoch:375/500 Train Loss:1.1471 Val Loss:1.5325 Val Acc:0.5744\n",
      "Epoch:376/500 Train Loss:1.1464 Val Loss:1.5244 Val Acc:0.575\n",
      "Epoch:377/500 Train Loss:1.1463 Val Loss:1.549 Val Acc:0.5739\n",
      "Epoch:378/500 Train Loss:1.1503 Val Loss:1.5264 Val Acc:0.5739\n",
      "Epoch:379/500 Train Loss:1.1479 Val Loss:1.5389 Val Acc:0.5706\n",
      "Epoch:380/500 Train Loss:1.142 Val Loss:1.5298 Val Acc:0.5733\n",
      "Epoch:381/500 Train Loss:1.149 Val Loss:1.5295 Val Acc:0.5708\n",
      "Epoch:382/500 Train Loss:1.1344 Val Loss:1.5238 Val Acc:0.5742\n",
      "Epoch:383/500 Train Loss:1.1447 Val Loss:1.5309 Val Acc:0.5697\n",
      "Epoch:384/500 Train Loss:1.1415 Val Loss:1.5341 Val Acc:0.5776\n",
      "Epoch:385/500 Train Loss:1.139 Val Loss:1.5304 Val Acc:0.57\n",
      "Epoch:386/500 Train Loss:1.142 Val Loss:1.5306 Val Acc:0.5742\n",
      "Epoch:387/500 Train Loss:1.1422 Val Loss:1.5379 Val Acc:0.5719\n",
      "Epoch:388/500 Train Loss:1.1372 Val Loss:1.5301 Val Acc:0.5754\n",
      "Epoch:389/500 Train Loss:1.129 Val Loss:1.5371 Val Acc:0.5756\n",
      "Epoch:390/500 Train Loss:1.1377 Val Loss:1.5247 Val Acc:0.58\n",
      "Epoch:391/500 Train Loss:1.1336 Val Loss:1.5281 Val Acc:0.5742\n",
      "Epoch:392/500 Train Loss:1.1339 Val Loss:1.5383 Val Acc:0.5716\n",
      "Epoch:393/500 Train Loss:1.1357 Val Loss:1.524 Val Acc:0.5773\n",
      "Epoch:394/500 Train Loss:1.1374 Val Loss:1.5303 Val Acc:0.5748\n",
      "Epoch:395/500 Train Loss:1.1222 Val Loss:1.5217 Val Acc:0.5765\n",
      "Epoch:396/500 Train Loss:1.1337 Val Loss:1.5269 Val Acc:0.5803\n",
      "Epoch:397/500 Train Loss:1.1301 Val Loss:1.5406 Val Acc:0.575\n",
      "Epoch:398/500 Train Loss:1.1364 Val Loss:1.5259 Val Acc:0.5751\n",
      "Epoch:399/500 Train Loss:1.1313 Val Loss:1.5247 Val Acc:0.575\n",
      "Epoch:400/500 Train Loss:1.1287 Val Loss:1.5317 Val Acc:0.5706\n",
      "Epoch:401/500 Train Loss:1.1228 Val Loss:1.5381 Val Acc:0.5754\n",
      "Epoch:402/500 Train Loss:1.1282 Val Loss:1.5387 Val Acc:0.5705\n",
      "Epoch:403/500 Train Loss:1.1288 Val Loss:1.5317 Val Acc:0.5801\n",
      "Epoch:404/500 Train Loss:1.1257 Val Loss:1.5321 Val Acc:0.5803\n",
      "Epoch:405/500 Train Loss:1.1218 Val Loss:1.5433 Val Acc:0.5762\n",
      "Epoch:406/500 Train Loss:1.1191 Val Loss:1.5321 Val Acc:0.5731\n",
      "Epoch:407/500 Train Loss:1.1159 Val Loss:1.5256 Val Acc:0.5733\n",
      "Epoch:408/500 Train Loss:1.1283 Val Loss:1.5246 Val Acc:0.5796\n",
      "Epoch:409/500 Train Loss:1.124 Val Loss:1.5297 Val Acc:0.5776\n",
      "Epoch:410/500 Train Loss:1.1213 Val Loss:1.5368 Val Acc:0.5713\n",
      "Epoch:411/500 Train Loss:1.1239 Val Loss:1.5305 Val Acc:0.5801\n",
      "Epoch:412/500 Train Loss:1.1191 Val Loss:1.522 Val Acc:0.5767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:413/500 Train Loss:1.1169 Val Loss:1.5312 Val Acc:0.5778\n",
      "Epoch:414/500 Train Loss:1.1165 Val Loss:1.5217 Val Acc:0.5782\n",
      "Epoch:415/500 Train Loss:1.1233 Val Loss:1.5297 Val Acc:0.5708\n",
      "Epoch:416/500 Train Loss:1.1214 Val Loss:1.5292 Val Acc:0.5782\n",
      "Epoch:417/500 Train Loss:1.1173 Val Loss:1.5288 Val Acc:0.573\n",
      "Epoch:418/500 Train Loss:1.1106 Val Loss:1.5304 Val Acc:0.5807\n",
      "Epoch:419/500 Train Loss:1.117 Val Loss:1.5356 Val Acc:0.575\n",
      "Epoch:420/500 Train Loss:1.112 Val Loss:1.5391 Val Acc:0.577\n",
      "Epoch:421/500 Train Loss:1.1153 Val Loss:1.5303 Val Acc:0.5787\n",
      "Epoch:422/500 Train Loss:1.1169 Val Loss:1.5277 Val Acc:0.5806\n",
      "Epoch:423/500 Train Loss:1.1051 Val Loss:1.5384 Val Acc:0.5812\n",
      "Epoch:424/500 Train Loss:1.115 Val Loss:1.5341 Val Acc:0.5772\n",
      "Epoch:425/500 Train Loss:1.1119 Val Loss:1.5217 Val Acc:0.5775\n",
      "Epoch:426/500 Train Loss:1.1057 Val Loss:1.5282 Val Acc:0.5782\n",
      "Epoch:427/500 Train Loss:1.1094 Val Loss:1.5267 Val Acc:0.5765\n",
      "Epoch:428/500 Train Loss:1.1106 Val Loss:1.5461 Val Acc:0.5731\n",
      "Epoch:429/500 Train Loss:1.1069 Val Loss:1.527 Val Acc:0.5759\n",
      "Epoch:430/500 Train Loss:1.1028 Val Loss:1.5197 Val Acc:0.5826\n",
      "Epoch:431/500 Train Loss:1.1082 Val Loss:1.5302 Val Acc:0.5773\n",
      "Epoch:432/500 Train Loss:1.099 Val Loss:1.5206 Val Acc:0.5786\n",
      "Epoch:433/500 Train Loss:1.1053 Val Loss:1.5297 Val Acc:0.5818\n",
      "Epoch:434/500 Train Loss:1.1015 Val Loss:1.531 Val Acc:0.5798\n",
      "Epoch:435/500 Train Loss:1.1042 Val Loss:1.5346 Val Acc:0.579\n",
      "Epoch:436/500 Train Loss:1.1006 Val Loss:1.5277 Val Acc:0.5759\n",
      "Epoch:437/500 Train Loss:1.1019 Val Loss:1.5345 Val Acc:0.5725\n",
      "Epoch:438/500 Train Loss:1.1022 Val Loss:1.5448 Val Acc:0.5697\n",
      "Epoch:439/500 Train Loss:1.1015 Val Loss:1.5244 Val Acc:0.5843\n",
      "Epoch:440/500 Train Loss:1.0935 Val Loss:1.5236 Val Acc:0.5807\n",
      "Epoch:441/500 Train Loss:1.0977 Val Loss:1.5406 Val Acc:0.5744\n",
      "Epoch:442/500 Train Loss:1.0922 Val Loss:1.5408 Val Acc:0.5734\n",
      "Epoch:443/500 Train Loss:1.0984 Val Loss:1.5207 Val Acc:0.577\n",
      "Epoch:444/500 Train Loss:1.098 Val Loss:1.533 Val Acc:0.579\n",
      "Epoch:445/500 Train Loss:1.0989 Val Loss:1.5088 Val Acc:0.5795\n",
      "Epoch:446/500 Train Loss:1.0941 Val Loss:1.5265 Val Acc:0.5789\n",
      "Epoch:447/500 Train Loss:1.102 Val Loss:1.5259 Val Acc:0.5784\n",
      "Epoch:448/500 Train Loss:1.0917 Val Loss:1.5212 Val Acc:0.5846\n",
      "Epoch:449/500 Train Loss:1.0915 Val Loss:1.5206 Val Acc:0.5851\n",
      "Epoch:450/500 Train Loss:1.0941 Val Loss:1.538 Val Acc:0.5868\n",
      "Epoch:451/500 Train Loss:1.1009 Val Loss:1.5315 Val Acc:0.5779\n",
      "Epoch:452/500 Train Loss:1.0956 Val Loss:1.5197 Val Acc:0.5778\n",
      "Epoch:453/500 Train Loss:1.0813 Val Loss:1.5264 Val Acc:0.5775\n",
      "Epoch:454/500 Train Loss:1.0911 Val Loss:1.5312 Val Acc:0.5747\n",
      "Epoch:455/500 Train Loss:1.0946 Val Loss:1.5123 Val Acc:0.5834\n",
      "Epoch:456/500 Train Loss:1.0883 Val Loss:1.5381 Val Acc:0.5795\n",
      "Epoch:457/500 Train Loss:1.0891 Val Loss:1.5434 Val Acc:0.5748\n",
      "Epoch:458/500 Train Loss:1.0879 Val Loss:1.5392 Val Acc:0.5699\n",
      "Epoch:459/500 Train Loss:1.0848 Val Loss:1.5171 Val Acc:0.5758\n",
      "Epoch:460/500 Train Loss:1.0841 Val Loss:1.5312 Val Acc:0.5751\n",
      "Epoch:461/500 Train Loss:1.0904 Val Loss:1.5294 Val Acc:0.5812\n",
      "Epoch:462/500 Train Loss:1.0929 Val Loss:1.5359 Val Acc:0.5814\n",
      "Epoch:463/500 Train Loss:1.082 Val Loss:1.5297 Val Acc:0.5745\n",
      "Epoch:464/500 Train Loss:1.0824 Val Loss:1.5286 Val Acc:0.5817\n",
      "Epoch:465/500 Train Loss:1.0858 Val Loss:1.5279 Val Acc:0.5828\n",
      "Epoch:466/500 Train Loss:1.0814 Val Loss:1.5358 Val Acc:0.5762\n",
      "Epoch:467/500 Train Loss:1.079 Val Loss:1.5273 Val Acc:0.5753\n",
      "Epoch:468/500 Train Loss:1.0804 Val Loss:1.527 Val Acc:0.5815\n",
      "Epoch:469/500 Train Loss:1.0835 Val Loss:1.5345 Val Acc:0.5748\n",
      "Epoch:470/500 Train Loss:1.0762 Val Loss:1.5268 Val Acc:0.5826\n",
      "Epoch:471/500 Train Loss:1.0775 Val Loss:1.5339 Val Acc:0.5748\n",
      "Epoch:472/500 Train Loss:1.0784 Val Loss:1.5365 Val Acc:0.5759\n",
      "Epoch:473/500 Train Loss:1.0732 Val Loss:1.5333 Val Acc:0.5772\n",
      "Epoch:474/500 Train Loss:1.0722 Val Loss:1.5203 Val Acc:0.5782\n",
      "Epoch:475/500 Train Loss:1.078 Val Loss:1.5294 Val Acc:0.5779\n",
      "Epoch:476/500 Train Loss:1.0729 Val Loss:1.5265 Val Acc:0.5815\n",
      "Epoch:477/500 Train Loss:1.0727 Val Loss:1.533 Val Acc:0.5845\n",
      "Epoch:478/500 Train Loss:1.0752 Val Loss:1.5417 Val Acc:0.5756\n",
      "Epoch:479/500 Train Loss:1.0718 Val Loss:1.5234 Val Acc:0.5809\n",
      "Epoch:480/500 Train Loss:1.078 Val Loss:1.5261 Val Acc:0.5818\n",
      "Epoch:481/500 Train Loss:1.0687 Val Loss:1.5275 Val Acc:0.5789\n",
      "Epoch:482/500 Train Loss:1.075 Val Loss:1.5199 Val Acc:0.5784\n",
      "Epoch:483/500 Train Loss:1.0703 Val Loss:1.53 Val Acc:0.5792\n",
      "Epoch:484/500 Train Loss:1.072 Val Loss:1.5175 Val Acc:0.5814\n",
      "Epoch:485/500 Train Loss:1.0711 Val Loss:1.5162 Val Acc:0.5834\n",
      "Epoch:486/500 Train Loss:1.0651 Val Loss:1.5339 Val Acc:0.5803\n",
      "Epoch:487/500 Train Loss:1.067 Val Loss:1.5441 Val Acc:0.5823\n",
      "Epoch:488/500 Train Loss:1.0732 Val Loss:1.5259 Val Acc:0.5792\n",
      "Epoch:489/500 Train Loss:1.0656 Val Loss:1.5242 Val Acc:0.5779\n",
      "Epoch:490/500 Train Loss:1.0691 Val Loss:1.5204 Val Acc:0.586\n",
      "Epoch:491/500 Train Loss:1.0644 Val Loss:1.5402 Val Acc:0.5782\n",
      "Epoch:492/500 Train Loss:1.073 Val Loss:1.5302 Val Acc:0.5801\n",
      "Epoch:493/500 Train Loss:1.0725 Val Loss:1.5206 Val Acc:0.5775\n",
      "Epoch:494/500 Train Loss:1.0653 Val Loss:1.5254 Val Acc:0.5829\n",
      "Epoch:495/500 Train Loss:1.0645 Val Loss:1.5317 Val Acc:0.5782\n",
      "Epoch:496/500 Train Loss:1.0693 Val Loss:1.5242 Val Acc:0.5809\n",
      "Epoch:497/500 Train Loss:1.0642 Val Loss:1.5268 Val Acc:0.5812\n",
      "Epoch:498/500 Train Loss:1.0599 Val Loss:1.5282 Val Acc:0.5804\n",
      "Epoch:499/500 Train Loss:1.063 Val Loss:1.53 Val Acc:0.5855\n",
      "Epoch:500/500 Train Loss:1.0597 Val Loss:1.5489 Val Acc:0.5725\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_encoder = None\n",
    "best_mlp = None\n",
    "evaluator = Evaluator(name='ogbn-arxiv')\n",
    "for e in range(1, epochs + 1):\n",
    "    train_loss = train(encoder, mlp, optimizer, data_2012_2013)\n",
    "    val_loss, val_acc = test(encoder, mlp, data_2012_2013, evaluator)\n",
    "    print(f\"Epoch:{e}/{epochs} Train Loss:{round(train_loss,4)} Val Loss:{round(val_loss,4)} Val Acc:{round(val_acc, 4)}\")\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_encoder = deepcopy(encoder)\n",
    "        best_mlp = deepcopy(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e6cea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5867909867909868"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6454958",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298845d3",
   "metadata": {},
   "source": [
    "## 2013-2014 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fe91203",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2013_2015 = temp_partition_arxiv(data, year_bound=[-1,2013,2015], proportion=1.0)\n",
    "data_2013_2015 = data_2013_2015.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd7cba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = test(best_encoder, best_mlp, data_2013_2015, evaluator)\n",
    "test_acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea6798e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4982811212539673, 0.5856903233269709)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f73924",
   "metadata": {},
   "source": [
    "## 2015-2016 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0ee8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2015_2017 = temp_partition_arxiv(data, year_bound=[-1,2015,2017], proportion=1.0)\n",
    "data_2015_2017 = data_2015_2017.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ad9c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = test(best_encoder, best_mlp, data_2015_2017, evaluator)\n",
    "test_acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d59936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5647748708724976, 0.5557200253753436)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a70a4e",
   "metadata": {},
   "source": [
    "## 2017-2018 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "823d9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017_2019 = temp_partition_arxiv(data, year_bound=[-1,2017,2019], proportion=1.0)\n",
    "data_2017_2019 = data_2017_2019.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10c0bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = test(best_encoder, best_mlp, data_2017_2019, evaluator)\n",
    "test_acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53a2f0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6256769895553589, 0.5425538143283699)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929e206",
   "metadata": {},
   "source": [
    "## 2019-2020 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e931c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2019_2021 = temp_partition_arxiv(data, year_bound=[-1,2019,2021], proportion=1.0)\n",
    "data_2019_2021 = data_2019_2021.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d913502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = test(best_encoder, best_mlp, data_2019_2021, evaluator)\n",
    "test_acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab582549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5660314559936523, 0.5557475875974734)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4f080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6325e50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5856903233269709, 0.5557200253753436, 0.5425538143283699, 0.5557475875974734]\n"
     ]
    }
   ],
   "source": [
    "print(test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "675ea34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5599279376570395\n"
     ]
    }
   ],
   "source": [
    "print(sum(test_acc_list) / len(test_acc_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
